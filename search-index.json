[
  {
    "title": "Time's Arrow",
    "url": "https://arghyadutta.github.io/notebooks/timesArrow.html",
    "content": "Arrow of Time and Irreversibility If I Get Time"
  },
  {
    "title": "Academic advice",
    "url": "https://arghyadutta.github.io/notebooks/academicAdvice.html",
    "content": "Advice on scientific writing by Hermut Grubmüller Practical, actionable advice. Showcase of Scholarly Writing Derek Dreyer; 2020. How To Write Papers So People Can Read Them (accessed 2025-09-04). Guide to english communication for scientists by Nature Ian Baldwin (Max Planck Institute): Making Scientific Writing Painless ; 2017.  (accessed 2025-09-04). How to do research See also If you're just starting to write scientific prose, watch the series Giving talks Writing in the Sciences by Kristin Sainani . Susan McConnell (Stanford): Designing Effective Scientific Presentations ; 2011.  (accessed 2025-09-04). Jhala, R. 2005. (An Opionionated Talk) On Preparing Good Talks (accessed 2025-09-04). Geroch, R. Suggestions For Giving Talks. arXiv:gr-qc/9703019 1997. Academic writing On Writing Schimel, J. (2011). Writing Science: How to Write Papers That Get Cited and Proposals That Get Funded. Oxford University Press."
  },
  {
    "title": "Statistics with Python",
    "url": "https://arghyadutta.github.io/notebooks/statPython.html",
    "content": " Histogram and KDE ¶ In [1]: import numpy as np import matplotlib.pyplot as plt import seaborn as sns fig , ax = plt . subplots ( 1 , 5 , figsize = ( 10 , 2 ), constrained_layout = True ) x = np . random . randint ( 0 , 9 , 1000 ) properties = { \"kde\" : True , \"bins\" : 9 } sns . histplot ( x , ax = ax [ 0 ], stat = \"count\" , ** properties ) sns . histplot ( x , ax = ax [ 1 ], stat = \"frequency\" , ** properties ) sns . histplot ( x , ax = ax [ 2 ], stat = \"probability\" , ** properties ) sns . histplot ( x , ax = ax [ 3 ], stat = \"percent\" , ** properties ) sns . histplot ( x , ax = ax [ 4 ], stat = \"density\" , ** properties ) pass ; Linear Regression in Python ¶ percent: normalize such that bar heights sum to 100 import numpy as np import matplotlib.pyplot as plt import seaborn as sns fig , ax = plt . subplots ( 1 , 5 , figsize = ( 10 , 2 ), constrained_layout = True ) x = np . random . randint ( 0 , 9 , 1000 ) properties = { \"kde\" : True , \"bins\" : 9 } sns . histplot ( x , ax = ax [ 0 ], stat = \"count\" , ** properties ) sns . histplot ( x , ax = ax [ 1 ], stat = \"frequency\" , ** properties ) sns . histplot ( x , ax = ax [ 2 ], stat = \"probability\" , ** properties ) sns . histplot ( x , ax = ax [ 3 ], stat = \"percent\" , ** properties ) sns . histplot ( x , ax = ax [ 4 ], stat = \"density\" , ** properties ) pass ; Statistics with Python Histogram and KDE ¶ Histogram and KDE for 1000 random integers from [0,8]. Make sure to mention what the y-axis denotes. In Seaborn, you can choose it via the stat parameter. From Seaborn documentation, stat has the following options [ws:seaborn] count: number of observations in each bin frequency: number of observations divided by the bin width probability or proportion: normalize such that bar heights sum to 1 percent: normalize such that bar heights sum to 100 density: normalize such that the total area of the histogram equals 1 See: Seaborn histplot documentation In [2]: In Seaborn, you can choose it via the stat parameter. From Seaborn documentation, stat has the following options [ws:seaborn] In [1]: In [2]: import pandas as pd import numpy as np from scipy import stats import matplotlib.pyplot as plt import seaborn as sns import statsmodels.api as sm # fix seed to get reproducible results # scipy.stats uses np.random to generate its random numbers, so setting seed using numpy works. (ref: https://stackoverflow.com/questions/16016959/scipy-stats-seed) np . random . seed ( seed = 23 ) nobs = 1000 # number of observations standard_normal = np . random . normal ( loc = 0.0 , scale = 1.0 , size = nobs ) shifted_normal = np . random . normal ( loc = 0.5 , scale = 1.0 , size = nobs ) plt . hist ( standard_normal , bins = 30 , histtype = \"step\" , label = \"standard normal\" ) plt . hist ( shifted_normal , bins = 30 , histtype = \"step\" , label = \"shifted normal\" ) plt . legend () # a quick way to fit a linear regression model using statmodels # ref: https://realpython.com/linear-regression-in-python/#advanced-linear-regression-with-statsmodels # simple linear regression # You need to add the column of ones to the inputs if you want statsmodels to calculate the intercept. It doesn’t take the intercept into account by default. x = standard_normal X = sm . add_constant ( standard_normal ) # print(x[:5], X[:5]) # create a y axis to test linear regression with an error term y = 2.3 + 1.55 * x + 2.3 * np . random . exponential ( size = nobs ) # be sure to provide X, not x lin_reg = sm . OLS ( y , X ) . fit () print ( lin_reg . summary ()) # you can get the predicted values using lin_reg.fittedvalues as used in the plot below. you can also use the model to predict y for new x values. x_new = np . linspace ( - 3 , 3 , 5 ) X_new = sm . add_constant ( x_new ) y_new = lin_reg . predict ( X_new ) # print(x_new,y_new) fig , ax = plt . subplots ( 1 , 2 ) ax [ 0 ] . scatter ( x , y , c = \"gray\" ) ax [ 0 ] . plot ( x , lin_reg . fittedvalues , c = \"orange\" ) ax [ 0 ] . scatter ( x_new , y_new , c = \"crimson\" ) # seaborn automatically plots with confidence interval sns . regplot ( x = x , y = y , ax = ax [ 1 ]) # multiple linear regression X = np . array ( list ( zip ( standard_normal , shifted_normal ))) X # You need to add the column of ones to the inputs if you want statsmodels to calculate the intercept. It doesn’t take intercept into account by default. X = sm . add_constant ( X ) X # create a y axis to test linear regression y = ( 2.3 + 1.55 * standard_normal + 2.4 * shifted_normal + 5 * np . sin ( np . random . random ( size = nobs )) ) lin_reg = sm . OLS ( y , X ) . fit () lin_reg . summary () # get statistical properties using scipy stats module stats . describe ( standard_normal ) stats . describe ( shifted_normal ) # checking if the distributions are normal (we know they are) stats . normaltest ( standard_normal ) stats . normaltest ( shifted_normal ) # KS-test stats . kstest ( standard_normal , stats . norm . cdf ) stats . kstest ( shifted_normal , stats . norm . cdf ) # two-tailed test # finds if the given random numbers are from same distribution stats . ttest_ind ( standard_normal , shifted_normal ) pass ; OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.262\nModel:                            OLS   Adj. R-squared:                  0.261\nMethod:                 Least Squares   F-statistic:                     354.1\nDate:                Thu, 04 Sep 2025   Prob (F-statistic):           7.79e-68\nTime:                        14:19:30   Log-Likelihood:                -2342.7\nNo. Observations:                1000   AIC:                             4689.\nDf Residuals:                     998   BIC:                             4699.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          4.5249      0.080     56.637      0.000       4.368       4.682\nx1             1.5599      0.083     18.816      0.000       1.397       1.723\n==============================================================================\nOmnibus:                      822.687   Durbin-Watson:                   1.998\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            27895.609\nSkew:                           3.536   Prob(JB):                         0.00\nKurtosis:                      27.890   Cond. No.                         1.08\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified. import pandas as pd import numpy as np from scipy import stats import matplotlib.pyplot as plt import seaborn as sns import statsmodels.api as sm # fix seed to get reproducible results # scipy.stats uses np.random to generate its random numbers, so setting seed using numpy works. (ref: https://stackoverflow.com/questions/16016959/scipy-stats-seed) np . random . seed ( seed = 23 ) nobs = 1000 # number of observations standard_normal = np . random . normal ( loc = 0.0 , scale = 1.0 , size = nobs ) shifted_normal = np . random . normal ( loc = 0.5 , scale = 1.0 , size = nobs ) plt . hist ( standard_normal , bins = 30 , histtype = \"step\" , label = \"standard normal\" ) plt . hist ( shifted_normal , bins = 30 , histtype = \"step\" , label = \"shifted normal\" ) plt . legend () # a quick way to fit a linear regression model using statmodels # ref: https://realpython.com/linear-regression-in-python/#advanced-linear-regression-with-statsmodels # simple linear regression # You need to add the column of ones to the inputs if you want statsmodels to calculate the intercept. It doesn’t take the intercept into account by default. x = standard_normal X = sm . add_constant ( standard_normal ) # print(x[:5], X[:5]) # create a y axis to test linear regression with an error term y = 2.3 + 1.55 * x + 2.3 * np . random . exponential ( size = nobs ) # be sure to provide X, not x lin_reg = sm . OLS ( y , X ) . fit () print ( lin_reg . summary ()) # you can get the predicted values using lin_reg.fittedvalues as used in the plot below. you can also use the model to predict y for new x values. x_new = np . linspace ( - 3 , 3 , 5 ) X_new = sm . add_constant ( x_new ) y_new = lin_reg . predict ( X_new ) # print(x_new,y_new) fig , ax = plt . subplots ( 1 , 2 ) ax [ 0 ] . scatter ( x , y , c = \"gray\" ) ax [ 0 ] . plot ( x , lin_reg . fittedvalues , c = \"orange\" ) ax [ 0 ] . scatter ( x_new , y_new , c = \"crimson\" ) # seaborn automatically plots with confidence interval sns . regplot ( x = x , y = y , ax = ax [ 1 ]) # multiple linear regression X = np . array ( list ( zip ( standard_normal , shifted_normal ))) X # You need to add the column of ones to the inputs if you want statsmodels to calculate the intercept. It doesn’t take intercept into account by default. X = sm . add_constant ( X ) X # create a y axis to test linear regression y = ( 2.3 + 1.55 * standard_normal + 2.4 * shifted_normal + 5 * np . sin ( np . random . random ( size = nobs )) ) lin_reg = sm . OLS ( y , X ) . fit () lin_reg . summary () # get statistical properties using scipy stats module stats . describe ( standard_normal ) stats . describe ( shifted_normal ) # checking if the distributions are normal (we know they are) stats . normaltest ( standard_normal ) stats . normaltest ( shifted_normal ) # KS-test stats . kstest ( standard_normal , stats . norm . cdf ) stats . kstest ( shifted_normal , stats . norm . cdf ) # two-tailed test # finds if the given random numbers are from same distribution stats . ttest_ind ( standard_normal , shifted_normal ) pass ; count: number of observations in each bin Make sure to mention what the y-axis denotes. density: normalize such that the total area of the histogram equals 1 Histogram and KDE for 1000 random integers from [0,8]. In [2]: import pandas as pd import numpy as np from scipy import stats import matplotlib.pyplot as plt import seaborn as sns import statsmodels.api as sm # fix seed to get reproducible results # scipy.stats uses np.random to generate its random numbers, so setting seed using numpy works. (ref: https://stackoverflow.com/questions/16016959/scipy-stats-seed) np . random . seed ( seed = 23 ) nobs = 1000 # number of observations standard_normal = np . random . normal ( loc = 0.0 , scale = 1.0 , size = nobs ) shifted_normal = np . random . normal ( loc = 0.5 , scale = 1.0 , size = nobs ) plt . hist ( standard_normal , bins = 30 , histtype = \"step\" , label = \"standard normal\" ) plt . hist ( shifted_normal , bins = 30 , histtype = \"step\" , label = \"shifted normal\" ) plt . legend () # a quick way to fit a linear regression model using statmodels # ref: https://realpython.com/linear-regression-in-python/#advanced-linear-regression-with-statsmodels # simple linear regression # You need to add the column of ones to the inputs if you want statsmodels to calculate the intercept. It doesn’t take the intercept into account by default. x = standard_normal X = sm . add_constant ( standard_normal ) # print(x[:5], X[:5]) # create a y axis to test linear regression with an error term y = 2.3 + 1.55 * x + 2.3 * np . random . exponential ( size = nobs ) # be sure to provide X, not x lin_reg = sm . OLS ( y , X ) . fit () print ( lin_reg . summary ()) # you can get the predicted values using lin_reg.fittedvalues as used in the plot below. you can also use the model to predict y for new x values. x_new = np . linspace ( - 3 , 3 , 5 ) X_new = sm . add_constant ( x_new ) y_new = lin_reg . predict ( X_new ) # print(x_new,y_new) fig , ax = plt . subplots ( 1 , 2 ) ax [ 0 ] . scatter ( x , y , c = \"gray\" ) ax [ 0 ] . plot ( x , lin_reg . fittedvalues , c = \"orange\" ) ax [ 0 ] . scatter ( x_new , y_new , c = \"crimson\" ) # seaborn automatically plots with confidence interval sns . regplot ( x = x , y = y , ax = ax [ 1 ]) # multiple linear regression X = np . array ( list ( zip ( standard_normal , shifted_normal ))) X # You need to add the column of ones to the inputs if you want statsmodels to calculate the intercept. It doesn’t take intercept into account by default. X = sm . add_constant ( X ) X # create a y axis to test linear regression y = ( 2.3 + 1.55 * standard_normal + 2.4 * shifted_normal + 5 * np . sin ( np . random . random ( size = nobs )) ) lin_reg = sm . OLS ( y , X ) . fit () lin_reg . summary () # get statistical properties using scipy stats module stats . describe ( standard_normal ) stats . describe ( shifted_normal ) # checking if the distributions are normal (we know they are) stats . normaltest ( standard_normal ) stats . normaltest ( shifted_normal ) # KS-test stats . kstest ( standard_normal , stats . norm . cdf ) stats . kstest ( shifted_normal , stats . norm . cdf ) # two-tailed test # finds if the given random numbers are from same distribution stats . ttest_ind ( standard_normal , shifted_normal ) pass ; frequency: number of observations divided by the bin width probability or proportion: normalize such that bar heights sum to 1 See: Seaborn histplot documentation OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.262\nModel:                            OLS   Adj. R-squared:                  0.261\nMethod:                 Least Squares   F-statistic:                     354.1\nDate:                Thu, 04 Sep 2025   Prob (F-statistic):           7.79e-68\nTime:                        14:19:30   Log-Likelihood:                -2342.7\nNo. Observations:                1000   AIC:                             4689.\nDf Residuals:                     998   BIC:                             4699.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          4.5249      0.080     56.637      0.000       4.368       4.682\nx1             1.5599      0.083     18.816      0.000       1.397       1.723\n==============================================================================\nOmnibus:                      822.687   Durbin-Watson:                   1.998\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            27895.609\nSkew:                           3.536   Prob(JB):                         0.00\nKurtosis:                      27.890   Cond. No.                         1.08\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
  },
  {
    "title": "Logic",
    "url": "https://arghyadutta.github.io/notebooks/logic.html",
    "content": "Recommended Weston, A. A Rulebook for Arguments , Fifth edition.; Hackett Publishing Company, Inc: Indianapolis; Cambridge, 2017. Almossawi, A. An Illustrated Book of Bad Arguments , Illustrated edition. Experiment, 2014. ( Available online ) Halbach, V. The Logic Manual ; Oxford University Press: Oxford; New York, 2010. If I Get Time Herbert, E. A Mathematical Introduction to Logic , 2nd edition.; Elsevier India, 2014"
  },
  {
    "title": "Philosophy of Science",
    "url": "https://arghyadutta.github.io/notebooks/philosophyOfScience.html",
    "content": "Recommended Alan, C. (2013). What Is This Thing Called Science? McGraw-Hill Education (UK). Courses on YouTube If I get time Philosophy of Science by Paul Hoyningen Karl Popper - Science: Conjectures and Refutations by Victor Gijsbers. Humphreys, P. (Ed.). (2019). The Oxford Handbook of Philosophy of Science (Reprint edition). Oxford University Press. Good description of of the main school of thoughts, especially the theories of Popper (falsification) and Kuhn (paradigm shift)."
  },
  {
    "title": "On music",
    "url": "https://arghyadutta.github.io/notebooks/music.html",
    "content": "Recommended Informative and sometimes fun, but the overall snobbish tone ruined it for me. Ayyangar, R. R. (2019). History of South Indian (Carnatic) Music , Third edition. Vipanci Charitable Trust. TMK has strong opinions, and I sometimes disagree with him, but the book is informative, empathetic, and well-written. Recommended. Incidentally, an organization—Kaahon—has a YouTube channel and they produced a couple of good documentaries on instrument makers from Bengal, like this one on Rudra Veena makers and this one on Sitar makers. Their tales are similar to the mridangam makers—especially the lack of recognition. The list is chronologically ordered. Ethnographic studies of choirs in Shillong and Goa to explore choral singing and postcolonial musical practices, highlighting the interplay of individual and collective identities. The tone is academic, yet it's accessible and well-written—a fairly rare occurrence. Krishna, T. M. (2022). Sebastian & Sons: A Brief History of Mrdangam Makers . Westland Publications Limited. Chatterjee, S. (2023). Choral Voices: Ethnographic Imaginations of Sound and Sacrality . Bloomsbury Publishing USA. Dhar, S. (2017). Raga'N Josh . The Orient Blackswan. Sheila Dhar’s witty reflections on the world of Indian classical music. Some descriptions are unforgettable, like when Pandit Pran Nath tuned a Tanpura for over an hour until it sounded just perfect. Mukhaopadhyay, K. P. (2014). Kudrat Rangibirangi (1st edition). Ananda Publishers. (In Bengali)"
  },
  {
    "title": "Irrationality and Stupidity",
    "url": "https://arghyadutta.github.io/notebooks/stupidity.html",
    "content": "Recommended Frankfurt, H. G. (2005). On Bullshit (1st edition). Princeton University Press. If I Get Time Bonhoeffer's Theory of Stupidity"
  },
  {
    "title": "Title",
    "url": "https://arghyadutta.github.io/notebooks/template.html",
    "content": ""
  },
  {
    "title": "Ursula K. Le Guin",
    "url": "https://arghyadutta.github.io/notebooks/leguin.html",
    "content": "Recommended Guin, U. K. L. (2018). The Books of Earthsea . Gollancz. Guin, U. K. L. (2016). A Wizard of Earthsea . Puffin. Le Guin's prose flows, interlaced with intricate, compassionate thoughts."
  },
  {
    "title": "Silence",
    "url": "https://arghyadutta.github.io/notebooks/silence.html",
    "content": "Recommended Pyrah, S. (2025, February 2). Quiet, please! The remarkable power of silence – for our bodies and our minds . The Guardian . Bold Books and Bones (Director). (2022, April 23). What do you hear when you find silence? YouTube . I hear the silence now, Alive within its heart Are the sounds that can not be heard That the ear may not dispart? IntoGreatSilence (Director). (2007, February 21). Official Into Great Silence US Trailer YouTube . An astounding documentary! From the description: \"Nestled deep in the postcard-perfect French Alps, the Grande Chartreuse is considered one of the world's most ascetic monasteries. In 1984, German filmmaker Philip Gröning wrote to the Carthusian order for permission to make a documentary about them. They said they would get back to him. Sixteen years later, they were ready. Gröning, sans crew or artificial lighting, lived in the monks' quarters for six months—filming their daily prayers, tasks, rituals and rare outdoor excursions. This transcendent, closely observed film seeks to embody a monastery, rather than simply depict one—it has no score, no voiceover and no archival footage. What remains is stunningly elemental: time, space and light. One of the most mesmerizing and poetic chronicles of spirituality ever created, INTO GREAT SILENCE dissolves the border between screen and audience with a total immersion into the hush of monastic life. More meditation than documentary, it's a rare, transformative theatrical experience for all.\" If I Get Time Shepherd, N., & Macfarlane, R. (2019). In the Cairngorms (UK ed. edition). Galileo Publishers. So if I bend my ear To silence, I grown aware The stir of sounds I have almost heard That are not quite there. On a Still Morning Nan Shepherd As white light gathers all –\nThe rose and the amethyst, The ice-green and the copper-green, The peacock blue and the mist – Robert Gershon (Director). (2022, March 29). And They Kept Silence YouTube . Shepherd, N. (2008). The living mountain: A celebration of the Cairngorm Mountains of Scotland . Edinburgh: Canongate. ( Archive )"
  },
  {
    "title": "Few well-written papers",
    "url": "https://arghyadutta.github.io/notebooks/wellWrittenPapers.html",
    "content": "Binder, K. Theory of First-Order Phase Transitions. Rep. Prog. Phys. 1987, 50 (7), 783. link . Neidhardt, F. C. Bacterial Growth: Constant Obsession with dN/Dt. J Bacteriol 1999, 181 (24), 7405–7408. link . Bauer, M.; Bialek, W. Information Bottleneck in Molecular Sensing. PRX Life 2023, 1 (2), 023005. link . Bialek, W. Physical Limits to Sensation and Perception. Annu. Rev. Biophys. Biophys. Chem. 1987, 16 (1), 455–478. link . Overbeek, J. T. G.; Voorn, M. J. Phase Separation in Polyelectrolyte Solutions. Theory of Complex Coacervation. Journal of Cellular and Comparative Physiology 1957, 49 (S1), 7–26. link . Some papers that I liked for their prose; of course, the content is great, too! Galstyan, V.; Phillips, R. Allostery and Kinetic Proofreading. J. Phys. Chem. B 2019, 123 (51), 10990–11002. link . Braberg, H.; Echeverria, I.; Kaake, R. M.; Sali, A.; Krogan, N. J. From Systems to Structure — Using Genetic Data to Model Protein Structures. Nat Rev Genet 2022, 23 (6), 342–354. link . van de Schoot, R.; Depaoli, S.; King, R.; Kramer, B.; Märtens, K.; Tadesse, M. G.; Vannucci, M.; Gelman, A.; Veen, D.; Willemsen, J.; Yau, C. Bayesian Statistics and Modelling. Nat Rev Methods Primers 2021, 1 (1), 1–26. link . Ghosh, A.; Radhakrishnan, J.; Chaikin, P. M.; Levine, D.; Ghosh, S. Coupled Dynamical Phase Transitions in Driven Disk Packings. Phys. Rev. Lett. 2022, 129 (18), 188002. link . I liked how concise and informative the abstract is! Deshpande, N. S.; Furbish, D. J.; Arratia, P. E.; Jerolmack, D. J. The Perpetual Fragility of Creeping Hillslopes. Nat Commun 2021, 12 (1), 3909. link . Thurston, W. P. On Proof and Progress in Mathematics. arXiv March 31, 1994. link . Mandelbrot, B. How Long Is the Coast of Britain? Statistical Self-Similarity and Fractional Dimension. Science 1967, 156 (3775), 636–638. link . Greener, Joe G., Shaun M. Kandathil, Lewis Moffat, and David T. Jones. “A Guide to Machine Learning for Biologists.” Nature Reviews Molecular Cell Biology 23, no. 1 (January 2022): 40–55. link . Cubuk, J.; Soranno, A. Macromolecular Crowding and Intrinsically Disordered Proteins: A Polymer Physics Perspective. ChemSystemsChem 2022. link . Park, J. J.; Lu, Y.-K.; Jamison, A. O.; Tscherbul, T. V.; Ketterle, W. A Feshbach Resonance in Collisions between Triplet Ground-State Molecules. Nature 2023, 614 (7946), 54–58. link . Another precise and well-written abstract! Fjelland, R. Why General Artificial Intelligence Will Not Be Realized. Humanit Soc Sci Commun 2020, 7 (1), 10. link . Jo, M. H.; Meneses, P.; Yang, O.; Carcamo, C. C.; Pangeni, S.; Ha, T. Determination of Single-Molecule Loading Rate during Mechanotransduction in Cell Adhesion. Science 2024, 383 (6689), 1374–1379. link Guven, J.; Manrique, G. Arresting the Collapse of a Catenary Arch. arXiv:1710.03433 [cond-mat, physics:physics] 2017. Marder, E. Grandmother Elephants. eLife 2013, 2, e01140. link . Foley, S.; Deserno, M. Stabilizing Leaflet Asymmetry under Differential Stress in a Highly Coarse-Grained Lipid Membrane Model. J. Chem. Theory Comput. 2020, 16 (11), 7195–7206. link . Luttinger, J. M. An Exactly Soluble Model of a Many‐Fermion System. Journal of Mathematical Physics 1963, 4 (9), 1154–1162. link . Well-written papers Deserno, M. Fluid Lipid Membranes – a Primer link Cohen, A. E.; Shi, Z. Do Cell Membranes Flow Like Honey or Jiggle Like Jello? BioEssays 2020, 42 (1), 1900142. link . Sing, C. E.; Perry, S. L. Recent Progress in the Science of Complex Coacervation. Soft Matter 2020, 16 (12), 2885–2914. link . Efron, B.; Halloran, E.; Holmes, S. Bootstrap Confidence Levels for Phylogenetic Trees. Proceedings of the National Academy of Sciences 1996, 93 (23), 13429–13429. link . Powers, A. S.; Pham, V.; Burger, W. A. C.; Thompson, G.; Laloudakis, Y.; Sexton, P. M.; Paul, S. M.; Christopoulos, A.; Thal, D. M.; Felder, C. C.; Valant, C.; Dror, R. O. Structural Basis of Efficacy-Driven Ligand Selectivity at GPCRs. Nat Chem Biol 2023, 1–10. link . Everaers, R.; Sukumaran, S. K.; Grest, G. S.; Svaneborg, C.; Sivasubramanian, A.; Kremer, K. Rheology and Microscopic Topology of Entangled Polymeric Liquids. Science 2004, 303 (5659), 823–826. link . Jia, X.; Lynch, A.; Huang, Y.; Danielson, M.; Lang’at, I.; Milder, A.; Ruby, A. E.; Wang, H.; Friedler, S. A.; Norquist, A. J.; Schrier, J. Anthropogenic Biases in Chemical Reaction Data Hinder Exploratory Inorganic Synthesis. Nature 2019, 573 (7773), 251–255. link . Tanaka, H.; Tong, H.; Shi, R.; Russo, J. Revealing Key Structural Features Hidden in Liquids and Glasses. Nat Rev Phys 2019, 1 (5), 333–348. link . França, T. F. A.; Monserrat, J. M. Writing Papers to Be Memorable, Even When They Are Not Really Read. BioEssays 2019, 41 (5), 1900035. link ."
  },
  {
    "title": "Fiction",
    "url": "https://arghyadutta.github.io/notebooks/fiction.html",
    "content": "Recommended Krystal, A. (2022), What’s the Deal, Hummingbird? , The New Yorker আমি গ্রামের ছেলে, কিন্তু সত্যিকারের ভালো একটা কিশোরসাহিত্য যে গ্রাম-শহরের সঙ্কীর্ণ ভূগোলের সীমা হেলায় অতিক্রম করে যেতে পারে তার প্রমাণ দেয় এই বই: খাস কোলকাতা শহরের চার কিশোরের adventure (এবং, আর বেশি misadventure) গুলো থেকে মজা পেতে আমার কোন অসুবিধা হয় নি। এই বইতে পাওয়া নারায়ণ গঙ্গোপাধ্যায়ের অধিকাংশ গল্পগুলিই নির্ভেজাল হাসির, অনাবিল আনন্দের উৎস: …কিন্তু পিসিমা ধোপার হিসেবের খাতায় এইসব দেখে ভীষণ রেগে গেল! রেগে গিয়ে হাতের কাছে আর কিছু না পেয়ে একটা চালকুমড়ো নিয়ে ফুচুদাকে তাড়া করলে। ঠিক যেন গদা হাতে নিয়ে শাড়িপরা ভীম দৌড়চ্ছে। তাই এই বই ক্লাসে প্রথম হবার দৌড়ে ক্লান্ত আজকের কিশোর-কিশোরীদের অবশ্যপাঠ্য। আর মজা ছাড়াও এই বই থেকে বাস্তব-জীবনে কাজে লাগানোর মত অনেক কিছু শেখা যায়। একটি নমুনা: Gangopadhyay, N. (2014). Samagra Kishor Sahitya (1st edition). Ananda Publishers. (Bengali) হেমেন্দ্রকুমার রায় রচনাবলী (১৭) Wood, J. (2013), Becoming them , The New Yorker More on it নিঃসন্দেহে আমার পড়া অন্যতম শ্রেষ্ঠ কিশোরসাহিত্য। ক্রিকেট খেলতে গিয়ে দেরি করে বাড়ি ফিরে মায়ের বকুনি খাওয়া, কিংবা জামরুল পারতে গিয়ে কাঠপিঁপড়ের কামড় খাওয়ার মত ছোটবেলার বিভিন্ন স্মৃতির সাথে এই বইটা পড়ার সুখস্মৃতি এমনভাবে জড়িয়ে আছে যে আজ আর আমার ছোটবেলাকে এই বইটার থেকে আলাদা করে ভাবতে পারি না। টেনিদার সাথে আমার প্রথম পরিচয় কোন একটা ক্লাসে প্রথম হবার দৌলতে দিদার উপহার দেওয়া টেনিদা সমগ্রর মাধ্যমে। আমি গ্রামের ছেলে, কিন্তু সত্যিকারের ভালো একটা কিশোরসাহিত্য যে গ্রাম-শহরের সঙ্কীর্ণ ভূগোলের সীমা হেলায় অতিক্রম করে যেতে পারে তার প্রমাণ দেয় এই বই: খাস কোলকাতা শহরের চার কিশোরের adventure (এবং, আর বেশি misadventure) গুলো থেকে মজা পেতে আমার কোন অসুবিধা হয় নি। এই বইতে পাওয়া নারায়ণ গঙ্গোপাধ্যায়ের অধিকাংশ গল্পগুলিই নির্ভেজাল হাসির, অনাবিল আনন্দের উৎস: ...টেনিদা তবু হাঁড়িটাকে ছাড়ে না। শেষকালে মুখের ওপর তুলে চোঁ করে রসটা পর্যন্ত নিকেশ করে দিলে। তারপর নাক-টাক কুঁচকে বললে, দুত্তোর, গোটাকয়েক ডেয়ো পিঁপড়েও খেয়ে ফেললুম রে! জ্যান্তও ছিল দু’-তিনটে! পেটের ভেতরে গিয়ে কামড়াবে না তো? হাবুল বললে, কামড়াইতেও পারে। কামড়াক গে, বয়ে গেল! একবার ভীমরুল-সুদ্ধ একটা জামরুল খেয়ে ফেলেছিলুম, তা সে-ই যখন কিছু করতে পারলে না, তখন ক’টা পিঁপড়েতে আর কী করবে! তাই এই বই ক্লাসে প্রথম হবার দৌড়ে ক্লান্ত আজকের কিশোর-কিশোরীদের অবশ্যপাঠ্য। আর মজা ছাড়াও এই বই থেকে বাস্তব-জীবনে কাজে লাগানোর মত অনেক কিছু শেখা যায়। একটি নমুনা: হুঁ, কবি হওয়া খুব খারাপ। আমার পিসতুতো ভাই ফুচুদা একবার কবি হয়েছিলো। দিনরাত কবিতা লিখত। একদিন রামধন ধোপার খাতায় কবিতা করে লিখল : পাঁচখানা ধুতি, সাতখানা শাড়ি এ-সব হিসেবে হইবে কিবা? এ-জগতে জীব কত ব্যাথা পায়, তাই ভাবি আমি রাত্রি-দিবা। রামধনের ওই বৃদ্ধ গাধা মনটি তাহার বড়ই সাদা- সে-বেচারা তার পিঠেতে চাপায়ে কত শাড়ি-ধুতি-প্যান্ট লইয়া যায়- মনোদুখে খালি বোঝা টেনে ফেরে গাধা একখানা ধুতি-প্যান্ট পরিতে না পায়! …কিন্তু পিসিমা ধোপার হিসেবের খাতায় এইসব দেখে ভীষণ রেগে গেল! রেগে গিয়ে হাতের কাছে আর কিছু না পেয়ে একটা চালকুমড়ো নিয়ে ফুচুদাকে তাড়া করলে। ঠিক যেন গদা হাতে নিয়ে শাড়িপরা ভীম দৌড়চ্ছে। তাই আর দেরি না করে বইটা পড়ে ফেলুন আর যদি পড়া হয়ে গিয়ে থাকে তো আরও একবার পড়ে নিন; প্রাণখোলা হাসি আপনাকে সুস্থ রাখবে। পাঁচখানা ধুতি, সাতখানা শাড়ি এ-সব হিসেবে হইবে কিবা? এ-জগতে জীব কত ব্যাথা পায়, তাই ভাবি আমি রাত্রি-দিবা। রামধনের ওই বৃদ্ধ গাধা মনটি তাহার বড়ই সাদা- সে-বেচারা তার পিঠেতে চাপায়ে কত শাড়ি-ধুতি-প্যান্ট লইয়া যায়- মনোদুখে খালি বোঝা টেনে ফেরে গাধা একখানা ধুতি-প্যান্ট পরিতে না পায়! ক্রিকেট খেলতে গিয়ে দেরি করে বাড়ি ফিরে মায়ের বকুনি খাওয়া, কিংবা জামরুল পারতে গিয়ে কাঠপিঁপড়ের কামড় খাওয়ার মত ছোটবেলার বিভিন্ন স্মৃতির সাথে এই বইটা পড়ার সুখস্মৃতি এমনভাবে জড়িয়ে আছে যে আজ আর আমার ছোটবেলাকে এই বইটার থেকে আলাদা করে ভাবতে পারি না। টেনিদার সাথে আমার প্রথম পরিচয় কোন একটা ক্লাসে প্রথম হবার দৌলতে দিদার উপহার দেওয়া টেনিদা সমগ্রর মাধ্যমে। হেমেন্দ্র রায়ের লেখা বই গুলো একটানা পড়লে ভালো লাগে না, অন্তত আমার। তাই ওনার বইগুলো একটু রয়েসয়ে পড়ি। হেমেন্দ্রকুমার রচনাবলীর এই খণ্ডটা পড়ে আমার বেশ ভালো লাগল। এর মধ্যে প্রথম তিনটি উপন্যাস রয়েছে ডাকাত দীনবন্ধুকে নিয়ে। দীনবন্ধু এমন একজন ডাকাত যে, সৌভাগ্যবশত, নিজের নামের মানে বোঝে; তাই তার করা ডাকাতিকে মেনে নিতে লেখক বা পাঠক কারোরই অসুবিধা হয় না। দীনবন্ধুকে ধরার ব্যর্থ চেষ্টা করে বেরায় গোয়েন্দা প্রশান্তবাবু, যাকে বুদ্ধিমান দীনবন্ধু ডাকে অশান্তবাবু বলে। এই দুজনকে নিয়ে লেখা গল্পগুলো বেশ সুখপাঠ্য। বইয়ের শেষ দুটি উপন্যাস জয়ন্ত, মানিক ও সুন্দরবাবুকে নিয়ে লেখা। এই দুটিতে রক্তপাতের কিছু বাড়াবাড়ি থাকলেও তা মাত্রা ছাড়িয়ে যায় না — তাই সব মিলিয়ে বইটা পড়তে বেশ ভালই লাগে। আজকালকার ছেলেমেয়েরা পড়লে তাদের খারাপ লাগবে বলে মনে হয় না। নিঃসন্দেহে আমার পড়া অন্যতম শ্রেষ্ঠ কিশোরসাহিত্য। More হেমেন্দ্র রায়ের লেখা বই গুলো একটানা পড়লে ভালো লাগে না, অন্তত আমার। তাই ওনার বইগুলো একটু রয়েসয়ে পড়ি। হেমেন্দ্রকুমার রচনাবলীর এই খণ্ডটা পড়ে আমার বেশ ভালো লাগল। এর মধ্যে প্রথম তিনটি উপন্যাস রয়েছে ডাকাত দীনবন্ধুকে নিয়ে। দীনবন্ধু এমন একজন ডাকাত যে, সৌভাগ্যবশত, নিজের নামের মানে বোঝে; তাই তার করা ডাকাতিকে মেনে নিতে লেখক বা পাঠক কারোরই অসুবিধা হয় না। দীনবন্ধুকে ধরার ব্যর্থ চেষ্টা করে বেরায় গোয়েন্দা প্রশান্তবাবু, যাকে বুদ্ধিমান দীনবন্ধু ডাকে অশান্তবাবু বলে। এই দুজনকে নিয়ে লেখা গল্পগুলো বেশ সুখপাঠ্য। বইয়ের শেষ দুটি উপন্যাস জয়ন্ত, মানিক ও সুন্দরবাবুকে নিয়ে লেখা। এই দুটিতে রক্তপাতের কিছু বাড়াবাড়ি থাকলেও তা মাত্রা ছাড়িয়ে যায় না — তাই সব মিলিয়ে বইটা পড়তে বেশ ভালই লাগে। আজকালকার ছেলেমেয়েরা পড়লে তাদের খারাপ লাগবে বলে মনে হয় না।"
  },
  {
    "title": "Academic Biographies",
    "url": "https://arghyadutta.github.io/notebooks/biography.html",
    "content": "Recommended If you want to read a book about a physicist who did brilliant work, made her way through the jungle of academia to the top, and fought against daily, casual sexism from her colleagues, friends, and family members, and yet somehow managed not to hate them, read this book. You won't be disappointed. Doxiadēs, A. K. (2001). Uncle Petros and Goldbach’s Conjecture: A Novel of Mathematical Obsession . Bloomsbury USA. Gennes, P.-G. de. (2004). Petit point: A candid portrait on the aberrations of science . World Scientific Publications Ulam, S. M. (1991). Adventures of a Mathematician . University of California Press. Archive Story of an unusual mathematician, Grigory Perelman. One of the best profiles that I've read in The New Yorker. A playlist with an extended interview of an intriguing academic: Freeman Dyson. Balaram P. (2017) Memories of a Bangalore Quartet . IISc Connet Hoad, P. (2024). ‘He was in mystic delirium’: was this hermit mathematician a forgotten genius whose ideas could transform AI – or a lonely madman? . Phil Hoad; Guardian The end life of Grothendieck. A series where Rob Phillips is interviewed by David Zierler for the Caltech Heritage project. Inspiring and bold! More on it As a physicist, it is easy—very easy—to suffer from mild to severe imposter syndrome. Physics is full of brilliant people, often leading one to feel like the only person who doesn’t understand an otherwise “simple” idea that everyone else seems to have seamlessly mastered. I read these biographies as a cure for these bouts of doubt. For me, they often show how exhilarating is the joy of understanding a natural phenomenon; the reason we started doing physics in the first place. This simple joy often gets overlooked when you have to apply for your next postdoc position, next faculty position, or apply for grants. Also, these books show how developing any substantial idea can be very time-consuming, in no sense straightforward, and taxing. This book is exceptionally well-written from both of these perspectives. Mary K. Gaillard is Berkeley's first tenured female faculty. Getting that position as a male physicist was not easy, as a female physicist it was close to impossible. She made it possible. And yet the book's tone is not accusatory; she is not into the blame game. She never said anything about why she and her first husband were divorced; she simply said it was very personal and did not think it was relevant for the aspiring female physicists, her target audience. She also was brutally honest about many incidents that were demeaning to her in many ways; she frequently quoted big names saying those horrible things to her. But when she turned out to be wrong, she admitted. The frankness is admirable. If you want to read a book about a physicist who did brilliant work, made her way through the jungle of academia to the top, and fought against daily, casual sexism from her colleagues, friends, and family members, and yet somehow managed not to hate them, read this book. You won't be disappointed. Ray, S., Spangenburg, R., & Moser, D. K. (1995). Niels Bohr: Gentle genius of Denmark . Facts on File. More on it A short and delightful book by Pierre-Gilles de Gennes, a French Nobel laureate in Physics. He had a distinctive research style. He could, somehow , distill essential insights from complex physical phenomena, and then present them with minimal equations and clear, concise prose. This particular book, however, is not about physics. It is written as fables about imaginary physicists navigating an all-too-real world. The stories are thought-provoking, controversial, and candid. Having spent a fair amount of time in academia, I know how true most of them are. I recommend it to anyone interested in the social dynamics of scientists. Nasar, S., & Gruber, D. (2006, August 28). Manifold Destiny . The New Yorker . As a physicist, it is easy—very easy—to suffer from mild to severe imposter syndrome. Physics is full of brilliant people, often leading one to feel like the only person who doesn’t understand an otherwise “simple” idea that everyone else seems to have seamlessly mastered. I read these biographies as a cure for these bouts of doubt. For me, they often show how exhilarating is the joy of understanding a natural phenomenon; the reason we started doing physics in the first place. This simple joy often gets overlooked when you have to apply for your next postdoc position, next faculty position, or apply for grants. Also, these books show how developing any substantial idea can be very time-consuming, in no sense straightforward, and taxing. This book is exceptionally well-written from both of these perspectives. Mary K. Gaillard is Berkeley's first tenured female faculty. Getting that position as a male physicist was not easy, as a female physicist it was close to impossible. She made it possible. Polchinski, J. (2017). Memories of a Theoretical Physicist . arXiv:1708.09093 If I get time A short and delightful book by Pierre-Gilles de Gennes, a French Nobel laureate in Physics. He had a distinctive research style. He could, somehow , distill essential insights from complex physical phenomena, and then present them with minimal equations and clear, concise prose. This particular book, however, is not about physics. It is written as fables about imaginary physicists navigating an all-too-real world. The stories are thought-provoking, controversial, and candid. Having spent a fair amount of time in academia, I know how true most of them are. I recommend it to anyone interested in the social dynamics of scientists. Halmos, P. R. (1985). I Want to be a Mathematician . Springer New York. Gaillard, M. K. (2015). A singularly unfeminine profession: One woman’s journey in physics . World Scientific Publishing Company. Frisch, O. R. (1979). What little I remember . Cambridge University Press. Archive And yet the book's tone is not accusatory; she is not into the blame game. She never said anything about why she and her first husband were divorced; she simply said it was very personal and did not think it was relevant for the aspiring female physicists, her target audience. She also was brutally honest about many incidents that were demeaning to her in many ways; she frequently quoted big names saying those horrible things to her. But when she turned out to be wrong, she admitted. The frankness is admirable. A nice introduction to the life and times of the physicist Niels Bohr. Mlodinow, L. (2003). Feynman’s Rainbow: A Search for Beauty in Physics and in Life . Warner Books."
  },
  {
    "title": "Non-linear Dynamics",
    "url": "https://arghyadutta.github.io/notebooks/nld.html",
    "content": "Krishnaswami, G. S. (2020). Notes on Nonlinear Dynamics . PDF Recommended Percival, I.; Richards, D. Introduction to Dynamics; Cambridge University Press: Cambridge ; New York, 1982. The chapter, chapter 13, on nonlinear differential equations is useful. Especially I liked his approach where he first introduced critical points and paths for linear systems, and then for nonlinear systems. This helped me. Ross, S. L. (2004). Differential equations (Third edition, Wiley student edition). Wiley India. Concise. Strogatz, S. H. (2015). Nonlinear dynamics and chaos: With applications to physics, biology, chemistry, and engineering (Second edition). Westview Press. Excellent book."
  },
  {
    "title": "Probability and Statistics",
    "url": "https://arghyadutta.github.io/notebooks/statistics.html",
    "content": "Neat. Particularly nice discussion on the confidence interval. Recommended MIT RES.6-012 Introduction to Probability, Spring 2018 Beyond Multiple Linear Regression: Applied Generalized Linear Models and Multilevel Models in R by Paul Roback and Julie Legler The Book of Statistical Proofs by Joram Soch and collaborators (it's an open book on GitHub) A placeholder note for both topics. Get the sixth edition, especially if you're buying in India. It's excellent and relatively cheap (~INR. 800, August 2025). Also, this edition provides code examples in R, a welcome change. The Little Handbook of Statistical Practice by Gerard E. Dallal Introduction to Probability, Statistics, and Random Processes by Hossein Pishro-Nik Holmes, S., & Huber, W. (2019). Modern statistics for modern biology . Cambridge university press. ( Free online copy ) A User’s Guide to Statistical Inference and Regression by Matthew Blackwell A YouTube playlist with beginner friendly lectures by John Tsitsiklis and Patrick Jaillet from MIT. They also wrote a book on probability and made a summary of it freely available . Undergraduate Probability and High-Dimensional Probability by Roman Vershynin, on YouTube. Regression and Other Stories by Andrew Gelman, Jennifer Hill, Aki Vehtari Ross, S. (2021) Introduction to Probability and Statistics for Engineers and Scientists , Sixth edition.; Elsevier, 2021."
  },
  {
    "title": "Biophysics",
    "url": "https://arghyadutta.github.io/notebooks/biophysics.html",
    "content": "Books Molecular biophysics by M. V. Volkenshtein Physical Biology of the Cell by Rob Phillips. An introductory biophysics course by Ali Hassanali. Theory of the stability of lyophobic colloids by Verwey and Overbeek Protein physics by Finkelstein and Ptitsyn Courses on YouTube Introduction to Neuroscience by Bing Wen Brunton. Ionic Solution Theory by H. Friedman and I. Prigogine Modelling dynamic phenomena in molecular and cellular biology by Lee Segel Tutorials on PyMOL from Molecular Memory. Molecular Biophysics by Erik Lindahl. Biophysics by Walter Hoppe, Wolfgang Lohmann, Hubert Markl, Hubert Ziegler Introduction to Proteomics ."
  },
  {
    "title": "Simpson's Paradox",
    "url": "https://arghyadutta.github.io/notebooks/simpsonsParadox.html",
    "content": "Recommended Pearl, J. (2016). Simpson’s Paradox: The riddle that would not die. Blog Post Bookbinder, H. (2025, April 3). Simpson’s Paradox Explains the World Scriptorium Philosophia Are subgroup discovery and Simpson's paradox related? I believe they are, though I haven't fully developed the idea yet. My thinking is that computing a global regression line can be misleading when the data contain strong local patterns that are not aligned with each other. I don't think this is something new, but it would be interesting to explore the idea a bit further."
  },
  {
    "title": "Mathematics for Quantum Mechanics",
    "url": "https://arghyadutta.github.io/notebooks/qMechMath.html",
    "content": "Recommended $\\rm{Tr}(A+B)=\\rm{Tr}(A)+\\rm{Tr}(B)$ The vectors in $\\mathcal{B}$ uniquely combine to generate vectors in $\\mathcal{V}$. (Of course! $2\\hat{i}+3\\hat{j}$ is a unique vector in the $\\hat{i}, \\hat{j}$ basis. For a proof, see p. 21 of Johnston 2021). Tensor products of (Hermitian, unitary, positive, projector) operators retain those properties. Johnston, N. (2021). Advanced Linear and Matrix Algebra . Springer International Publishing. $\\ket{v}\\otimes(\\ket{w_1}+\\ket{w_2}) = \\ket{v}\\otimes\\ket{w_1} + \\ket{v}\\otimes\\ket{w_2}$ Some properties of the adjoint operator: Examples of vector spaces include $\\mathbb{R}^n$, $\\mathbb{C}^n$, $\\mathcal{M}_{m,n}(\\mathbb{F})$ (space of $m\\times n$ matrices with elements from the field $\\mathbb{F}$). One important point is when it is said that $\\mathcal{V}$ is a vector space over a field $\\mathbb{F}$, it means that the scalars used in the scalar multiplication comes from $\\mathbb{F}$ (also known as a ground field ), not that the elements of the vectors are from $\\mathbb{F}$. For example Hermitian matrices can be defined over $\\mathbb{R}$ or $\\mathbb{C}$, though its elements are from $\\mathbb{C}$. This is important since, for example, the space of $n\\times n$ Hermitian matrices $\\mathcal{M}^{\\rm H}_n$ is not a vector space over $\\mathbb{C}$, but it is a vector space over $\\mathbb{R}$. Example: $A=\\begin{pmatrix} 0 1 \\\\ 1 0 \\end{pmatrix} \\in \\mathcal{M}^{\\rm H}_2$ but since $(iA)^\\dagger\\neq iA$, it's not Hermitian, so $\\mathcal{M}^{\\rm H}_2$ isn't closed under scalar multiplication over $\\mathbb{C}$. Distributivity: $(c+d)\\ket{v}=c\\ket{v}+d\\ket{v}$ If $\\ket{i}$ and $\\ket{j}$ are orthonormal bases in $\\mathcal{V}$ and $\\mathcal{W}$ then $\\ket{i}\\otimes \\ket{j}$ is a basis for $\\mathcal{V}\\otimes \\mathcal{W}$. Linear Operators $\\rm{Tr}(A\\ket{\\psi}\\bra{\\psi})=\\bra{\\psi}{A}\\ket{\\psi}$ Useful Bases Subspace Incomplete, brief, and evolving \"lookup table\" on quantum mechanics for personal use, closely following Nielsen and Chuang (2010) and others. There may be typos (even conceptual errors!). If you find one, please tell me about it (argphy@gmail.com). Also, feel free to use the material for your own use. Associativity: $(\\ket{v}+\\ket{w})+\\ket{x}=\\ket{v}+(\\ket{w}+\\ket{x})$ Example: the set of non-invertible $2 \\times 2$ matrices is *not* a subspace of $\\mathcal{M}_2$. $\\begin{smallmatrix}1 0 \\\\ 0 0\\end{smallmatrix}$ and $\\begin{smallmatrix} 0 0 \\\\ 0 1\\end{smallmatrix}$ are non-invertible but their sum $\\begin{smallmatrix}1 0 \\\\ 0 1\\end{smallmatrix}$ is invertible. $P=\\sum_{i=1}^k \\ket{i}\\bra{i}$ Mermin, N. D. (2007). Quantum Computer Science: An Introduction . Cambridge University Press. $\\frac{1}{\\sqrt{2}}(\\ket{0}-\\ket{1}) = \\frac{1}{\\sqrt{2}}\\begin{bmatrix}1 \\\\-1\\end{bmatrix} \\equiv \\ket{-}$ Nielsen, M. A., & Chuang, I. L. (2010). Quantum computation and quantum information (10th anniversary ed). Cambridge University Press. Gram–Schmidt method is used to make a linearly independent set of vectors an orthonormal basis set. This is done by normalizing the first vector from the linearly independent set to make it the first basis element, and then progressively making the other vectors orthonormal by removing the projections. Wikipedia has a neat illustration, check it. Also Savov (2020) explained it well. Todo: Add a better synopsis later. Closure under addition: $\\ket{v} + \\ket{w} \\in \\mathcal{V}$ A set of vectors $\\mathcal{B} \\subseteq \\mathcal{V}$ is linearly dependent if there exists a set of scalars $c_1,c_2,\\cdots,c_k$, not all zeros, such that $c_i\\ket{v}*i=\\ket{0}$. $\\mathcal{B}$ is linearly independent if it is not linearly dependent. (It's funny 😄, but see the example for what is means.) Spectral Decomposition First, a definition. The adjoint/Hermitian conjugate of an operator $A$ is the unique operator $A^\\dagger$ that satisfies the relation $(\\ket{v},A\\ket{w})=(A^\\dagger \\ket{v},\\ket{w})$ for any $\\ket{v}$ and $\\ket{w}$. For a positive operator : $(\\ket{v},A\\ket{v})\\geq 0$. If it is positive, then $A$ is called positive definite. $A^\\dagger A$ is positive for any operator $A$. $P_i P_j = \\delta_{ij}P_i$ Identity and Pauli Matrices Linear Combination Cohen Tannoudji (2020) (pp. 103–108) and Mermin (2007) pp. 159–164 for careful definition of bra vectors Levi-Civita Some Useful Relations (incomplete, like other good things in life) Commutativity: $\\ket{v} + \\ket{w} = \\ket{w} + \\ket{v}$ Spanning set need not be unique . $\\begin{smallmatrix}1 \\\\1 \\end{smallmatrix}$ and $\\begin{smallmatrix}1 \\\\-1 \\end{smallmatrix}$ span $\\mathbb{C}^2$, too. Also, $\\hat{i}$ and $\\hat{j}$ and $45$° anti-clockwise-rotated vectors $(\\hat{i}+\\hat{j})$ and $(-\\hat{i}+\\hat{j})$ both spans $\\mathbb{R}^2$. $\\{I,X,Y,Z\\}$ spans $\\mathcal{M}_2(\\mathbb{C})$. Distributivity: $c(\\ket{v}+\\ket{w}) = c\\ket{v} +c\\ket{w}$ $\\frac{1}{\\sqrt{2}}(\\ket{0}-i\\ket{1} = \\frac{1}{\\sqrt{2}}\\begin{bmatrix}1 \\\\-i\\end{bmatrix} \\equiv \\ket{y_-}$ $1\\ket{v}=\\ket{v}$ Cohen-Tannoudji, C., Diu, B., & Laloë, F. (2020). Quantum mechanics. Volume 1: Basic concepts, tools, and applications (S. Reid Hemley, N. Ostrowsky, & D. Ostrowsky, Trans.; Second edition). Wiley-VCH Verlag GmbH & Co. KGaA. Linear operators on $\\mathcal{V}\\otimes \\mathcal{W}$ : If $A: \\mathcal{V} \\rightarrow \\mathcal{V}'$ and $B: \\mathcal{W} \\rightarrow \\mathcal{W}'$ are two linear operators, then $A\\otimes B: \\mathcal{V}\\otimes \\mathcal{W} \\rightarrow \\mathcal{V}' \\otimes \\mathcal{W}'$ is defined as Orthonormal Basis Set $\\rm{Tr}(zA)=z\\rm{Tr}(A)$ Span Computational Basis Sadun, L. A. (2008). Applied linear algebra: The decoupling principle (2nd ed). American Mathematical Society. Linear Dependence and Independence Outer-product representation reduces to diagonal representation (eigendecomposition/orthonormal decomposition) when $\\ket{v_i}, \\ket{w_j}$ are orthonormal eigenvalues of $A$. If $\\mathcal{V}$ and $\\mathcal{W}$ are $m$ and $n$ dimensional vector spaces, then $\\mathcal{V} \\otimes \\mathcal{W}$ (read as 'V tensor W') is an $mn$ dimensional vector space. Let $\\mathcal{V}$ be a set of elements (vectors) with two operations, addition and multiplication with a scalar from a field $\\mathbb{F}$ (typically $\\mathbb{C}$ in QM). $\\mathcal{V}$ is a called vector space if for vectors $\\ket{v},\\ket{w},\\ket{x} \\in \\mathcal{V}$ and scalars $c,d \\in \\mathbb{F}$, the following ten conditions are satisfied: $\\frac{1}{\\sqrt{2}}(\\ket{0}+\\ket{1}) = \\frac{1}{\\sqrt{2}}\\begin{bmatrix} 1 \\\\1 \\end{bmatrix} \\equiv\\ket{+}$ Interestingly, for any ket vector in $V$, there exists a bra vector in $V^*$; but for any bra vector, there may not exist a ket vector if the ket belongs to an infinite-dimensional Hilbert space (Cohen Tannoudji 2020 showed a counter-example). For finite-dimensional vector spaces, they are of the same size, and so you'll always get kets corresponding to bras, though. Existence of an 'inverse': a vector $-\\ket{v}$ such that $\\ket{v}+(-\\ket{v})=\\ket{0}$ Types of Linear Operators Inner product in a tensor-product space A map between two vector spaces $\\mathcal{V}$ and $\\mathcal{W}$, $A: \\mathcal{V} \\rightarrow \\mathcal{W}$, that preserves linearity $A (c_1 \\ket{v_1} + c_2 \\ket{v_2}) = c_1 A \\ket{v_1} + c_2 A \\ket{v_2}$. Generally, it has a matrix representation given by $A\\ket{v_j}=\\sum_i A_{ij}\\ket{w_i}$. The representation depends on the basis. Any vector of the form $\\sum^k_{i=1} c_i \\ket{v_i}$ is called a linear combination. Interesting: The Identity matrix *cannot* be written as a linear combination of Pauli matrices. $P^2 =P$ Normal operator : $A$ is normal if $A^\\dagger A = A A^\\dagger$. A normal matrix is Hermitian if and only if it has real eigenvalues. Eigenvectors of an Hermitian operators with distinct eigenvalues are orthogonal. Tensor Products Bra Vectors When we represent a vector in some basis, we order the basis set so that the vector can be written in terms of its 'coordinates' $[\\ket{v}]_\\mathcal{B} \\equiv (c_1,\\cdots,c_n)$. The coordinates depend on the bases . The number of vectors in a basis sets the dimension of the vector space. If $\\mathcal{V}$ has a basis with $n$ vectors, then any set of $m>n$ vectors will be linearly dependent and any set of $m < n$ vectors cannot span $\\mathcal{V}$. (p. 27, Johnston 2021) Vector Space Inner Product Sadun (2008) has a neat discussion in section 6.3 titled 'Bras, Kets, and Duality' pp. 152–156 $(\\ket{v_1}+\\ket{v_2})\\otimes\\ket{w}=\\ket{v_1}\\otimes\\ket{w}+\\ket{v_2}\\otimes\\ket{w}$ $\\rm{Tr}(AB)=\\rm{Tr}(BA)$ A basis set whose elements are orthonormal $\\bra{i},\\ket{j}=\\delta_{ij}$ (like unit vectors). They are complete: $\\sum_i \\ket{i}\\bra{i} = I$. Functions of Operators Savov, I. (2020). No Bullshit Guide to Linear Algebra (2nd V2.2 ed. edition). Minireference Co. Existence of a 'zero vector' $\\ket{0}$ such that $\\ket{v}+\\ket{0}=\\ket{v}$ Interestingly, if the eigenvectors of $A: \\mathcal{V} \\rightarrow \\mathcal{V}$ have non-degenerate eigenvalues, then they form a basis (eigenbasis) $A = \\sum_i \\lambda_i \\ket{i}\\bra{i}$. Example: our favorite Hamiltonian operator and it's energy eigenvalues: $H=\\sum_E E\\ket{E}\\bra{E}$ Notation: $\\ket{\\psi}^{\\otimes 2} \\equiv \\ket{\\psi}\\otimes \\ket{\\psi}$ Trace It obeys all properties of linear operators. Kronecker product as matrix representation for $A\\otimes B$ Eigenvectors Simultaneous Diagonalization Theorem All *finite* linear combinations of vectors of $\\mathcal{B} \\subseteq \\mathcal{V}$. $\\rm{span}(\\mathcal{B})$ is a subspace of $\\mathcal{V}$. If $\\rm{span}(\\mathcal{B})=\\mathcal{V}$ then $\\mathcal{V}$ is said to be spanned by $\\mathcal{B}$. For example, a vector spans a line and two non-parallel vectors spans $\\mathbb{R}^2$. A basis of a vector space $\\mathcal{V}$ is a set of vectors in $\\mathcal{V}$ that spans it and is linearly independent. Example, $\\begin{smallmatrix} 1 \\\\ 0 \\end{smallmatrix}$ and $\\begin{smallmatrix} 0 \\\\1 \\end{smallmatrix}$ spans $\\mathbb{C}^2$. Outer-product and Diagonal Representation $z(\\ket{v}\\otimes\\ket{w})=(z\\ket{v})\\otimes\\ket{w}=\\ket{v}\\otimes(z\\ket{w})$ $\\frac{1}{\\sqrt{2}}(\\ket{0}+i\\ket{1}) = \\frac{1}{\\sqrt{2}}\\begin{bmatrix}1 \\\\i \\end{bmatrix} \\equiv \\ket{y_+}$ A unitary operator is a normal operator with $A^\\dagger A = A A^\\dagger=I$. Since $U$ is normal, it's diagonalizable, too. $U$ preserves inner product $(U\\ket{v}, U\\ket{w}) = \\ket{v}\\ket{w}$. $U = \\sum_i \\ket{w_i}\\bra{v_i}$ $\\rm{Tr}(A)=\\sum_i A_{ii}$ A bra vector is a linear functional $\\mathcal{V} \\rightarrow \\mathbb{C}$; i.e. it takes a ket vector to a complex number: $(\\bra{v})\\ket{w} = \\braket{v}{w}$. Cohen Tannoudji (2020) showed that the space of all such linear functionals forms another vector space $V^*$—known as the dual space of $V$ (pp. 103–108). A bra vector is an element of $V^*$ . A bra vector can also be represented as an adjoint vector: $\\ket{v}^\\dagger \\equiv \\bra{v}$. Constructing a basis can be very non-trivial for infinite-dimensional vector spaces. (More on this on pp. 29–30, Johnston 2021) A function $(,):\\mathcal{V} \\times \\mathcal{V} \\rightarrow \\mathbb{C}$. Some properties: If $\\mathcal{V}$ is a vector space and $\\mathcal{S} \\subseteq \\mathcal{V}$, then $\\mathcal{S}$ is a subspace iff it is closed under addition and scalar multiplication. (You need not check all ten properties. See p. 9, Johnston (2021) for a proof). A self-adjoint/Hermitian operator satisfies $A^\\dagger=A$. Singular Value Decomposition Polar Decomposition Similarity Transformation $c(d\\ket{v})=(cd)\\ket{v}$ Example: Two non-parallel vectors on a plane are linearly independent, three of more of them on a plane are linearly dependent. Notice that an infinite set of vectors can be linearly independent—you only need to show that there are not any finite linear combination of them such that all the scalars used in the combination are zeros. Think about the set $\\{1,x,x^2,\\cdots\\}$. Though the set contains infinitely many elements, it is linearly independent. Because if $\\sum_{i=0}^p c_i x^i=0$ then all $c_i$s are equal to 0. (set $x=0 \\implies c_0=0$; take derivatives and show $c_1=0$ and so on. See p. 17 of Johnston (2021)). So we cannot find any linear combination which is equal to 0 with at least one non-zero coefficient, implying that the set is not linearly dependent. So it's linearly independent! Closure under scalar multiplication: $c\\ket{v} \\in \\mathcal{V}$ Why you may not want to read this page Projector Operator Orthogonal Vectors $P^\\dagger=P$ Special case: Measurement operator $M=\\sum_i\\lambda_i P_i$"
  },
  {
    "title": "LaTeX",
    "url": "https://arghyadutta.github.io/notebooks/latex.html",
    "content": "Recommended Cleaning up BibTex files Draw a symbol to get its LaTeX command Create LaTeX tables from data A no-frills LaTeX format Detexify Online BibTex tidy"
  },
  {
    "title": "Stoicism",
    "url": "https://arghyadutta.github.io/notebooks/stoicism.html",
    "content": "Recommended Seneca, A. L. (2000). Letters From A Stoic (Reprint edition). Penguin UK. Long, A. A. (2004). Epictetus: A Stoic and Socratic Guide to Life (1st edition). Clarendon Press. Farnsworth is a careful writer, and he lays down the main tenets of Stoicism with ample quotes and nuanced analysis. If I get time Not recommended Farnsworth, W. (2018). The Practicing Stoic: A Philosophical User’s Manual . David R. Godine, Publisher. Epictetus. (2018). How to Be Free: An Ancient Guide to the Stoic Life (A. Long, Trans.; Early Printing edition). Princeton University Press. Another run-of-the-mill, cliché-filled self-help book written to optimize every second of your life. Don't buy it. Holiday, R. (2016). The Daily Stoic: 366 Meditations on Wisdom, Perseverance, and the Art of Living. Profile Books."
  },
  {
    "title": "Academia: Issues",
    "url": "https://arghyadutta.github.io/notebooks/academiaIssues.html",
    "content": "Recommended To be fair, I've published in Nature family of journals—but never as a corresponding author. I think Sierra's is argument is sound. Academia, the Good Side Nothing we make, alas, is perfect. Lenz, M. (2020). The adversarial culture in philosophy does not serve the truth Aeon Ideas. Aeon. See also Curry, S. Sick of Impact Factors . Reciprocal Space . Retrieved December 6, 2021. The unsavory side of academia. Sierra, C. A. (n.d.). Reject Nature: Elite journals and the defeat of science . Retrieved December 6, 2021, from his blog \". Lin, J. (2010). Unraveling tenure at MIT . The Tech Skinner, B. (2019) What It Means, and Doesn’t Mean, to Get a Job in Physics . Gravity and Levity"
  },
  {
    "title": "Germany",
    "url": "https://arghyadutta.github.io/notebooks/germany.html",
    "content": "Recommended A Guide to German Verbs for Beginners Bilingual Electronic Books – Dual Language German/English – Doppeltext 15 Great German Children’s Books for Readers of All Ages | FluentU German MacGregor, N. (2014). Germany: Memories of a Nation (1st edition). Penguin. Highly recommended to all interested in the perilous journey undertaken by mankind during WW2. German History in Documents and Images Learning German: An Annotated Parallel Texts Reading List German.net First, without obsessing over the brutality and sadism of individual Nazi party members, this series provides a complete and unnerving account of how the Nazis, after coming in power, \"won\" the support of German people. It portrays the contribution of Joseph Goebbels, the Reich Propaganda minister, in mobilizing the people to a war for the \"Living space\". More on these Recounting the experience of individuals brings home, as nothing else can, the sheer complexity of the choices they had to make, and the difficult and often opaque nature of the situations they confronted. Contemporaries could not see things as clearly as we can, with the gift of hindsight: they could not know in 1930 what was to come in 1933, they could not know in 1933 what was to come in 1939 or 1942 or 1945. If they had known, doubtless the choices they made would have been different. One of the greatest problems in writing history is to imagine oneself back in the world of the past, with all the doubts and uncertainties people faced in dealing with a future that for the historian has also become the past. Developments that seem inevitable in retrospect were by no means so at the time, and in writing this book I have tried to remind the reader repeatedly that things could easily have turned out very differently to the way they did at a number of points in the history of Germany in the second half of the nineteenth century and the first half of the twentieth. People make their own history, as Karl Marx once memorably observed, but not under conditions of their own choosing. These conditions included not only the historical context in which they lived, but also the way in which they thought, the assumptions they acted upon, and the principles and beliefs that informed their behavior. A central aim of this book is to re-create all these things for a modern readership, and to remind readers that, to quote another well-known aphorism about history, 'the past is a foreign country: they do things differently there'. —Richard J. Evans A splendid set of books! The first volume deals with the coming in power of the Nazi party; the second volume is about the social, political and many other dimensions of German life under the Nazi rule and, finally, the third volume describes the expansion of the Third Reich and its subsequent downfall in 1945. I liked these books for a number of reasons: First, without obsessing over the brutality and sadism of individual Nazi party members, this series provides a complete and unnerving account of how the Nazis, after coming in power, \"won\" the support of German people. It portrays the contribution of Joseph Goebbels, the Reich Propaganda minister, in mobilizing the people to a war for the \"Living space\". Second, the series provides a rare glimpse into the life of the common members of the Nazi party, Nazi party bosses and, also, ordinary German folks. How did the Germans react to the overbearing regime of the Nazi party? Were they happy under its rule? Did they share the murderous attitude of Nazi party towards \"subhuman(!)\" Polish and Slovakian people and, worst of them all, according to Nazi ideology, Jewish people? As expected, these questions do not have a binary answer and the author explored them with the help of many diaries (most notably, a jew's—Victor Klemperer's—and a Nazi party member's, Melita Maschmann's). Finally, it's reassuring to learn about humane actions done by some Germans to help the oppressed and to observe that the Germans did not lose their sense of humor even under such oppressive regime, as evidenced by many jokes mentioned in the book. Though, let me be clear, this does not at all means that Evans has tried to defend the German atrocities committed during the war. Quite contrarily, he devoted almost 40% of the third volume detailing the massacre brought upon by the military and the SS on the civilian of the conquered countries. Highly recommended to all interested in the perilous journey undertaken by mankind during WW2. The Most Frequent German Words – Deutsch 101-326 Textbooks - German Second, the series provides a rare glimpse into the life of the common members of the Nazi party, Nazi party bosses and, also, ordinary German folks. How did the Germans react to the overbearing regime of the Nazi party? Were they happy under its rule? Did they share the murderous attitude of Nazi party towards \"subhuman(!)\" Polish and Slovakian people and, worst of them all, according to Nazi ideology, Jewish people? As expected, these questions do not have a binary answer and the author explored them with the help of many diaries (most notably, a jew's—Victor Klemperer's—and a Nazi party member's, Melita Maschmann's). Dartmouth German Learning the language Reverso | Free Translation, Dictionary Denglisch Dictionary: When Languages Collide The first volume deals with the coming in power of the Nazi party; the second volume is about the social, political and many other dimensions of German life under the Nazi rule and, finally, the third volume describes the expansion of the Third Reich and its subsequent downfall in 1945. I liked these books for a number of reasons: Evans, R. J. (2004). Coming Of The Third Reich . Penguin Evans, R. J. (2006). Third Reich in Power , 1933-1939. Penguin Evans, R. J. (2009). Third Reich at War , Penguin Finally, it's reassuring to learn about humane actions done by some Germans to help the oppressed and to observe that the Germans did not lose their sense of humor even under such oppressive regime, as evidenced by many jokes mentioned in the book. Though, let me be clear, this does not at all means that Evans has tried to defend the German atrocities committed during the war. Quite contrarily, he devoted almost 40% of the third volume detailing the massacre brought upon by the military and the SS on the civilian of the conquered countries. A splendid set of books! Grammar | DW Learn German"
  },
  {
    "title": "Blogs",
    "url": "https://arghyadutta.github.io/notebooks/blogs.html",
    "content": "AI: A Guide for Thinking Humans Separated by a Common Language Machine Learning Research Blog Condensed concepts Blogs I Read Writing Science The 20% Statistician off the convex path Ahead of AI And then. Academics and beyond nanoscale views Scatterings Forms of life, forms of mind The Last Word On Nothing Almost Sure. A random mathematical blog Blogs as Modern Commonplace Books, and the Pleasures Thereof; Ana Ulin; Her blog (2018) Statistical Modeling, Causal Inference, and Social Science \"We humans are such collecting creatures. We love creating sets of things, hoarding what we know or what we have, maybe arranging it all carefully for display.\" Probably overthinking it Blogging Academic Garden Simon Willison's weblog"
  },
  {
    "title": "Software Tools",
    "url": "https://arghyadutta.github.io/notebooks/softwareTools.html",
    "content": "The Zotero connector allows you to add papers to your Zotero Library from the browser. It can also automatically redirect journal webpages through an institutional proxy, allowing you to download papers with your institutional credentials. HTML cleanup tool & simplifier Feedbro helps to get articles from any website that provides an RSS feed (blogs, journals, magazines…). A good alternative to Feedly. Gesturefy is such a neat add-on! It lets you do several things, like going back to the previous webpage, only with gestures of your mouse. webplotdigitizer - extract data from plots, images, and maps Tools OmniDiskSweeper finds and helps delete large files. Tabliss can modify new tab page in the browser. uBlock Origin is the ad blocker you should be using. Tt even lets you disable all javascript on a website, if you're adventurous. Find Journals' short names Rectangle moves and resizes windows. Single-file is a neat add-on that can download a webpage with everything, including figures, in a single HTML file. Very useful if you want to locally archive, for example, a blog post or a Twitter thread. Return YouTube dislikes brings back a useful feature. Addons for Mozilla Firefox Tranquility Reader can strip down a webpage to main texts and images, letting you read it without 1000 flashy annoying things. Nifty and Free Software For MacBook AppCleaner helps to cleanly uninstall an app. You open a webpage like Cosma Shalizi's (check it!) and find hundreds of links. Which one is active and which one is dead? Link analyzer does that job for you with one click. Duplicate File Finder Remover finds (and removes) duplicate files. Hide Youtube-Shorts removes YouTube shorts from its homepage, subscriptions page, and search results. It can also hide the \"Shorts\" tab. Good riddance! Get unicode of symbols Amphetamine keeps your MacBook awake, which can be useful during presentations, for example. Learn touch typing"
  },
  {
    "title": "Relativity: Special and General Theory",
    "url": "https://arghyadutta.github.io/notebooks/relativity.html",
    "content": "Recommended Really good introductory book. Especially the first few chapters introducing the idea of tensors are excellent. Guidry, M. (2019) Modern General Relativity: Black Holes, Gravitational Waves, and Cosmology ; Cambridge university press Schutz, B. F. (2009) A First Course in General Relativity , 2nd ed.; Cambridge University Press. If I get time It should not be the first book, but it's well-written and full of insights. Coleman, S. (2021) Sidney Coleman’s Lectures on Relativity , ; Griffiths, D. J., Derbes, D., Sohn, R. B., Eds.; Cambridge University Press"
  },
  {
    "title": "Melancholia and Grief",
    "url": "https://arghyadutta.github.io/notebooks/melancholia.html",
    "content": "Crosley, S. (2024), The Tail End, What we lose when we lose a pet , The New Yorker . Recommended Grief , Louise Erdrich \"Everybody should know what it is to have friends like these. Everybody should know what it is to be loved like this.\" Astonishingly beautiful and tender recollection of a friend. Father by Ted Kooser \"Whether due to a downsizing, divorce, or death, an estate sale is a sort of liminal space – a passing of the tools and accumulated flotsam of a life onto descendants who cannot always bear the added weight and so jettison it.\" Smith, N. (2024), Why rabbits? Towards a better, floofier world , Noahpinion \"People often ask me: “Why rabbits?” Usually my answer is just “They’re floofy.” Heartfelt reminiscence. Polk, E. (2024), Peregrinations of grief , Aeon . Shenoda, S. (2025). It's Just Stuff What Estate Sales Reveal About Us . Plough Read it Sometimes you have to take your own hand as though you were a lost child and bring yourself stumbling home over twisted ice. Whiteness drifts over your house. A page of warm light falls steady from the open door. Here is your bed, folded open. Lie down, lie down, let the blue snow cover you. (from \"Original Fire: Selected and New Poems\", Harper Perennial, 2004)"
  },
  {
    "title": "Poetry",
    "url": "https://arghyadutta.github.io/notebooks/poetry.html",
    "content": "East Coker Ella Wheeler Wilcox Du Fu Father Ted Kooser I, Too The Naming of Cats Few poems. T S Eliot I mean, wow! In my beginning is my end. In succession Houses rise and fall, crumble, are extended, Are removed, destroyed, restored, or in their place Is an open field, or a factory, or a by-pass. Old stone to new building, old timber to new fires, Old fires to ashes, and ashes to the earth Which is already flesh, fur and faeces, Bone of man and beast, cornstalk and leaf. Houses live and die: there is a time for building And a time for living and for generation And a time for the wind to break the loosened pane And to shake the wainscot where the field-mouse trots And to shake the tattered arras woven with a silent motto. The Year Like this one An abandoned courtyard: an old tree: A temple bell lying on its side: The world I live in. They win and we lose; we lose and they win. Vines wrap around the rotting bones. She knows he won’t come back from the army, but patches the clothes he left just in case. Or this (I feel this) I am about to scream madly in the office, Especially when they bring more papers to pile higher on my desk. Langston Hughes Bank Fishing for Bluegills"
  },
  {
    "title": "Fountain Pen, Pencil, Paper",
    "url": "https://arghyadutta.github.io/notebooks/stationery.html",
    "content": "Lamy 2000 And The Origins Of Lamy Design . (2012, August 11). The Fountain Pen Network Fountain Pen Network I have read (and spent) a bit too much on fountain pens, pencils, and paper—in general, writing paraphernalia. Maybe someday I'll write about them, but not today. Today, I am busy filling out electronic forms, Word files, and Excel sheets that my workplace is asking for. In case you're serious about Lamy 2000."
  },
  {
    "title": "Combinatorics",
    "url": "https://arghyadutta.github.io/notebooks/combinatorics.html",
    "content": "Graham, R. L., Knuth, D. E., & Patashnik, O. (2017). Concrete mathematics: A foundation for computer science (2. ed., 31. print). Addison-Wesley. If I Get Time Bona, M. (2011). Walk Through Combinatorics, A: An Introduction To Enumeration And Graph Theory."
  },
  {
    "title": "Dimensional Analysis",
    "url": "https://arghyadutta.github.io/notebooks/dimensionalAnalysis.html",
    "content": "Recommended Anything Lemons writes is well-written; this is not an exception. McKinley, G. H. (2024). Getting the (dimensionless) numbers right . Nature Chemical Engineering, 1(1), Article 1 . Chapter 1 contains a short but useful discussion. Morin, D. J. (2008). Introduction to classical mechanics: With problems and solutions . Cambridge university press. Lemons, D. S. (2017). A Student’s Guide to Dimensional Analysis (1st ed.). Cambridge University Press."
  },
  {
    "title": "Loschmidt Paradox",
    "url": "https://arghyadutta.github.io/notebooks/loschmidtParadox.html",
    "content": "Recommended An overwhelmingly large number of microstates are compatible with a macrostate, specified by a few state quantities like pressure, volume, and temperature. That means the initial state typically evolves to one of numerous possible microstates which cannot be distinguished macroscopically. That means the exactly one final state that allows the system to go back to the initial state is the actual final state can occur with a miniscule probability. Boltzmann solved this paradox by noting that: Swendsen, R. H. (2008). Explaining irreversibility . American Journal of Physics, 76(7), 643–648 . Consider the time evolution of a system $N$ particles. In the $6N$-dimensional phase space, a point $(q_i, p_i)\\equiv(q_1,p_1,q_2,p_2,\\cdots,q_{3N},p_{3N})$ denote one microstate. Loschmidt argued that for a system starting from $(q_i(t_0),p_i(t_0))$ and evolving to $(q_i(t_f),p_f(t_f))$, if we reverse the momenta of all particles at $t_f$—i.e., if we can prepare a system described by $(q_i(t_0),-p_i(t_0))$—then the system should evolve back to the initial state at a later time because the laws of classical mechanics are time reversible. If true, this implies the amazing phenomenon of a gas spontaneously returning to a smaller volume from a larger volume. Greiner, W., Neise, L., & Stöcker, H. (1995). Thermodynamics and Statistical Mechanics. Springer. (pp. 43–46) As a concrete example, the number of microstate of a system with $N$ particles is proportional to $V^N$. So if you halve the volume, the available microstates decrease by $1/2^N$. So, in the thermodynamic limit, the probability of a gas spontaneously collapsing to half of its volume is $1/2^N \\rightarrow 0$. Moreover, all the microstates with same total energy can be found with equal probability."
  },
  {
    "title": "Food and Drink",
    "url": "https://arghyadutta.github.io/notebooks/foodNDrink.html",
    "content": "Apples on a scale from most tart to most sweet A nice blog on Konkani recipes. How to make Chocolate 14 Types Of Rice And The Best Dishes To Use Them from Food Republic \"I was once told by a grandmother, who lives in my seaside village but grew up in the northern dry regions where the Senegal River winds across a crispy and prickly savanna not far from the great desert, about a watermelon varietal called beref, which is cultivated mostly for its seeds but also serves as a kind of water reserve. “There’s water that you can drink from it like a coconut,” she said.\" Advanced Coffee Making , Lecture at Assembly Coffee London Orwell, G. (1946) A Nice Cup of Tea . Evening Standard The Complete Guide to Sugar Around the World by Craig Cavallo Lewis, J. Tell Me Why the Watermelon Grows. Switchyard Few interesting videos and articles. \"Tenthly, one should pour tea into the cup first.\" And of course, the man had one more rule \"Lastly, tea…\"."
  },
  {
    "title": "Books and Libraries",
    "url": "https://arghyadutta.github.io/notebooks/bookNLibrary.html",
    "content": "Piper, A. (2018). Enumerations: Data and literary study . The University of Chicago Press. Eco is interesting, as usual. Meanwhile, Carrière: And anyway, what about the cultures that haven’t developed what we call philosophy? That’s what I meant just now by saying that anthropology is just as important. The notion of a ‘philosophical concept’, for instance, is a purely Western one. Try explaining ‘concept’ to an Indian—even a highly sophisticated one—or ‘transcendence’ to a Chinese person! (p. 233 in the eBook) Really, my man? (To be fair, he wrote a long French play based on the Indian epic Mahabharata, as he was \"completely enchanted\" by it. So, maybe he meant something else here? I don't know.) Piper, A. (2009). Dreaming in books: The making of the bibliographic imagination in the Romantic age . University of Chicago press. Is it okay to appreciate creations by imperfect human beings? Morton offers a nuanced perspective. If I get time Morton, B. (2019, January 8). Virginia Woolf? Snob! Richard Wright? Sexist! Dostoyevsky? Anti-Semite! The New York Times . Carrière, J.-C., & Eco, U. (2011). This is Not the End of the Book . Harvill Secker. Piper, A. (2012). Book was there: Reading in electronic times . University of Chicago Press."
  },
  {
    "title": "Sanskrit",
    "url": "https://arghyadutta.github.io/notebooks/sanskrit.html",
    "content": "Recommended Ruppel, A. M. The Cambridge Introduction to Sanskrit , 1st ed.; Cambridge University Press, 2017. https://doi.org/10.1017/9781107088283 ."
  },
  {
    "title": "Foundations of Quantum mechanics",
    "url": "https://arghyadutta.github.io/notebooks/qMechFoundations.html",
    "content": "Recommended Fun slides. Courses on YouTube Foundations of Quantum Mechanics by Florian Marquardt . Another series, by Rob Spekkens, on Foundations of Quantum Mechanics . Foundations of quantum mechanics is a great subject, until you want to publish, of course. Norsen's accessible and mildly opinionated book provides a good introduction. Asher Peres's book goes deep early on and is easily the most difficult of all the books mentioned in this list. It's very rewarding, though. Peres, A. (2010). Quantum theory: Concepts and methods . Kluwer Acad. Publ. Norsen, T. (2017). Foundations of Quantum Mechanics: An Exploration of the Physical Meaning of Quantum Theory (1st ed.). Springer International Publishing"
  },
  {
    "title": "Advaita Vedanta",
    "url": "https://arghyadutta.github.io/notebooks/advaitaVedanta.html",
    "content": "Bramhasutra A complete, heavily annotated bengali translation of the Shankaracharya's commentary of the Bramha Sutra. The footnotes are often useful, particularly to clarify doubts regarding Mimansa and Nyaya. Accurate, concise english translations of Shankaracharya's commentaries on eight of the principal upanishads: Isa, Kena, Katha, Taittariya (vol. 1) and Aitareya, Mundaka, Mandukya (with Karika), Prasna (vol. 2). Swami, Gambhirananda (1984). Bhagavad Gita: With the commentary of Shankaracharya . Advaita Ashrama. Vishwarupananda, S. (1997). Vedanta Darshan (4 volumes, in Bengali). Udbodhan Karyalaya, Kolkata. Accurate english translation of Shankaracharya's commentary. Shrimad Bhagwat Gita (ShankarBhashya Hindi Anuvad Sahit) (2022). Gita Press, Gorakhpur. 1) Isshadi Nau Upnishad Shankar Bhashya Sahit 2) Chandogya Upanishad 3) Brihadaranyak Upanishad Gita Press, Gorakhpur Upanishads 1) Shankaracharya (1957). Eight Upanisads Vol 1 (S. Gambhirananda, Trans.). Advaita Ashrama. 2) Shankaracharya (1957). Eight Upanisads Vol 2 (S. Gambhirananda, Trans.). Advaita Ashrama. Shankaracharya (1956). Bramhasutra Bhasya (Sw. Gambhirananda, Trans.). Advaita Ashrama. Gita Shankaracharya's commentaries, along with his hymns and other writings, are now made available online in original (Sanskrit) by Shringeri math. Hindi translation Hindi translation of the commentaries of Shankaracharya of eleven principal upanishads. Excellent, nuanced translations. English translation"
  },
  {
    "title": "Hydrophobicity",
    "url": "https://arghyadutta.github.io/notebooks/hydrophobicity.html",
    "content": "Kauzmann, W. Some Factors in the Interpretation of Protein Denaturation. In Advances in Protein Chemistry ; Elsevier, 1959; Vol. 14, pp 1–63. https://doi.org/10.1016/S0065-3233(08)60608-7 . Ben-Naim, A. Hydrophobic Interaction and Structural Changes in the Solvent. Biopolymers 1975 , 14 (7), 1337–1355. https://doi.org/10.1002/bip.1975.360140704 . Xi, E.; Patel, A. J. The Hydrophobic Effect, and Fluctuations: The Long and the Short of It. Proc. Natl. Acad. Sci. U.S.A. 2016 , 113 (17), 4549–4551. https://doi.org/10.1073/pnas.1603014113 . A result of interplay between entropy and enthalpy. Tanford, C. The Hydrophobic Effect and the Organization of Living Matter. Science 1978 , 200 (4345), 1012–1018. https://doi.org/10.1126/science.653353 . Berne, B. J.; Weeks, J. D.; Zhou, R. Dewetting and Hydrophobic Interaction in Physical and Biological Systems. Annual Review of Physical Chemistry 2009 , 60 (1), 85–103. https://doi.org/10.1146/annurev.physchem.58.032806.104445 If I Get Time Lazaridis, T. Hydrophobic Effect. In eLS ; John Wiley & Sons, Ltd, 2013. https://doi.org/10.1002/9780470015902.a0002974.pub2 ."
  },
  {
    "title": "Artificial Intelligence",
    "url": "https://arghyadutta.github.io/notebooks/ai.html",
    "content": "The Limits of AI by Hubert Dreyfus (1985) Chomsky, N., Roberts, I., & Watumull, J. (2023, March 8). The False Promise of ChatGPT . The New York Times . Pei Wang's A Gentle Introduction to AGI . A neat and well-argued metaphor. Excellent and up-to-date broad overview of AI. Issues of AI Geoffrey Hinton's profile by the New Yorker. Useful read, despite the almost click-bait title. LLMs affected—destroyed—the learning process of young students in a way I wouldn't have believed was possible—until I began teaching. Rothman offers an alternative approach. Partial list, reflecting my annoyance with the ongoing hype about AGI. Compelling and persuasive article against AGI, often elaborating on Dreyfus's ideas. Chiang, T. (2023, February 9). ChatGPT Is a Blurry JPEG of the Web . The New Yorker . Pins down issues with the methods and performance metrics. Rothman, J. (2025, April 29). Why Even Try if You Have A.I.? The New Yorker . Heitzinger, C., & Woltran, S. (2024). A Short Introduction to Artificial Intelligence: Methods, Success Stories, and Current Limitations . Introduction to Digital Humanism: A Textbook (pp. 135–149). Springer Nature Switzerland. Chayka, K. (2025, April 2). The Limits of A.I.-Generated Miyazaki . The New Yorker . Rothman, J. (2023, November 13). Why the Godfather of A.I. Fears What He’s Built . The New Yorker . Artificial General Intelligence (AGI) Fjelland, R. (2020). Why general artificial intelligence will not be realized . Humanities and Social Sciences Communications, 7(1), 10 . Provocative but thoughtful, as usual from Chomsky. Burnett, D. G. (2025, April 26). Will the Humanities Survive Artificial Intelligence? The New Yorker . Müller, V. C. (2023). Ethics of Artificial Intelligence and Robotics . In E. N. Zalta & U. Nodelman (Eds.), The Stanford Encyclopedia of Philosophy Broad discussion with well-chosen pointers for further reading. Crockett, MJ. (2025, February 27). AI is ‘beating’ humans at empathy and creativity. But these games are rigged . The Guardian . Dreyfus lays down the core objections against AGI."
  },
  {
    "title": "Biology",
    "url": "https://arghyadutta.github.io/notebooks/biology.html",
    "content": "Recommended A unique book, like Phillips's other ones. Milo, R., & Phillips, R. (2016). Cell biology by the numbers. Garland Science, Taylor & Francis Group. ( Online free version ) Medawar, P. B., & Medawar, J. S. (1985). Aristotle to Zoos: A Philosophical Dictionary of Biology . Oxford University Press. Outstanding content and Harold is a master of prose. Much better than standard popular science books. I haven't read another 'dictionary' quite like it. An Owner's Guide to the Human Genome: an introduction to human population genetics, variation and disease by Jonathan Pritchard Harold, F. M. (2001). The Way of the Cell: Molecules, Organisms, and the Order of Life . Oxford University Press."
  },
  {
    "title": "On Writing (and Some Examples)",
    "url": "https://arghyadutta.github.io/notebooks/writing.html",
    "content": "Recommendations and resources on writing Recommended Yong, E. (2021). America Is Not Ready for Omicron . The Atlantic . Good introduction to English grammar and writing beyond simple definitions. The Writer's workshop Common Errors in English Usage Wolchover has an uncanny ability to phrase difficult ideas in popular language; this essay demonstrates that once again. Some well-written essays Kolln, M. J., & Gray, L. S. (2015). Rhetorical Grammar: Grammatical Choices, Rhetorical Effects . Pearson Education. White, E. B. (1949). Here is New York . Reprinted in Essays of E. B. White (2016) . Wolchover, N. (2020). What Is a Particle? Quanta . Internet resources Neelakantan, A. (2021). Write because it makes you think or feel. the Record . Why read fiction? And does good writing matter? Mukherjee, S. (2023). All the Carcinogens We Cannot See . The New Yorker . Ed Yong shows that one can write a good essay only when one deeply cares about the topic. Smith, Z. (2023). On Killing Charles Dickens . The New Yorker . Zadie Smith tries—and fails—to ignore Dickens while writing a nineteenth-century historical fiction. Writing Style—An Online Guide YouTube playlists McPhee J. (2015) The Art of Omission . The New Yorker . Williams, J. M. (2000). Style: Ten lessons in clarity and grace (6. ed). Longman. Writing is emotional: it's hard to remove unnecessary sentences that you've written. McPhees shares few personal stories. If I get time Gaitskill M. (2022) The deracination of literature . Unheard . Grammar and Stylistics: Writing Clearly by Randall Eggert from University of Utah. Orwell gives five rules for good writing and adds a sixth: \"Break any of these rules sooner than say anything outright barbarous.\" Excellent! See, for example, the one on tense and aspect . Newport, C. (2024). One Reason Hybrid Work Makes Employees Miserable. And how to fix it. The Atlantic . Purdue Online Writing Lab Punctuation guide Possibly the most famous essay on NYC—for good reasons. Orwell, G. (1946). Politics and the English Language; George Orwell . Horizon ."
  },
  {
    "title": "Bose–Einstein Condensates",
    "url": "https://arghyadutta.github.io/notebooks/bec.html",
    "content": "Recommended Proukakis, N. P. A Century of Bose–Einstein Condensation . Commun Phys 2025, 8 (1), 264 . Schwartz, M. Lecture 12: Bose-Einstein Condensation , 2019. PDF"
  },
  {
    "title": "How to do research",
    "url": "https://arghyadutta.github.io/notebooks/researchMethodology.html",
    "content": "Recommended Dyson argues why we need both specialists and generalists in research. Dyson, F. (2009). Birds and Frogs . Notices of the AMS, 56(2), 212–223 . Guttal, V. (n.d.). Assorted articles on professional skills . Curated blog series . Schwartz, M. A. (2008). The importance of stupidity in scientific research . Journal of Cell Science, 121(11), 1771 . Rockmore, D. (2019). The Myth and Magic of Generating New Ideas . The New Yorker . Mckenji, R. (2011). Towards research independence . Blogpost . A wonderful guide to working scientists. Brilliant! Imposter syndrome is the worst; Schwartz argues why feeling stupid is the norm in scientific research. Nielsen, M. A. (2004). Principles of Effective Research . Blogpost . Hamming, R. (1986). You and Your Research . Lecture transcript . I wish I had a foolproof protocol. Since I don't, here are some useful pointers. And my thoughts on personal knowledge management may be useful. Finding the Search Terms for Your Literature Review , 2022. (accessed 2025-09-04). Research ideas are difficult to conceive; Rockmore tells what works for him. Chatterjee, A. (2015). Some musings on academic research . Personal webpage ."
  },
  {
    "title": "Ehrenfest Theorem",
    "url": "https://arghyadutta.github.io/notebooks/ehrenfest.html",
    "content": "Recommended Another surprisingly thorough note from Nicholas Wheeler from Reed College (Yes, Griffiths is from Reed, too). A. Messiah. Quantum Mechanics: Two Volumes Bound as One; Dover Publications, Inc: Garden City, New York, 2020. Wheeler, N. Remarks Concerning the Status & Some Ramifications of Ehrenfest’s Theorem, 1998. PDF (accessed 2024-10-03). Chapters 5, 6 discusses commutators and Ehrenfest's equation."
  },
  {
    "title": "Electromagnetism",
    "url": "https://arghyadutta.github.io/notebooks/electromagnetism.html",
    "content": "Recommended Griffiths, D. J. (2023). Introduction to Electrodynamics (5th ed.). Cambridge University Press. Purcell, E. M., & Morin, D. J. (2013). Electricity and Magnetism (Third edition). Cambridge University Press. This book is on a league of it's own. The CUP edition is expensive, especially for Indian students. But note that it is far more carefully edited than the recent cheaper Berkeley series edition which claims to use the SI units. For a fun aside, see: Fahy, S.; O’Sullivan, C. All Magnetic Phenomena Are NOT Due to Electric Charges in Motion . Am. J. Phys. 2022, 90 (1), 7–8 . And: Griffiths, D. Reply to: All Magnetic Phenomena Are NOT Due to Electric Charges in Motion American Journal of Physics 2022, 90 (1), 9–9 ."
  },
  {
    "title": "C. S. Lewis",
    "url": "https://arghyadutta.github.io/notebooks/cSLewis.html",
    "content": "Recommended C. S Lewis, Meditation in a Toolshed , The Coventry Evening Telegraph (1945) . Lewis on looking at and looking along. Thoughtful piece."
  },
  {
    "title": "Funny",
    "url": "https://arghyadutta.github.io/notebooks/funny.html",
    "content": "Joel Grus—Fizz Buzz in Tensorflow Cartoons by Sidney Harris A map of every city in Europe Steve Plimpton’s collection of quotes"
  },
  {
    "title": "Symbolic Regression",
    "url": "https://arghyadutta.github.io/notebooks/symbolicRegression.html",
    "content": "Recommended SISSO (Luca's group) and subsequent applications in material science. Schmidt and Lipson (2009). Note this paper has been criticized for not citing the literature (Crutchfield et al. 1987 and 1998). Ouyang, R., Ahmetcik, E., Carbogno, C., Scheffler, M., & Ghiringhelli, L. M. (2019). Simultaneous learning of several materials properties from incomplete databases with multi-task SISSO . Journal of Physics: Materials, 2(2), 024002 . Koza, J. R. (1994). Genetic programming as a means for programming computers by natural selection . Statistics and Computing, 4(2), 87–112 . Dutta, A., Vreeken, J., Ghiringhelli, L. M., & Bereau, T. (2021). Data-driven equation for drug–membrane permeability across drugs and membranes. The Journal of Chemical Physics, 154(24), 244114. https://doi.org/10.1063/5.0053931 Schmidt, M., & Lipson, H. (2009). Distilling Free-Form Natural Laws from Experimental Data . Science, 324(5923), 81–85 . Purcell, T. A. R., Scheffler, M., Carbogno, C., & Ghiringhelli, L. M. (2022). SISSO++: A C++ Implementation of the Sure-Independence Screening and Sparsifying Operator Approach . Journal of Open Source Software, 7(71), 3960 . A method of systematically generating algebraic equations from data, with potential applications in discovering equations and even laws. The result, so far, has been quite promising, but no \"laws\" have been discovered, as yet. Genetic algorithm paper by Koza (1994) AI Feynmann from Tegmark. Timeline of papers Yes, that's me. :P Ouyang, R., Curtarolo, S., Ahmetcik, E., Scheffler, M., & Ghiringhelli, L. M. (2018). SISSO: A compressed-sensing method for identifying the best low-dimensional descriptor in an immensity of offered candidates . Physical Review Materials, 2(8), 083802 . Udrescu, S.-M., & Tegmark, M. (2020). AI Feynman: A Physics-Inspired Method for Symbolic Regression arXiv:1905.11481 . Crutchfield, J. P. (1987). Equations of Motion from a Data Series . Complex Systems, 1, 417–452. Crutchfield, J. P., & Young, K. (1989). Inferring statistical complexity . Physical Review Letters, 63(2), 105–108 ."
  },
  {
    "title": "Evaluating Clustering Performance",
    "url": "https://arghyadutta.github.io/notebooks/clustPerformance.html",
    "content": " Completeness Score ¶ [[8 0]\n [2 2]] Calinski–Harabasz index (CHI) is the ratio of the sum of between-clusters dispersion for all clusters and sum of inter-cluster dispersion for all clusters. Better-defined clusters have higher CHI value. Completeness Score ¶ All members of a class belongs to the same cluster. $c = 1 - \\frac{H(K|C)}{H(K)}$ $c \\in[0,1]$. higher is better. Silhouette coefficient ¶ References: Davies-Bouldin Index ¶ Surprisingly, DB values closer to zero indicate a better partition , unlike Calinski-Harabasz index and Silhouette coefficient. For CHI and DBI formulas, check the referenced scikit doc page. V-Measure ¶ The harmonic mean of the homogeneity score and completeness scores: $v = 2 \\cdot \\frac{h \\cdot c}{h + c}$. It's implemented in scikit as $v = \\frac{(1 + \\beta) \\times \\text{homogeneity} \\times \\text{completeness}}{(\\beta \\times \\text{homogeneity} + \\text{completeness})}$ Building on the pair-confusion matrix, FMI, a measure for cluster similarity, is computed using the elements of $C$ as\n$$FMI = \\frac{TP}{\\sqrt{(TP + FP) (TP + FN)}}.$$ For a single sample it is Evaluation of quality of clusters is often done in two ways: $n$: total number of samples. Calinski Harabasz Index ¶ Mutual-information-based similarity score ¶ Let's say, we have 2 sets of labels, $U$ and $V$, for $N$ objects. If $P(i)=|U_i|/N$ and $P'(j)=|V_i|/N$ are the probabilities that a randomly-picked object from $U$ ($V$) falls into class $U_i$ ($V_j$), then the entropies and the mutual information between $U$ and $V$ are computed as\n$$\n\\begin{aligned}\nH(U) &= - \\sum_{i=1}^{|U|}P(i)\\log(P(i))\\\\\nH(V) &= - \\sum_{j=1}^{|V|}P'(j)\\log(P'(j))\\\\\n\\text{MI}(U, V) &= \\sum_{i=1}^{|U|}\\sum_{j=1}^{|V|}P(i, j)\\log\\left(\\frac{P(i,j)}{P(i)P'(j)}\\right)\\\\\n&= \\sum_{i=1}^{|U|} \\sum_{j=1}^{|V|} \\frac{|U_i \\cap V_j|}{N}\\log\\left(\\frac{N|U_i \\cap V_j|}{|U_i||V_j|}\\right)\\\\\n\\text{NMI}(U, V) &= \\frac{\\text{MI}(U, V)}{\\text{mean}(H(U), H(V))}\\\\\n\\text{AMI} &= \\frac{\\text{MI} - E[\\text{MI}]}{\\text{mean}(H(U), H(V)) - E[\\text{MI}]}\n\\end{aligned}\n$$ AMI is adjusted against chance. NMI and MI are not. AMI $<0$: bad (i.e. independent labeling) $=0$ (random labeling) $\\simeq 1$ (agreement between the ground truth and predicted labels) References: C. D. Manning, P. Raghavan, H. Schütze. Introduction to Information Retrieval; Cambridge University Press: New York, 2008. pp. 356–359 (Good discussion.) In [4]: from clusim.clustering import Clustering import clusim.sim as sim true_labels = [ 1 , 1 , 1 , 2 , 2 , 2 , 3 , 3 , 3 ] predicted_labels = [ 1 , 2 , 2 , 3 , 3 , 1 , 1 , 1 , 1 ] single_cluster_labels = [ 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 ] completely_fragmented_labels = [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 ] # Their Data is Differently Formatted. true_clustering = Clustering () . from_membership_list ( true_labels ) predicted_clustering = Clustering () . from_membership_list ( predicted_labels ) predicted_single_cluster = Clustering () . from_membership_list ( single_cluster_labels ) predicted_completely_fragmented = Clustering () . from_membership_list ( completely_fragmented_labels ) for _ in [ predicted_clustering , predicted_single_cluster , predicted_completely_fragmented , ]: print ( f \"FMI = { sim . fowlkes_mallows_index ( true_clustering , _ ) } , NMI = { sim . nmi ( true_clustering , _ ) } , elem-cent = { sim . element_sim ( true_clustering , _ ) } \" ) # The Package Can Compute Many Scores such As... (code from Their Documentation https://hoosier-clusters.github.io/clusim/html/clusim.html) row_format2 = \" {:>25} \" * ( 2 ) for simfunc in sim . available_similarity_measures : print ( row_format2 . format ( simfunc , eval ( \"sim.\" + simfunc + \"(true_clustering, predicted_clustering)\" ) ) ) $n_{c,k}$: number of samples from class $c$ assigned to cluster $k$. In [3]: FMI = TP / np . sqrt (( TP + FP ) * ( TP + FN )) print ( FMI ) # You Can also Compute FMI Using Scikit-learn. The Results Match, of Course. print ( metrics . fowlkes_mallows_score ([ 0 , 0 , 1 , 1 ], [ 0 , 0 , 1 , 2 ])) # Also Two Same Partitions Will Have no Off-diagonal Elements in the Pair Confusion Matrix and a FMI Score of 1. C = pair_confusion_matrix ([ 0 , 0 , 1 , 1 ], [ 0 , 0 , 1 , 1 ]) print ( C ) TN = C [ 0 , 0 ] FP = C [ 0 , 1 ] FN = C [ 1 , 0 ] TP = C [ 1 , 1 ] FMI = TP / np . sqrt (( TP + FP ) * ( TP + FN )) print ( FMI ) print ( metrics . fowlkes_mallows_score ([ 0 , 0 , 1 , 1 ], [ 0 , 0 , 1 , 1 ])) 0.7071067811865475\n0.7071067811865476\n[[8 0]\n [0 4]]\n1.0\n1.0 All members of a class belongs to the same cluster. $c = 1 - \\frac{H(K|C)}{H(K)}$ $c \\in[0,1]$. higher is better. A. J. Gates et al. Element-Centric Clustering Comparison Unifies Overlaps and Hierarchy. Sci Rep 2019, 9 (1), 8574. DOI . NOTE: Most of this notebook's content is adapted—often copied!—from scikit-learn's excellent documentation on clustering . This is a compilation of definitions and code-snippets that I found useful while working on a project, along with some of my own thoughts. Element-centric similarity and issues with FMI and NMI ¶ $$\ns = \\frac{b - a}{\\max(a, b)}.\n$$ $b$: mean distance between a sample all other points in the next nearest cluster . Homogeneity score ($h$) is defined as\n$$\n\\begin{align*}\n&h = 1 - \\frac{H(C|K)}{H(C)}\\;\\text{where}\\\\\n&H(C) = - \\sum_{c=1}^{|C|} \\frac{n_c}{n} \\cdot \\log\\left(\\frac{n_c}{n}\\right)\\\\\n&H(C|K) = - \\sum_{c=1}^{|C|} \\sum_{k=1}^{|K|} \\frac{n_{c,k}}{n}\\cdot \\log\\left(\\frac{n_{c,k}}{n_k}\\right)\n\\end{align*}\n$$ 0 meaning overlapping clusters, and In [2]: from sklearn.metrics.cluster import pair_confusion_matrix from sklearn import metrics import numpy as np C = pair_confusion_matrix ([ 0 , 0 , 1 , 1 ], [ 0 , 0 , 1 , 2 ]) print ( C ) TN = C [ 0 , 0 ] FP = C [ 0 , 1 ] FN = C [ 1 , 0 ] TP = C [ 1 , 1 ] checking if the generated clusters are consistent. Two clusters can be compared using the pair-confusion Matrix. Davies-Bouldin Index ¶ $=0$ (random labeling) In [1]: In [1]: from sklearn import metrics labels_true = [ 0 , 0 , 0 , 1 , 1 , 1 ] labels_pred = [ 0 , 0 , 1 , 1 , 2 , 2 ] print ( metrics . homogeneity_score ( labels_true , labels_pred )) print ( metrics . completeness_score ( labels_true , labels_pred )) print ( metrics . v_measure_score ( labels_true , labels_pred , beta = 0.6 )) Scikit mentions a drawback: Important : Scikit returns the mean of all Silhouette coefficients of the samples. So, a higher silhouette score means better defined clusters. Fowlkes–Mallows Index ¶ AMI is adjusted against chance. NMI and MI are not. FMI = 0.4811252243246881, NMI = 0.5451600159416435, elem-cent = 0.5407407407407406\nFMI = 0.5, NMI = 0.0, elem-cent = 0.33333333333333326\nFMI = 0.0, NMI = 0.6666666666666665, elem-cent = 0.33333333333333326\n            jaccard_index                   0.3125\n               rand_index       0.6944444444444444\n            adjrand_index      0.26666666666666655\n    fowlkes_mallows_index       0.4811252243246881\n                 fmeasure      0.47619047619047616\n             purity_index       0.7777777777777777\n     classification_error      0.22222222222222232\n        czekanowski_index      0.47619047619047616\n               dice_index      0.47619047619047616\n           sorensen_index      0.47619047619047616\n    rogers_tanimoto_index       0.5319148936170213\n          southwood_index      0.45454545454545453\n      pearson_correlation      0.00102880658436214\n         corrected_chance      0.16994265720286564\n      sample_expected_sim      0.10526315789473684\n                      nmi       0.5451600159416435\n                       mi       0.8233232815796736\n                   adj_mi       0.3410389011275906\n                      rmi       0.1464053299155769\n                       vi       1.3738364418444755\n       geometric_accuracy       0.7777777777777778\n          overlap_quality                     -0.0\n                     onmi       0.6303315236619905\n              omega_index      0.26666666666666655 $-1$ meaning incorrect clustering, In [3]: C. D. Manning, P. Raghavan, H. Schütze. Introduction to Information Retrieval; Cambridge University Press: New York, 2008. pp. 356–359 (Good discussion.) In [4]: In [3]: FMI = TP / np . sqrt (( TP + FP ) * ( TP + FN )) print ( FMI ) # You Can also Compute FMI Using Scikit-learn. The Results Match, of Course. print ( metrics . fowlkes_mallows_score ([ 0 , 0 , 1 , 1 ], [ 0 , 0 , 1 , 2 ])) # Also Two Same Partitions Will Have no Off-diagonal Elements in the Pair Confusion Matrix and a FMI Score of 1. C = pair_confusion_matrix ([ 0 , 0 , 1 , 1 ], [ 0 , 0 , 1 , 1 ]) print ( C ) TN = C [ 0 , 0 ] FP = C [ 0 , 1 ] FN = C [ 1 , 0 ] TP = C [ 1 , 1 ] FMI = TP / np . sqrt (( TP + FP ) * ( TP + FN )) print ( FMI ) print ( metrics . fowlkes_mallows_score ([ 0 , 0 , 1 , 1 ], [ 0 , 0 , 1 , 1 ])) The second approach is the only option if no ground truths are available. $c = 1 - \\frac{H(K|C)}{H(K)}$ Gates et al. raise objections against using Fowlkes–Mallows Index and NMI, specifically NMI; they proposed a new one. Notebooks Silhouette coefficient ¶ For a single sample it is $$\ns = \\frac{b - a}{\\max(a, b)}.\n$$ $a$: mean distance between a sample all other points in the same cluster $b$: mean distance between a sample all other points in the next nearest cluster . $s$ takes values in $[-1,1]$ with $-1$ meaning incorrect clustering, 0 meaning overlapping clusters, and 1 meaning highly-dense, well-separated clusters. So, a higher silhouette score means better defined clusters. Important : Scikit returns the mean of all Silhouette coefficients of the samples. Scikit mentions a drawback: The Silhouette Coefficient is generally higher for convex clusters than other concepts of clusters, such as density based clusters like those obtained through DBSCAN. Fowlkes–Mallows Index ¶ Building on the pair-confusion matrix, FMI, a measure for cluster similarity, is computed using the elements of $C$ as\n$$FMI = \\frac{TP}{\\sqrt{(TP + FP) (TP + FN)}}.$$ FMI is useful because it gives a number—and not a matrix like the pair confusion matrix—for quickly comparing two clusterings. In [2]: Homogeneity Score ¶ Homogeneity score ($h$) is defined as\n$$\n\\begin{align*}\n&h = 1 - \\frac{H(C|K)}{H(C)}\\;\\text{where}\\\\\n&H(C) = - \\sum_{c=1}^{|C|} \\frac{n_c}{n} \\cdot \\log\\left(\\frac{n_c}{n}\\right)\\\\\n&H(C|K) = - \\sum_{c=1}^{|C|} \\sum_{k=1}^{|K|} \\frac{n_{c,k}}{n}\\cdot \\log\\left(\\frac{n_{c,k}}{n_k}\\right)\n\\end{align*}\n$$ $n$: total number of samples. $n_c$, $n_k$: number of samples in class $c$ and cluster $k$, respectively. $n_{c,k}$: number of samples from class $c$ assigned to cluster $k$. Higher $h$ means better clusters. $h$ takes values in $[0,1]$. Surprisingly, DB values closer to zero indicate a better partition , unlike Calinski-Harabasz index and Silhouette coefficient. FMI is useful because it gives a number—and not a matrix like the pair confusion matrix—for quickly comparing two clusterings. https://github.com/Hoosier-Clusters/clusim of Gates et al. with their package 0.7071067811865475\n0.7071067811865476\n[[8 0]\n [0 4]]\n1.0\n1.0 comparing how well the predicted clusters compare with the ground truth and AMI $<0$: bad (i.e. independent labeling) $=0$ (random labeling) $\\simeq 1$ (agreement between the ground truth and predicted labels) A. J. Gates, Y.-Y. Ahn. CluSim: A Python Package for Calculating Clustering Similarity. Journal of Open Source Software 2019, 4 (35), 1264. DOI . In [2]: from sklearn.metrics.cluster import pair_confusion_matrix from sklearn import metrics import numpy as np C = pair_confusion_matrix ([ 0 , 0 , 1 , 1 ], [ 0 , 0 , 1 , 2 ]) print ( C ) TN = C [ 0 , 0 ] FP = C [ 0 , 1 ] FN = C [ 1 , 0 ] TP = C [ 1 , 1 ] [[8 0]\n [2 2]] $c \\in[0,1]$. higher is better. Calinski Harabasz Index ¶ Calinski–Harabasz index (CHI) is the ratio of the sum of between-clusters dispersion for all clusters and sum of inter-cluster dispersion for all clusters. Better-defined clusters have higher CHI value. It's implemented in scikit as $v = \\frac{(1 + \\beta) \\times \\text{homogeneity} \\times \\text{completeness}}{(\\beta \\times \\text{homogeneity} + \\text{completeness})}$ $n_c$, $n_k$: number of samples in class $c$ and cluster $k$, respectively. $s$ takes values in $[-1,1]$ with Let's say, we have 2 sets of labels, $U$ and $V$, for $N$ objects. If $P(i)=|U_i|/N$ and $P'(j)=|V_i|/N$ are the probabilities that a randomly-picked object from $U$ ($V$) falls into class $U_i$ ($V_j$), then the entropies and the mutual information between $U$ and $V$ are computed as\n$$\n\\begin{aligned}\nH(U) &= - \\sum_{i=1}^{|U|}P(i)\\log(P(i))\\\\\nH(V) &= - \\sum_{j=1}^{|V|}P'(j)\\log(P'(j))\\\\\n\\text{MI}(U, V) &= \\sum_{i=1}^{|U|}\\sum_{j=1}^{|V|}P(i, j)\\log\\left(\\frac{P(i,j)}{P(i)P'(j)}\\right)\\\\\n&= \\sum_{i=1}^{|U|} \\sum_{j=1}^{|V|} \\frac{|U_i \\cap V_j|}{N}\\log\\left(\\frac{N|U_i \\cap V_j|}{|U_i||V_j|}\\right)\\\\\n\\text{NMI}(U, V) &= \\frac{\\text{MI}(U, V)}{\\text{mean}(H(U), H(V))}\\\\\n\\text{AMI} &= \\frac{\\text{MI} - E[\\text{MI}]}{\\text{mean}(H(U), H(V)) - E[\\text{MI}]}\n\\end{aligned}\n$$ For CHI and DBI formulas, check the referenced scikit doc page. In [1]: from sklearn import metrics labels_true = [ 0 , 0 , 0 , 1 , 1 , 1 ] labels_pred = [ 0 , 0 , 1 , 1 , 2 , 2 ] print ( metrics . homogeneity_score ( labels_true , labels_pred )) print ( metrics . completeness_score ( labels_true , labels_pred )) print ( metrics . v_measure_score ( labels_true , labels_pred , beta = 0.6 )) 0.6666666666666669\n0.420619835714305\n0.5467344787062375 from sklearn import metrics labels_true = [ 0 , 0 , 0 , 1 , 1 , 1 ] labels_pred = [ 0 , 0 , 1 , 1 , 2 , 2 ] print ( metrics . homogeneity_score ( labels_true , labels_pred )) print ( metrics . completeness_score ( labels_true , labels_pred )) print ( metrics . v_measure_score ( labels_true , labels_pred , beta = 0.6 )) Homogeneity Score ¶ 1 meaning highly-dense, well-separated clusters. $\\simeq 1$ (agreement between the ground truth and predicted labels) $<0$: bad (i.e. independent labeling) In [4]: from clusim.clustering import Clustering import clusim.sim as sim true_labels = [ 1 , 1 , 1 , 2 , 2 , 2 , 3 , 3 , 3 ] predicted_labels = [ 1 , 2 , 2 , 3 , 3 , 1 , 1 , 1 , 1 ] single_cluster_labels = [ 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 ] completely_fragmented_labels = [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 ] # Their Data is Differently Formatted. true_clustering = Clustering () . from_membership_list ( true_labels ) predicted_clustering = Clustering () . from_membership_list ( predicted_labels ) predicted_single_cluster = Clustering () . from_membership_list ( single_cluster_labels ) predicted_completely_fragmented = Clustering () . from_membership_list ( completely_fragmented_labels ) for _ in [ predicted_clustering , predicted_single_cluster , predicted_completely_fragmented , ]: print ( f \"FMI = { sim . fowlkes_mallows_index ( true_clustering , _ ) } , NMI = { sim . nmi ( true_clustering , _ ) } , elem-cent = { sim . element_sim ( true_clustering , _ ) } \" ) # The Package Can Compute Many Scores such As... (code from Their Documentation https://hoosier-clusters.github.io/clusim/html/clusim.html) row_format2 = \" {:>25} \" * ( 2 ) for simfunc in sim . available_similarity_measures : print ( row_format2 . format ( simfunc , eval ( \"sim.\" + simfunc + \"(true_clustering, predicted_clustering)\" ) ) ) FMI = 0.4811252243246881, NMI = 0.5451600159416435, elem-cent = 0.5407407407407406\nFMI = 0.5, NMI = 0.0, elem-cent = 0.33333333333333326\nFMI = 0.0, NMI = 0.6666666666666665, elem-cent = 0.33333333333333326\n            jaccard_index                   0.3125\n               rand_index       0.6944444444444444\n            adjrand_index      0.26666666666666655\n    fowlkes_mallows_index       0.4811252243246881\n                 fmeasure      0.47619047619047616\n             purity_index       0.7777777777777777\n     classification_error      0.22222222222222232\n        czekanowski_index      0.47619047619047616\n               dice_index      0.47619047619047616\n           sorensen_index      0.47619047619047616\n    rogers_tanimoto_index       0.5319148936170213\n          southwood_index      0.45454545454545453\n      pearson_correlation      0.00102880658436214\n         corrected_chance      0.16994265720286564\n      sample_expected_sim      0.10526315789473684\n                      nmi       0.5451600159416435\n                       mi       0.8233232815796736\n                   adj_mi       0.3410389011275906\n                      rmi       0.1464053299155769\n                       vi       1.3738364418444755\n       geometric_accuracy       0.7777777777777778\n          overlap_quality                     -0.0\n                     onmi       0.6303315236619905\n              omega_index      0.26666666666666655 Arghya Dutta Notebooks V-Measure ¶ 0.6666666666666669\n0.420619835714305\n0.5467344787062375 $a$: mean distance between a sample all other points in the same cluster Pair confusion matrix ¶ Two clusters can be compared using the pair-confusion Matrix. from sklearn.metrics.cluster import pair_confusion_matrix from sklearn import metrics import numpy as np C = pair_confusion_matrix ([ 0 , 0 , 1 , 1 ], [ 0 , 0 , 1 , 2 ]) print ( C ) TN = C [ 0 , 0 ] FP = C [ 0 , 1 ] FN = C [ 1 , 0 ] TP = C [ 1 , 1 ] FMI = TP / np . sqrt (( TP + FP ) * ( TP + FN )) print ( FMI ) # You Can also Compute FMI Using Scikit-learn. The Results Match, of Course. print ( metrics . fowlkes_mallows_score ([ 0 , 0 , 1 , 1 ], [ 0 , 0 , 1 , 2 ])) # Also Two Same Partitions Will Have no Off-diagonal Elements in the Pair Confusion Matrix and a FMI Score of 1. C = pair_confusion_matrix ([ 0 , 0 , 1 , 1 ], [ 0 , 0 , 1 , 1 ]) print ( C ) TN = C [ 0 , 0 ] FP = C [ 0 , 1 ] FN = C [ 1 , 0 ] TP = C [ 1 , 1 ] FMI = TP / np . sqrt (( TP + FP ) * ( TP + FN )) print ( FMI ) print ( metrics . fowlkes_mallows_score ([ 0 , 0 , 1 , 1 ], [ 0 , 0 , 1 , 1 ])) Element-centric similarity and issues with FMI and NMI ¶ Gates et al. raise objections against using Fowlkes–Mallows Index and NMI, specifically NMI; they proposed a new one. References: A. J. Gates et al. Element-Centric Clustering Comparison Unifies Overlaps and Hierarchy. Sci Rep 2019, 9 (1), 8574. DOI . A. J. Gates, Y.-Y. Ahn. CluSim: A Python Package for Calculating Clustering Similarity. Journal of Open Source Software 2019, 4 (35), 1264. DOI . https://github.com/Hoosier-Clusters/clusim of Gates et al. with their package from clusim.clustering import Clustering import clusim.sim as sim true_labels = [ 1 , 1 , 1 , 2 , 2 , 2 , 3 , 3 , 3 ] predicted_labels = [ 1 , 2 , 2 , 3 , 3 , 1 , 1 , 1 , 1 ] single_cluster_labels = [ 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 ] completely_fragmented_labels = [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 ] # Their Data is Differently Formatted. true_clustering = Clustering () . from_membership_list ( true_labels ) predicted_clustering = Clustering () . from_membership_list ( predicted_labels ) predicted_single_cluster = Clustering () . from_membership_list ( single_cluster_labels ) predicted_completely_fragmented = Clustering () . from_membership_list ( completely_fragmented_labels ) for _ in [ predicted_clustering , predicted_single_cluster , predicted_completely_fragmented , ]: print ( f \"FMI = { sim . fowlkes_mallows_index ( true_clustering , _ ) } , NMI = { sim . nmi ( true_clustering , _ ) } , elem-cent = { sim . element_sim ( true_clustering , _ ) } \" ) # The Package Can Compute Many Scores such As... (code from Their Documentation https://hoosier-clusters.github.io/clusim/html/clusim.html) row_format2 = \" {:>25} \" * ( 2 ) for simfunc in sim . available_similarity_measures : print ( row_format2 . format ( simfunc , eval ( \"sim.\" + simfunc + \"(true_clustering, predicted_clustering)\" ) ) ) Mutual-information-based similarity score ¶ The Silhouette Coefficient is generally higher for convex clusters than other concepts of clusters, such as density based clusters like those obtained through DBSCAN. There are several coefficients that quantify the quality of clustering. Some that do not need a ground truth are Silhouette coefficient, Calinski–Harabasz index, and Davies–Bouldin index. Others such as homogeneity Score, completeness score, and V Measure need a ground truth. Evaluating Clustering Performance NOTE: Most of this notebook's content is adapted—often copied!—from scikit-learn's excellent documentation on clustering . This is a compilation of definitions and code-snippets that I found useful while working on a project, along with some of my own thoughts. Evaluation of quality of clusters is often done in two ways: comparing how well the predicted clusters compare with the ground truth and checking if the generated clusters are consistent. The second approach is the only option if no ground truths are available. There are several coefficients that quantify the quality of clustering. Some that do not need a ground truth are Silhouette coefficient, Calinski–Harabasz index, and Davies–Bouldin index. Others such as homogeneity Score, completeness score, and V Measure need a ground truth. Pair confusion matrix ¶ The harmonic mean of the homogeneity score and completeness scores: $v = 2 \\cdot \\frac{h \\cdot c}{h + c}$. Higher $h$ means better clusters. $h$ takes values in $[0,1]$."
  },
  {
    "title": "Statistical Mechanics: Few introductory books and papers",
    "url": "https://arghyadutta.github.io/notebooks/statMechCourse.html",
    "content": "Books Some books and papers that I recommend in my statistical mechanics courses at undergrad and grad level. For more, see Statistical Mechanics: Resources . Reif, F. Statistical Physics: Berkeley Physics Course, Vol. 5 ; Mcgraw-Hill College, 1967. Böttcher, L.; Herrmann, H. J. (2021) Computational Statistical Physics , Cambridge University Press Kennett, M. P. (2020) Essential Statistical Physics , 1st ed.; Cambridge University Press Huang, K. (1987). Statistical mechanics (2nd ed). Wiley. Baez, J. C. What Is Entropy? arXiv September 13, 2024 . Broad yet concise. The chapter on the general properties of the partition function has a nice discussion on the zeroes of the partition function and connections to phase transition. Entropy Bhattacharjee, J. K. Entropy à La Boltzmann . Resonance 2001, 6 (9), 19–34 . Much more detailed than the ones listed so far. Styer, D. Entropy as Disorder: History of a Misconception . The Physics Teacher 2019, 57 (7), 454–458 . Böttcher's and Kennett's are two beginner friendly, undergrad-level recent books covering computational and theoretical aspects of statistical mechanics. Bhattacharjee, S. M. Entropy and Perpetual Computers . arXiv October 29, 2003 ."
  },
  {
    "title": "Academic Life: Features",
    "url": "https://arghyadutta.github.io/notebooks/academiaFeatures.html",
    "content": "Recommended Marder, E. (2013). Grandmother elephants . eLife, 2, e01140 . Mermin, N. D. (1990). Commencement address at St. John's College, Santa Fe in Boojums all the way through: Communicating science in a prosaic age. Cambridge University Press. In stead of discussing how society is systematically undermining—and mocking—any vestige of scholarly life that's still there, let me just point to some articles and books that show why scholarship is still a worthy vocation. On learning from the older generation of scientists. Marder is a brilliant scientist and a perceptive writer—read her work. Gadagkar, R. (2012). The Luxury of Introspection . In Uber das Kolleg hinaus (pp.152–157). Wissenschaftskolleg zu Berlin, Germany. PDF . Hitz, Z. (2020). Lost in Thought: The Hidden Pleasures of an Intellectual Life (First Edition). Princeton University Press. On how scientific methods help us to make sense of the world. Delightful essay on a scholar and his notes. Thomas, K. (2010). Working Methods . London Review of Books Hitz, Z. (2023, September 1). What Is Time For? Plough"
  },
  {
    "title": "Quantum Mechanics via Moment Dynamics",
    "url": "https://arghyadutta.github.io/notebooks/dynamics.html",
    "content": "Sarkar, P., & Bhattacharjee, J. K. (2020). Nonlinear parametric oscillator: A tool for probing quantum fluctuations . Physics Review E, 102(5), 052204 . Recommended Can we study a quintessential quantum phenomenon—such as tunneling—by analyzing dynamical equations of the moments of the quantum distribution? Biswas, S., Chattopadhyay, R., & Bhattacharjee, J. K. (2018). Propagation of arbitrary initial wave packets in a quantum parametric oscillator: Instability zones for higher order moments . Physics Letters A, 382(18), 1202–1206 Ray, S., Bhattacharyya, S., & Bhattacharjee, J. K. (2024). Dynamical System Description of Quantum Tunneling in a Double Well Potential . Physics Letters A, 130174 . Neat. Also see other notes by Wheeler. (This is not John, but Nicholas Wheeler. A colleague of David Griffiths at the Reed college) Ballentine, L. E., & McRae, S. M. (1998). Moment equations for probability distributions in classical and quantum mechanics . Physical Review A, 58(3), 1799–1809 . Sarkar, P., Chattopadhyay, R., & Bhattacharjee, J. K. (2024). Quantum dynamics of wave packets in a Morse potential: A dynamical system approach . Physical Review E, 110(3), 034207 . Chawla, R., & Bhattacharjee, J. K. (2019). Quantum dynamics from fixed points and their stability . European Physical Journal B, 92(9), 196 . Wheeler, N. (1998). Remarks Concerning the Status & Some Ramifications of Ehrenfest’s Theorem . PDF"
  },
  {
    "title": "Bayesian Methods",
    "url": "https://arghyadutta.github.io/notebooks/bayesian.html",
    "content": "Recommended van de Schoot, R.; Depaoli, S.; King, R.; Kramer, B.; Märtens, K.; Tadesse, M. G.; Vannucci, M.; Gelman, A.; Veen, D.; Willemsen, J.; Yau, C. Bayesian Statistics and Modelling . Nat Rev Methods Primers 2021, 1 (1), 1–26 . Online Resources Bayesian methods course by Richard McElreath Titelbaum, M. G. How to Think like a Bayesian . Psyche. 2024 . (accessed 2024-01-29). Neat description of Bayesian ideas (it's a neuroscience book). Eddy, S. R. What Is Bayesian Statistics? Biotechnol 2004, 22 (9), 1177–1178 . The most beginner friendly introduction to Bayesian statistics that I've came across. Statistics for application by Phillippe Rigollet Ma, W. J.; Kording, K.; Goldreich, D. Bayesian Models of Perception and Action: An Introduction ; The MIT Press: London, England, 2023"
  },
  {
    "title": "Linear algebra",
    "url": "https://arghyadutta.github.io/notebooks/linearAlgebra.html",
    "content": "Also see: Basic Mathematics for Quantum Mechanics The Matrix Cookbook by Kaare Brandt Petersen and Michael Syskind Pedersen Singular Value Decomposition by Steven Brunton. Online resources"
  },
  {
    "title": "Teaching",
    "url": "https://arghyadutta.github.io/notebooks/teaching.html",
    "content": "Su, F. (2013). The Lesson of Grace in Teaching . His blog (2013) Recommended \"Your accomplishments are NOT what make you a worthy human being. You learn this lesson by receiving GRACE: good things you didn't earn or deserve, but you're getting them anyway.\" Hoare, E. (2025). Gentleness in Academia . Plough ."
  },
  {
    "title": "Genetic Algorithm",
    "url": "https://arghyadutta.github.io/notebooks/geneticAlgorithm.html",
    "content": "Recommended Procedure: It defines a population of chromosomes, which are candidate solutions to the optimization problem). It then does the following operations on the initial set of parents: selection, crossover, mutation, elicitation. (pp. 2–4, Carr 2014) Kim, C., Batra, R., Chen, L., Tran, H., & Ramprasad, R. (2021). Polymer design using genetic algorithm and machine learning . Computational Materials Science, 186, 110067 . One practical example is shown in Kim et al. 2021 who designed new polymers starting from a set of parent polymers with the goal of achieving high glass-transition temperature $T_\\text{g}$ (to ensure mechanical stability at high temperatures) and high band-gap $E_\\text{g}$ (to ensure protection from dielectric breakdown). They proposed 192 new polymers that may have that function based on designing new polymers using GA and then evaluating the $T_\\text{g}$ and $E_\\text{g}$ of those from pre-trained ML models. GA is a type of evolutionary computing. It imitates biological evolution to find the `fittest' solution to an optimization problem. So, it is another optimization algorithm like gradient descent or exhaustive search. The fitness is determined by a fitness function . Carr, J. (2014). An Introduction to Genetic Algorithms . PDF"
  },
  {
    "title": "Physics Free Books",
    "url": "https://arghyadutta.github.io/notebooks/physicsFreeBooks.html",
    "content": "Lecture notes on Soft Matter and Interfaces by Lydéric Bocquet Lecture notes on Statistical Physics by Lydéric Bocquet Entropy by John Baez Electrodynamics by Phil Nelson (excellent book!) Feynman lectures Online course on topology in condensed matter Group Theory in Physics: An Introduction with Mathematica by Balasubramanian Ananthanarayan, Souradeep Das, Amitabha Lahiri, Suhas Sheikh, Sarthak Talukdar Understanding the Properties of Matter by Michael de Podesta"
  },
  {
    "title": "Complex Systems",
    "url": "https://arghyadutta.github.io/notebooks/complexity.html",
    "content": "Recommended Chandler, D. (1987). Introduction to Modern Statistical Mechanics . Oxford University Press. (Excellent book.) Bianconi, G. et al. (2023). Complex systems in the spotlight: Next steps after the 2021 Nobel Prize in Physics . Journal of Physics: Complexity, 4(1), 010201 . Taroni, A. (2015). 90 years of the Ising model . Nature Physics, 11(12), 997–997. (for the story) Mitchell, M. (2009). Complexity: A guided tour . Oxford University Press. Anderson, P. W. (1972). More Is Different: Broken symmetry and the nature of the hierarchical structure of science . Science, 177(4047), 393–396 Parisi, G. (2002). Complex Systems: A Physicist’s Viewpoint arXiv:cond-mat/0205297 For Ising Model: Böttcher, L., & Herrmann, H. J. (2021). Computational Statistical Physics (1st ed.). Cambridge University Press. If I get time For Computational Physics Crutchfield, J. P., & Young, K. (1989). Inferring statistical complexity . Physical Review Letters, 63(2), 105–108 ."
  },
  {
    "title": "Physics: Visualizations and Experiments",
    "url": "https://arghyadutta.github.io/notebooks/physicsExperiments.html",
    "content": "Triple Point of Water Sublimation of iodine Quantum mechanics: Stern and Gerlach experiment Marangoni Bursting: Evaporation-Induced Emulsification of a Two-Component Droplet Critical Point of carbon dioxide Melting of a cube of gold metal using the embedded atom method (eam) force field Quantum mechanics: Wave Particle Duality Brownian motion Cameras and Lenses A Nobel laureate and a flea circus join forces for an unforgettable demonstration of inertia Flight manifest: from take-off to landing, a bird’s eye introduction to flying"
  },
  {
    "title": "Socialism",
    "url": "https://arghyadutta.github.io/notebooks/socialism.html",
    "content": "Russell, B. (1956). Why I am Not a Communist . In Portraits from Memory. https://www.rjgeib.com/thoughts/opiate/why.html Recommended One notable feature of this book is that it scarcely discussed about Marxism as implemented in Russia, North Vietnam, and China. Instead we get a thorough discussion of Cuban and Sweden’s socialism, which is interesting! Also the enrichment, and fragmentation, of Socialist ideal after the rise of Feminist and Left-Green movement were not left. The author is not oblivious to the issues of the system, but he doesn't shy away from pointing out the strengths. Recommended! In the first part, he presents some arguments to convince the reader that Lenin, in the short time he was in power, had shown remarkable similarity with Stalin: punishing and 'liquidating' dissenters, being ruthless with the concerns of common Russians, specially peasants, overruling the possibility of a democracy—at times berating it—, and taking brutal steps to implement communist policies. Gilabert, P., & O’Neill, M. (2024). Socialism . In E. N. Zalta & U. Nodelman (Eds.). The Stanford Encyclopedia of Philosophy Gellately, R. (2008). Lenin, Stalin, and Hitler: The Age of Social Catastrophe (Reprint edition). Vintage. Gellately sometimes has placed opinions, which are not well supported by data presented in the text, in between well-argued points. Though they do not contradict his claims, I found that problematic. Also while describing the show trials, he did not present the allegations, however superficial they may be, of the Communist party against its members. The clashes between the Nazis and the Communist party members of Germany are somewhat less described. Overall, I will recommend this book to anyone interested in the modus operandi of totalitarian regimes. Mommsen, P. (2025, March 3). The Quest to Emancipate Labor. Plough Edwards, J., & Leiter, B. (2025). Marx . Routledge, Taylor & Francis group. PhilosophyInsights (Director). (2018, May 23). Stephen Hicks: How Failed Marxist Predictions Led to the Postmodern Left YouTube More A mostly cogent analysis of the similarities and dissimilarities between the totalitarian regimes of Lenin, Stalin, and Hitler—and he anticipated that the inclusion of Lenin with the other two will shock many. In the first part, he presents some arguments to convince the reader that Lenin, in the short time he was in power, had shown remarkable similarity with Stalin: punishing and 'liquidating' dissenters, being ruthless with the concerns of common Russians, specially peasants, overruling the possibility of a democracy—at times berating it—, and taking brutal steps to implement communist policies. In the subsequent sections, he shows parallels between Hitler and Stalin. His account of Stalin is particularly well written. While dekulakizing USSR, after the number of rich, and yet unpunished, peasants reached a really low number, it became difficult to determine \"who is a kulak?\". To settle that it was decided that any family with two samovars or a \"status symbol\" was to be considered as a kulak. One astonishing fact about that period is recent analyses show that the cheap slave labour by Gulag inmates was actually counter-productive for Russian economy. Like the common Russians, the party members also faced Stalin's wrath. According to Stalin, if any member committed suicide to evade a show trial, he \"is covering his track and deceiving the party for one last time. Suicide is simply a method to spit on the party for the last time\". The Communist party received enormous support from the citizens, mainly urban population. At the time of show trial of Nikolai Bukharin, after complications regarding the New Economic Policy, many prominent literary figures singed an open letter saying \"We demand the spies' execution! We shall not allow the enemies of the Soviet Union to live\". The writers include, among others, Mikhail Sholokov, Alexei Tolstoy, and surprisingly, Vasily Grossman, who was once saved from imprisonment by Bukharin's direct help. Grossman demanded \"No mercy to the Trotskyite degenerates, the murderous accomplishes of fascism.\" I mean, wow. Gellately sometimes has placed opinions, which are not well supported by data presented in the text, in between well-argued points. Though they do not contradict his claims, I found that problematic. Also while describing the show trials, he did not present the allegations, however superficial they may be, of the Communist party against its members. The clashes between the Nazis and the Communist party members of Germany are somewhat less described. Overall, I will recommend this book to anyone interested in the modus operandi of totalitarian regimes. In the subsequent sections, he shows parallels between Hitler and Stalin. His account of Stalin is particularly well written. While dekulakizing USSR, after the number of rich, and yet unpunished, peasants reached a really low number, it became difficult to determine \"who is a kulak?\". To settle that it was decided that any family with two samovars or a \"status symbol\" was to be considered as a kulak. One astonishing fact about that period is recent analyses show that the cheap slave labour by Gulag inmates was actually counter-productive for Russian economy. Like the common Russians, the party members also faced Stalin's wrath. According to Stalin, if any member committed suicide to evade a show trial, he \"is covering his track and deceiving the party for one last time. Suicide is simply a method to spit on the party for the last time\". A lucid and thoughtful book; the author has done an impressive job. Not entirely on socialism, but it touches on Marx's alienation. I think the author is hopelessly romantic—but then what’s life if one can't even dream? After describing the basic tenets of Socialism, the author discussed about the existent Socialist tradition before Marx came up with his scientific socialism. I liked the informative analysis on the early Utopians like the popular, but theoretically not so convincing, Etienne Cabet and more sound theorists like Henri de Saint-Simon, Charles Fourier, and Robert Owens. Then the Anarchist theories of Proudhon and Bakunin were also discussed in some detail. This studies were interspersed with the author’s thoughtful comment throughout, which I liked. And here is one slightly unnerving quote from Proudhon: To be governed is to be watched over, inspected, spied on, directed, legislated at, regulated, docketed, indoctrinated, preached at, controlled, assessed, weighed, censored, ordered about, by men who have neither the right nor the knowledge nor the virtue…. That’s government, that’s its justice, that’s its morality! If I get time The Communist party received enormous support from the citizens, mainly urban population. At the time of show trial of Nikolai Bukharin, after complications regarding the New Economic Policy, many prominent literary figures singed an open letter saying \"We demand the spies' execution! We shall not allow the enemies of the Soviet Union to live\". The writers include, among others, Mikhail Sholokov, Alexei Tolstoy, and surprisingly, Vasily Grossman, who was once saved from imprisonment by Bukharin's direct help. Grossman demanded \"No mercy to the Trotskyite degenerates, the murderous accomplishes of fascism.\" I mean, wow. A mostly cogent analysis of the similarities and dissimilarities between the totalitarian regimes of Lenin, Stalin, and Hitler—and he anticipated that the inclusion of Lenin with the other two will shock many. Ferretter, L. (2006). Louis Althusser (R. Eaglestone, Ed.; 1st edition). Routledge. Arrow offers a careful analysis of capitalism and socialism, without asking you to join a camp. Arrow, K. J. (1978). A Cautious Case for Socialism . Dissent Magazine Newman, M. Socialism: A Very Short Introduction . Oxford University Press, 2005. My thoughts A lucid and thoughtful book; the author has done an impressive job. After describing the basic tenets of Socialism, the author discussed about the existent Socialist tradition before Marx came up with his scientific socialism. I liked the informative analysis on the early Utopians like the popular, but theoretically not so convincing, Etienne Cabet and more sound theorists like Henri de Saint-Simon, Charles Fourier, and Robert Owens. Then the Anarchist theories of Proudhon and Bakunin were also discussed in some detail. This studies were interspersed with the author’s thoughtful comment throughout, which I liked. And here is one slightly unnerving quote from Proudhon: To be governed is to be watched over, inspected, spied on, directed, legislated at, regulated, docketed, indoctrinated, preached at, controlled, assessed, weighed, censored, ordered about, by men who have neither the right nor the knowledge nor the virtue…. That’s government, that’s its justice, that’s its morality! One notable feature of this book is that it scarcely discussed about Marxism as implemented in Russia, North Vietnam, and China. Instead we get a thorough discussion of Cuban and Sweden’s socialism, which is interesting! Also the enrichment, and fragmentation, of Socialist ideal after the rise of Feminist and Left-Green movement were not left. The author is not oblivious to the issues of the system, but he doesn't shy away from pointing out the strengths. Recommended! Thorough summary; recommended."
  },
  {
    "title": "Art",
    "url": "https://arghyadutta.github.io/notebooks/art.html",
    "content": "The Drawing Database-Northern Kentucky University (Director). (2020, August 18). Art History & Drawing: 15 Minutes with Kathe Kollwitz YouTube Modern vietnamese artist.I love the way she uses colors and patterns. I haven't seen anything quite like Paske's Surreal, melancholic, otherworldly art. Yuki Kawae Favorite artists Xuan Loc Xuan Jeannie Lynn Paske Józef Wilkoń LearnFree (Director). (2016, October 5). Beginning Graphic Design: Color . YouTube Nan Goldin How to design things—doors, software interfaces, teapots, switches—so that people find them delightful to use? Some of his works Mangla, R. (2015, June 8). A Brief History of Ultramarine—The World’s Costliest Color . The Paris Review . Design Bentley, T. (2017, July 15). The Obsessive Art and Great Confession of Charlotte Salomon . The New Yorker . Geometric patterns on sand. Meditative, ephemeral. Norman, D. (2013). The Design of Everyday Things (2nd edition). Basic Books. Ernst Barlach Käthe Kollwitz Colors Wonderful. Gregory Fromenteau Charlotte Salomon Surreal, slow. Jamie Windsor. (2019, October 29). Wabi-Sabi: When Bad Photos Are Better YouTube Victoria and Albert Museum (Director). (2018, June 6). In Search of Forgotten Colours—Sachio Yoshioka and the Art of Natural Dyeing YouTube"
  },
  {
    "title": "Philosophy: Online Resources",
    "url": "https://arghyadutta.github.io/notebooks/philosophyResources.html",
    "content": "Project Gutenberg Digital Library of Darshan Manisha (Texts on Indian Philosophy) archive.org Two rather obvious entries, but the more I explore them, the more I get pleasantly surprised. They are true treasure troves. Fadedpage Canada's project Gutenberg. Surprisingly rich collection. Open Greek and Latin Perseus Digital Library Classic books (from the U.S. Library of Congress)"
  },
  {
    "title": "Mathematical Methods in Physics: Resources",
    "url": "https://arghyadutta.github.io/notebooks/mathematicalMethods.html",
    "content": "Recommended Visual group theory Special functions The World of Mathematical Equations Garg, A. (2023) Mathematics with a Scientific Sensibility ; Northwestern University. Visualizing prime factors Excellent book and the author has made it freely available ! Balakrishnan, V. (2020). Mathematical Physics: Applications and Problems . Springer International Publishing. Index to Catalogue of Lattices Scholarpedia Short, insightful notes by Markus Deserno Affine transformations"
  },
  {
    "title": "Machine Learning: Free Books",
    "url": "https://arghyadutta.github.io/notebooks/mlFreeBooks.html",
    "content": "Model-Based Machine Learning by John Winn Information Theory, Inference, and Learning Algorithms by David MacKay Deep Learning for Molecules and Materials by Andrew D. White The Little Book of Deep Learning by François Fleuret. He formatted the book for reading on smartphones—such innovative people! Interpretable Machine Learning by Christoph Molnar An Introduction to Statistical Learning by Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani The Elements of Statistical Learning by Trevor Hastie, Robert Tibshirani, Jerome Friedman"
  },
  {
    "title": "Self-avoiding Random Walks",
    "url": "https://arghyadutta.github.io/notebooks/saw.html",
    "content": "Recommended There are no known method to calculate $c_n$ other than numerical enumeration and asymptotic analysis, as far as I know. The following table shows the length of largest SAWs that have been enumerated, with $c_n$ often becoming as large as $10^{30}$: Lawler, G. F. (2011). Scaling limits and the Schramm-Loewner evolution . Probability Surveys, 8 . Nienhuis, B. (1982). Exact Critical Point and Critical Exponents of $\\mathrm{O}(n)$ Models in Two Dimensions . Physical Review Letters, 49(15), 1062–1065 . The non-universal amplitudes $B$ and $C$ depend on lattice symmetry and dimension. $c_n$ is conjectured to behave as: $c_n \\approx A \\mu^n n^{\\gamma -1}$ where $\\mu$ is the connective constant (Sometimes it's called growth constant and $k=\\log \\mu$ is called connective constant). Jensen, I. (2004). Self-avoiding walks and polygons on the triangular lattice . Journal of Statistical Mechanics: Theory and Experiment, 2004(10), P10008 . However, the $\\nu$ and $\\gamma$ only depend on the dimension—they are universal. Asymptotically, $\\langle R_\\mathrm{ g}^2\\rangle$ and $\\langle R_\\mathrm{ e}^2\\rangle$ are conjectured to behave as $\\langle R_\\mathrm{ e}^2\\rangle \\approx BN^{2\\nu}$ and $\\langle R_\\mathrm{ g}^2\\rangle \\approx CN^{2\\nu}$. Clisby, N., & Dünweg, B. (2016). High-precision estimate of the hydrodynamic radius for self-avoiding walks . Physical Review E, 94(5), 052102 . Let $c_n$ be the number and $R_e$ and $R_g$ be the end-to-end distance and radius of gyration of a $n$-step self-avoiding random walk with vertices at $\\omega_0,\\omega_2,\\cdots,\\omega_n$. Then the radius of gyration and end-to-end distance, two relevant measures quantifying the \"size\" the SAW, are defined as follows: Asymptotic results (large $n$) Numerical Results There are no known method to calculate $c_n$ other than numerical enumeration and asymptotic analysis, as far as I know. The following table shows the length of largest SAWs that have been enumerated, with $c_n$ often becoming as large as $10^{30}$: $d$ $\\max(n)$ Symmetry Reference 2 71 Square Jensen (2004) 3 36 Simple cubic Lawler (2011) 3 28 BCC Schram (2017) 3 24 FCC Schram (2017) Asymptotic results (large $n$) Asymptotically, $\\langle R_\\mathrm{ g}^2\\rangle$ and $\\langle R_\\mathrm{ e}^2\\rangle$ are conjectured to behave as $\\langle R_\\mathrm{ e}^2\\rangle \\approx BN^{2\\nu}$ and $\\langle R_\\mathrm{ g}^2\\rangle \\approx CN^{2\\nu}$. The non-universal amplitudes $B$ and $C$ depend on lattice symmetry and dimension. $c_n$ is conjectured to behave as: $c_n \\approx A \\mu^n n^{\\gamma -1}$ where $\\mu$ is the connective constant (Sometimes it's called growth constant and $k=\\log \\mu$ is called connective constant). The non-universal amplitude $A$ depends on both the dimension and lattice symmetry. However, the $\\nu$ and $\\gamma$ only depend on the dimension—they are universal. $d$ $\\nu$ $\\gamma$ $c_n\\approx$ Comment Reference 2 $3/4$ $43/32 = 1.343 75$ $A \\mu^n n^{11/32}$ Exact Nienhuis (1982), Nienhuis (1984) 3 0.58759700(40)$\\approx 3/5$ $1.15695300(95) \\approx 7/6$ $A \\mu^n n^{1/6}$ Estimate Schram (2017), Slade (2019) (for summary) 4 $1/2$ with log correction $\\langle R_\\mathrm{ e}^2\\rangle \\approx B n(\\log n)^{1/4}$ $\\langle R_\\mathrm{ g}^2\\rangle \\approx C n(\\log n)^{1/4}$ 1 with log correction $A \\mu^n (\\log n)^{1/4}$ Upper critical dimension Slade (2019) $\\geq 5$ 1/2 1 $A \\mu^n$ Probably exact Slade (2019), p.4 Recommended Clisby, N., & Dünweg, B. (2016). High-precision estimate of the hydrodynamic radius for self-avoiding walks . Physical Review E, 94(5), 052102 . Jensen, I. (2004). Self-avoiding walks and polygons on the triangular lattice . Journal of Statistical Mechanics: Theory and Experiment, 2004(10), P10008 . Lawler, G. F. (2011). Scaling limits and the Schramm-Loewner evolution . Probability Surveys, 8 . Nienhuis, B. (1982). Exact Critical Point and Critical Exponents of $\\mathrm{O}(n)$ Models in Two Dimensions . Physical Review Letters, 49(15), 1062–1065 . Nienhuis, B. (1984). Critical behavior of two-dimensional spin models and charge asymmetry in the Coulomb gas . Journal of Statistical Physics, 34(5), 731–761 . Schram, R. D., Barkema, G. T., Bisseling, R. H., & Clisby, N. (2017). Exact enumeration of self-avoiding walks on BCC and FCC lattices . Journal of Statistical Mechanics: Theory and Experiment, 2017(8), 083208 . Slade, G. (2019). Self-avoiding walk, spin systems and renormalization . Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences, 475(2221), 20180549 . Asymptotic results (large $n$) Asymptotically, $\\langle R_\\mathrm{ g}^2\\rangle$ and $\\langle R_\\mathrm{ e}^2\\rangle$ are conjectured to behave as $\\langle R_\\mathrm{ e}^2\\rangle \\approx BN^{2\\nu}$ and $\\langle R_\\mathrm{ g}^2\\rangle \\approx CN^{2\\nu}$. The non-universal amplitudes $B$ and $C$ depend on lattice symmetry and dimension. $c_n$ is conjectured to behave as: $c_n \\approx A \\mu^n n^{\\gamma -1}$ where $\\mu$ is the connective constant (Sometimes it's called growth constant and $k=\\log \\mu$ is called connective constant). The non-universal amplitude $A$ depends on both the dimension and lattice symmetry. However, the $\\nu$ and $\\gamma$ only depend on the dimension—they are universal. $d$ $\\nu$ $\\gamma$ $c_n\\approx$ Comment Reference 2 $3/4$ $43/32 = 1.343 75$ $A \\mu^n n^{11/32}$ Exact Nienhuis (1982), Nienhuis (1984) 3 0.58759700(40)$\\approx 3/5$ $1.15695300(95) \\approx 7/6$ $A \\mu^n n^{1/6}$ Estimate Schram (2017), Slade (2019) (for summary) 4 $1/2$ with log correction $\\langle R_\\mathrm{ e}^2\\rangle \\approx B n(\\log n)^{1/4}$ $\\langle R_\\mathrm{ g}^2\\rangle \\approx C n(\\log n)^{1/4}$ 1 with log correction $A \\mu^n (\\log n)^{1/4}$ Upper critical dimension Slade (2019) $\\geq 5$ 1/2 1 $A \\mu^n$ Probably exact Slade (2019), p.4 Recommended Clisby, N., & Dünweg, B. (2016). High-precision estimate of the hydrodynamic radius for self-avoiding walks . Physical Review E, 94(5), 052102 . Jensen, I. (2004). Self-avoiding walks and polygons on the triangular lattice . Journal of Statistical Mechanics: Theory and Experiment, 2004(10), P10008 . Lawler, G. F. (2011). Scaling limits and the Schramm-Loewner evolution . Probability Surveys, 8 . Nienhuis, B. (1982). Exact Critical Point and Critical Exponents of $\\mathrm{O}(n)$ Models in Two Dimensions . Physical Review Letters, 49(15), 1062–1065 . Nienhuis, B. (1984). Critical behavior of two-dimensional spin models and charge asymmetry in the Coulomb gas . Journal of Statistical Physics, 34(5), 731–761 . Schram, R. D., Barkema, G. T., Bisseling, R. H., & Clisby, N. (2017). Exact enumeration of self-avoiding walks on BCC and FCC lattices . Journal of Statistical Mechanics: Theory and Experiment, 2017(8), 083208 . Slade, G. (2019). Self-avoiding walk, spin systems and renormalization . Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences, 475(2221), 20180549 . The non-universal amplitude $A$ depends on both the dimension and lattice symmetry. Nienhuis, B. (1984). Critical behavior of two-dimensional spin models and charge asymmetry in the Coulomb gas . Journal of Statistical Physics, 34(5), 731–761 . Slade, G. (2019). Self-avoiding walk, spin systems and renormalization . Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences, 475(2221), 20180549 . To get an idea about the size of $n$-step SAWs, one computes their values averaged over all $n$-step SAWs: Schram, R. D., Barkema, G. T., Bisseling, R. H., & Clisby, N. (2017). Exact enumeration of self-avoiding walks on BCC and FCC lattices . Journal of Statistical Mechanics: Theory and Experiment, 2017(8), 083208 ."
  },
  {
    "title": "Dictionary",
    "url": "https://arghyadutta.github.io/notebooks/dictionary.html",
    "content": "Dictionary of Untranslatables: A Philosophical Lexicon ; Cassin, B., Rendall, S., Apter, E. S., Eds.; Translation/Transnation Ser; Princeton University Press: Princeton, 2014. Recommended Langenscheidt Basic German Vocabulary. Hauptbd. A Learner’s Dictionary Divided into Subject Categories with Example Sentences ; Langenscheidt: Berlin, 2010. Surprisingly good. Mautner, T. (Ed.). (2005). Dictionary Of Philosophy (2nd edition). Penguin UK. Editors of the A. H. (2018). The American Heritage Dictionary Of The English Language , Fifth Edition: Fiftieth Anniversary Printing (Indexed edition). Collins Reference. Medawar, P. B., & Medawar, J. S. (1985). Aristotle to Zoos: A Philosophical Dictionary of Biology . Oxford University Press. Tschirner, E.; Möhring, J. A Frequency Dictionary of German . Auburn, D. et al. (2012). Oxford American Writer’s Thesaurus (3rd edition). Oxford University Press Inc. Somers, J. (2014). You’re probably using the wrong dictionary His blog Usage notes, fights between descriptivists and prescriptivists, illustrations, portraits—what more could you ask for in a dictionary? If I Get Time Cuddon, J. A., & Habib, M. A. R. (2015). Dictionary of Literary Terms & Literary (5th edition). Penguin UK. Excellent! I love dictionaries! An ode to Webster’s 1913 dictionary ."
  },
  {
    "title": "Internet and Society",
    "url": "https://arghyadutta.github.io/notebooks/internetNUs.html",
    "content": "\"On the internet, a highly functional person is one who can promise everything to an indefinitely increasing audience at all times.\" Tolentino, J. (2019). The I in the Internet . Trick mirror: Reflections on self-delusion (First edition). Random House.\n        ( Free online version )"
  },
  {
    "title": "Chemistry: Online Resources",
    "url": "https://arghyadutta.github.io/notebooks/chemistry.html",
    "content": "Illustrated Glossary of Organic Chemistry MolView Beautiful Chemistry"
  },
  {
    "title": "Engineering Physics",
    "url": "https://arghyadutta.github.io/notebooks/enggPhysics.html",
    "content": "Wonderful visualizations of physics. A few references for the Engineering Physics course (FIC-102) that I teach at SRM University-AP. Balakrishnan, V. How Is a Vector Rotated? Resonance 1999, 4 (10), 61–68 . This is the main textbook for this course. SRM-AP library has many copies of this well-written book. I strongly recommend reading relevant parts—or, better, all—of it. Serway, R. A.; Jewett, J. W.; Peroomian, Vahé. Physics for Scientists and Engineers, Tenth edition.; Cengage: Boston, MA, USA, 2019. Required resources Recommended additional readings"
  },
  {
    "title": "Visualization",
    "url": "https://arghyadutta.github.io/notebooks/visualization.html",
    "content": "Recommended LearnFree (Director). (2016, October 5). Beginning Graphic Design: Color [Video recording] . cmasher Inkscape tutorial for beginners (really good) Muth, L. C. Your Friendly Guide to Colors in Data Visualisation . 2018, Her Blog Fundamentals of Data Visualization HiPlot Plotly Figma Palettable Got this link from Hacker News when I posted a Nature Communication paper The misuse of colour in science communication there.\n        This webpage contains color gradient files, and importing them in matplotlib is possible using packages like getcpt-master . Servier Medical ART Gosling BioRender Fundamentals of Data Visualization by Claus O. Wilke The Elements of Visual Grammar: A Designer's Guide for Writers, Scholars, and Professionals by Angela Riechers Veusz 2D Information is beautiful cpt-city: An archive of color gradients Canva Software Tools If I get time Inkscape tutorials by TJ Free WashU Epigenome Browser Inkscape explained by Logos by Nick ggcoverage Data to Viz Useful Python package. Contains some new colormaps; I don't like them that much but they are interesting. HiGlass I wrote few simple codes to create decent plots using python. SciDraw Gephi Websites for drawing flowcharts etc. from text Useful Python package. Contains an extensive set of diverging, sequential, and qualitative colormaps. Very useful."
  },
  {
    "title": "Phase diagrams",
    "url": "https://arghyadutta.github.io/notebooks/phaseDiagram.html",
    "content": "Gasic, A. G.; Boob, M. M.; Prigozhin, M. B.; Homouz, D.; Daugherty, C. M.; Gruebele, M.; Cheung, M. S. Critical Phenomena in the Temperature-Pressure-Crowding Phase Diagram of a Protein . Phys. Rev. X 2019, 9 (4), 041035 . Intriguing and informative! Here is a rough drawing of a few. I came to know about protein's interesting phase diagrams from a YouTube course by Ali Hassanali (ICTP). For some examples, see Hawley (1971), Asherie (2004), and Gasic et al. (2019). Protein $P–T$ phase diagrams Asherie, N. Protein Crystallization and Phase Diagrams . Methods 2004, 34 (3), 266–272 . Hawley, S. A. Reversible Pressure-Temperature Denaturation of Chymotrypsinogen . Biochemistry 1971, 10 (13), 2436–2442 ."
  },
  {
    "title": "High-performance Computing",
    "url": "https://arghyadutta.github.io/notebooks/hpc.html",
    "content": "Recommended Introduction to parallel Programming in Open MP NPTEL course by Yogish Sabharwal from IIT Delhi Useful if you're coming from a python background and not C/C++ or Fortran. C++ by iamcanadian Improving and optimizing Python by Sebastian Mathôt Courses on YouTube A short series on asymptotic notation and analysis of algorithms by David Scot Taylor . Check his YouTube channel, Algorithms with Attitude, for more stuff. Zaccone, G. (2019). Python parallel programming cookbook. Message-passing programming with MPI Introduction to SLURM Pointers in C and C++ HPC by Matthew Jacob from IISc Pacheco, P. S. (2011). An introduction to parallel programming . Morgan Kaufmann. Intro to parallel programming"
  },
  {
    "title": "Open Source: Ideas and Tools",
    "url": "https://arghyadutta.github.io/notebooks/openSource.html",
    "content": "Find out publishers' copyright policies. Writings and rehearsals by Nathan Schneider Open policy finder Interesting work regarding free and open source software If I Get Time Simard, M.-A., Butler, L.-A., Alperin, J. P., & Haustein, S. (2024). We need to rethink the way we identify diamond open access journals in quantitative science studies . Quantitative Science Studies, 5(4), 1042–1046 ."
  },
  {
    "title": "R Programming",
    "url": "https://arghyadutta.github.io/notebooks/rProgramming.html",
    "content": "If I get time Long, J., & Teetor, P. (2019). R Cookbook: Proven Recipes for Data Analysis, Statistics, and Graphics (2nd ed. edition). O’Reilly Media."
  },
  {
    "title": "Statistical Mechanics: Resources",
    "url": "https://arghyadutta.github.io/notebooks/statMechReadingMaterial.html",
    "content": "Courses on YouTube Non-equilibrium Statistical Mechanics by V. S. Balakrishnan . Introductory lectures on statistical mechanics by John Preskill Huang, K. (1987). Statistical mechanics (2nd ed). Wiley. The chapters on aggregation, fragmentation, and adsorption are particularly good. Non-equilibrium Statistical Mechanics by V. S. Balakrishnan . Good book with a spectacular beginning where he puts the partition function in the place it deserves. Broad yet concise. The chapter on the general properties of the partition function has a nice discussion on the zeroes of the partition function and connections to phase transition. Allen, M. P., & Tildesley, D. J. (2017). Computer simulation of liquids (Second edition). Oxford University Press. Introductory lectures on statistical mechanics by John Preskill Kardar, M. (2007). Statistical Physics of Fields (1st ed.). Cambridge University Press. Tolman, R. C. (1979). The principles of statistical mechanics . Dover Publications. Krapivsky, P. L., Redner, S., & Ben-Naim, E. (2010). A Kinetic View of Statistical Physics (1st ed.). Cambridge University Press. Balakrishnan, V. (2021). Elements of Nonequilibrium Statistical Mechanics. Springer International Publishing. Ryogo Kubo's books are some of the most didactic books I've ever read. The best resource for exactly solvable models. Bayesian Statistics Hydrophobicity Excellent book on problems, more so because it comes with solutions. Check it if you want to know, for example, under which conditions bosons with a $p^s$ dispersion relation in $d$-dimensions will form a condensate. Feynman, R. (1998). Statistical Mechanics: A Set Of Lectures . Taylor & Francis. Also see Toda, M., Kubo, R., & Saitô, N. (1992). Statistical Physics I: Equilibrium Statistical Mechanics (Vol. 30). Springer Berlin Heidelberg. Kubo, R., Toda, M., & Hashitsume, N. (1991). Statistical Physics II (Vol. 31). Springer Berlin Heidelberg. If you're trying to calculate higher-order Feynman diagrams for scalar field theories, Kleinert and Schulte-Frohlinde's book is the best reference . Clean, accessible discussions on numerical approaches in statistical mechanics. For instance, check the discussion on how ensembles are used in molecular dynamics simulations. It has a neat discussion on mean field theories (particularly $\\phi^6$ theory related to tricritical points). A thorough book and—unusually—starts with detailed discussions on entropy. The book uses slightly unwieldy, verbose notations, but I think explicit notations is a feature here, not a problem. I've only had a look at this book, but I'm thoroughly impressed by its depth. Swendsen, R. H. (2019). An Introduction to Statistical Mechanics and Thermodynamics (2nd ed.). Oxford University Press. Thorough, excellent treatment of non-equilibrium stat mech. A list of advanced textbooks. For introductory books, see this list . Kleinert, H., & Schulte-frohlinde, V. (2001). Critical Properties Of $\\phi^4$-Theories . World Scientific. Bose–Einstein Condensates Chandler, D. (1987). Introduction to Modern Statistical Mechanics. OUP USA. Recommended Books Self-Avoiding Random Walks Dalvit, D. A. R., Frastai, J., & Lawrie, I. D. (1999). Problems on statistical mechanics . Institute of Physics. Concise, but so well-written! Check, for example, the first chapter on thermodynamics (and especially Legendre transforms). Ambegaokar, V. (1996). Reasoning about luck: Probability and its uses in physics . Cambridge University Press. (Archive) Complex Systems (related ideas to stat mech.) To read Baxter, R. J. (2008). Exactly Solved Models in Statistical Mechanics . Dover Publications. Nishimori, H., & Ortiz, G. (2011). Elements of phase transitions and critical phenomena . Oxford University Press. My favorite book regarding field-theoretic formulation of critical phenomena. The chapters on the perturbative renormalization group of $\\phi^4$ theories are superb."
  },
  {
    "title": "Learnability in Machine Learning",
    "url": "https://arghyadutta.github.io/notebooks/learnability.html",
    "content": "Output : $y$ (good/bad customer) Online Resources Input : $\\mathbf{x}$ (customer application) Learning algorithm: $\\mathcal{A}$ (e.g. back-propagation for neural network.) It does the searching and produces $g$. Especially useful for its introductory discussion on learnability and VC dimension. Machine learning course by Yaser Abu-Mostafa. The hypothesis set and the learning algorithm $(\\mathcal{H}, \\mathcal{A})$ together are known as the learning model . We don't know $f$, we can only guess what it is from the data. The learning algorithm picks $g\\simeq f$ from the hypothesis set $\\mathcal{H}$. Hypothesis : $g:\\mathcal{X}\\rightarrow\\mathcal{Y}$, $g \\in \\mathcal{H}$. We hope that $g$ approximates $f$ well, that is the goal of learning. Data : $(\\mathbf{x}_1, y_1),(\\mathbf{x}_2, y_2),\\cdots,(\\mathbf{x}_N, y_N)$ (historical records of credit customers) Target function : $f:\\mathcal{X}\\rightarrow\\mathcal{Y}$ (ideal credit approval formula) Hypothesis set : $\\mathcal{H}={h}$. It plays a pivotal role. It can be a linear regression, a neural network, a support vector machine…"
  },
  {
    "title": "Python programming",
    "url": "https://arghyadutta.github.io/notebooks/python.html",
    "content": "Recommended Ramalho, L. (2022). Fluent Python: Clear, Concise, and Effective Programming , Second Edition (Second edition). O’Reilly. McKinney, W. (2022). Python for Data Analysis: Data Wrangling with pandas, NumPy, and Jupyter (3rd edition). O’Reilly Media. McKinney, W. Python for Data Analysis NumPy Illustrated: The Visual Guide to NumPy by Lev Maximov A Visual Intro to NumPy and Data Representation by Jay Alammar Why Python is Slow: Looking Under the Hood by Jake Vanderplus 5 Good Python Habits; 2024. YouTube (accessed 2025-08-22)."
  },
  {
    "title": "Subgroup Discovery",
    "url": "https://arghyadutta.github.io/notebooks/subgroupDiscovery.html",
    "content": "Subgroup discovery (SGD) is a local knowledge discovery method. It identifies subsets in a set of elements that 'stands out' with respect to some property of the elements. Recommended Lopez-Martinez-Carrasco, A.; Juarez, J. M.; Campos, M.; Mora-Caselles, F. Subgroups: A Python Library for Subgroup Discovery. SoftwareX 2024 , 28 , 101895. https://doi.org/10.1016/j.softx.2024.101895 . Given: Sample $S \\subseteq P$, Target variable $y:P\\rightarrow {a, b, c, \\cdots}$, and Features $x_j: P\\rightarrow X_j$ It has been used to find out subgroup properties that contribute to one of the two crystal structures of 82 octet binaries (Ghiringhelli et al. 2015). SGD predicts two subgroups which contain elements that have either a zinc blend structure or a rock salt structure (Goldsmith et al 2017, Boley et al. 2017). For a review of the SGD, see Atzmueller (2015). What's new in it then from clustering? My current understanding (need to check and update): Compared to global methods like decision tree, regression, or compressed sensing, SGD is a local method meaning it does not attempt to classify all the elements of the parent set into subsets, rather it tries to find subclasses which are of high quality with respect to the desired property. Algorithm (tentative, need to check) Define: Propositions $Pi_x = {\\pi_1,\\cdots, \\pi_k}$, Selection language $\\mathcal{L}_x = {\\sigma(i)=\\pi_{j_1}(i)\\wedge\\cdots\\wedge \\pi_{j_t}(i)}$ Optimize: $f(Q)=\\textrm{cov}(Q)^\\gamma \\textrm{eff}(Q)_+$ where $Q=\\{i \\in S: \\sigma(i)= \\textrm{True}\\}$ (extension), $\\textrm{cov}(Q)=|Q|/|S|$ (coverage), $\\textrm{eff}(Q)= \\frac{H_y(S)-H_y(Q)}{H_y(S)}$ (effect), and $H_y(Q) = -\\sum_v p_Q(y=v) \\log p_Q(y=v)$ (entropy). Atzmueller, M. Subgroup Discovery. WIREs Data Mining and Knowledge Discovery 2015 , 5 (1), 35–49. https://doi.org/10.1002/widm.1144 . Boley, M.; Goldsmith, B. R.; Ghiringhelli, L. M.; Vreeken, J. Identifying Consistent Statements about Numerical Data with Dispersion-Corrected Subgroup Discovery. Data Min Knowl Disc 2017 , 31 (5), 1391–1418. https://doi.org/10.1007/s10618-017-0520-3 . New (~2024–25): GitHub Repo with Python implementation Also see Lopez-Martinez-Carrasco et al. (2024).  (I tried a Java implementation few years ago; it was useful but a bit clunky.) Goldsmith, B. R.; Boley, M.; Vreeken, J.; Scheffler, M.; Ghiringhelli, L. M. Uncovering Structure-Property Relationships of Materials by Subgroup Discovery. New J. Phys. 2017 , 19 (1), 013031. https://doi.org/10.1088/1367-2630/aa57c2 . Ghiringhelli, L. M.; Vybiral, J.; Levchenko, S. V.; Draxl, C.; Scheffler, M. Big Data of Materials Science: Critical Role of the Descriptor. Phys. Rev. Lett. 2015 , 114 (10), 105503. https://doi.org/10.1103/PhysRevLett.114.105503 . Package"
  },
  {
    "title": "Excel keyboard shortcuts for Mac",
    "url": "https://arghyadutta.github.io/notebooks/excelShortcuts.html",
    "content": "Selection Editing Aids Function Key Shortcuts I am not a fan of Excel, but also can't avoid it at work. So, I made this cheat-sheet of sorts using ChatGPT. I haven't tested most of them. I plan to use and edit it as needed. (Last updated: 2025-08-25, Monday) macOS vs Excel Conflicts Navigation Why you may not want to use this: It's ChatGPT-generated content! Formatting Other Useful Shortcuts Editing & Entering"
  },
  {
    "title": "Deep Learning",
    "url": "https://arghyadutta.github.io/notebooks/deepLearning.html",
    "content": "Deep Learning by Frank Noe . Online Resources Leisurely paced, meticulous."
  },
  {
    "title": "Indian history",
    "url": "https://arghyadutta.github.io/notebooks/india.html",
    "content": "Recommended More A well-researched and engaging read. The author’s sincerity comes through—she writes with empathy for the people living by the Indus, offering many thoughtful insights. I didn’t always agree with her, especially in her somewhat cautious stance on religion, but overall, I’d recommend the book. Roychowdhury, T. Bangalnama | বাঙালনামা; Ananda Publishers. It focuses on the political history of Bengal since the time of Mahabharata and to the 1970s. Yes a bit too broad, but the author has done a good job in documenting the major political upheavals. I liked a nuanced discussion on the gradual deterioration of Hindu–Muslim relations, starting 1900. Though, at times, the writing is dry and descriptive, it mostly reads well. One issue: a history book should have some decent maps and charts delineating the successions of kings; it lacks those. More Candid, witty, and sometimes sarcastic, the book starts with a take on colonial India. It then moves into the author’s experiences working as a historian—in the National Archives, at Delhi University, and later at Oxford. A great pick if you're looking for something that's both fun to read and full of insight into India and how an Indian sees Europe. There's also an abridged English translation called \"The World in Our Time: A Memoir\". Sengupta, N. K. Land of Two Rivers: A History of Bengal from the Mahabharata to Mujib ; Penguin Books: New Delhi, 2011. If I Get Time Sastri, K. A. N.; Gurukkal, P. M. R.; Champakalakshmi, R. The Illustrated History of South India ; Oxford: New Delhi, 2009. More It focuses on the political history of Bengal since the time of Mahabharata and to the 1970s. Yes a bit too broad, but the author has done a good job in documenting the major political upheavals. I liked a nuanced discussion on the gradual deterioration of Hindu–Muslim relations, starting 1900. Though, at times, the writing is dry and descriptive, it mostly reads well. One issue: a history book should have some decent maps and charts delineating the successions of kings; it lacks those. Albinia, A. Empires of the Indus: The Story of a River ; John Murray: London, 2008."
  },
  {
    "title": "Buddha and Buddhism",
    "url": "https://arghyadutta.github.io/notebooks/buddhism.html",
    "content": "Recommended The Dhammapada: Verses and Stories Rahula, W. (1974). What the Buddha Taught: Revised and Expanded Edition with Texts from Suttas and Dhammapada (Revised edition). Grove Press. If I Get Time Bhattacharya, K. (1975). On the Bramhan in Buddhist Literature. Oriental Journal Tirupati, XVIII. Kornfield, J. (2005). The Dhammapada: A New Translation of the Buddhist Classic with Annotations (G. Fronsdal, Trans.). Shambhala."
  },
  {
    "title": "Quantum mechanics: Books and Online Resources",
    "url": "https://arghyadutta.github.io/notebooks/qMech.html",
    "content": "Recommended Sakurai, J. J., & Napolitano, J. (2021). Modern quantum mechanics (3rd ed). Cambridge University Press. Shankar, R. (1994). Principles of quantum mechanics (2nd ed). Plenum Press. Das, A. (2006). Field Theory: A Path Integral Approach (2nd ed., Vol. 75). World Scientific. Bohm, D. (1989). Quantum Theory . Dover Publications. For the path integral formalism of QM, the first few chapters of Ashok Das's book provide a pedagogic introduction. I have discovered this gem surprisingly late. Bohm lucidly discussed several topics which are hard to find in other books (one example would be statistical properties of correlations for classical and quantum systems, see chapter 10!) It's very much worth watching despite the poor recording quality. I love this book! The chapters on rotational invariance, addition of angular momenta, scattering theory, and path integral are particularly good. Ekert is a patient teacher. The accompanying free book is useful, too. Townsend, J. S. (2012). A modern approach to quantum mechanics (2nd ed). University Science Books. From YouTube It's at Griffiths's level and comes with several detailed examples. Though not as popular, it does provide a solid introduction. Griffiths, D. J., & Schroeter, D. F. (2018). Introduction to quantum mechanics (Third edition). Cambridge University Press. Gasiorowicz, S. (1974). Quantum physics . New York, Wiley. A completely different sort of book which teaches QM through a series of problems. Flügge, S. (1999). Practical quantum mechanics . Springer. A good undergrad-level textbook. The chapter on solutions to time-independent Schrödinger equation is neat. One of the best graduate-level QM book. Precise and concise. An easier version of Sakurai's book. (And there exists a cheaper Indian edition.) Lectures on Quantum Computing . Introduction to Quantum Information Science by Artur Ekert. Quantum entanglement by Mark Wilde"
  },
  {
    "title": "Quantum Information",
    "url": "https://arghyadutta.github.io/notebooks/qInfo.html",
    "content": "Recommended See also Basic Mathematics for Quantum Mechanics Quantum Computation Lecture Notes (Caltech CS219) by John Preskill"
  },
  {
    "title": "Personal Knowledge Management",
    "url": "https://arghyadutta.github.io/notebooks/pkm.html",
    "content": "Weekly Review: Using Obsidian to Close Open Loops at the End of the Week . Managing project folders is tricky. Dan Larremore suggests making these folders in your project folder: raw-inputs, cleaned-inputs, code, output, figures, and writing. I think that's very useful. Also, keeping a README file in plain text format in your project folder and actively updating it is crucial. Think carefully about how the newfound piece of information connects to what you already know. Think before you write a note—a note’s usefulness generally derives from the effort you put in. See how to take effective research notes . Don’t copy text written by others; instead archive it in Zotero if you really want to keep a copy. Copying text for your personal notes defeats the purpose of taking notes: understanding and summarizing ideas for yourself . If you're putting effort into writing notes, consider not writing them in a proprietary format (Word, Google Doc, etc.) and using plain text format, for example, markdown or LaTeX . See The Plain Text Project and Ycombinator post on Managing my personal knowledge base . Example: If you're writing a note on phase diagrams of proteins, connect it to existing notes, if any, on thermodynamic phases. Referencing: Use some marker, like `name-year-firstWord`, in the text and include a full bibliography at the end. You can also use pandoc to generate sorted bibliographies automatically; search for how to do it. Related point : If you don’t have a note yet, it can be tempting to start one, which can be useful or a waste of time depending on your needs. Since these are your personal notes , they need not be encyclopedic. Knowledge gaps in personal notes are fine given our limited time, but if the topic you skipped repeatedly comes up in your research, consider reading about it and writing a note. A few thoughts on taking notes for your work or research: Software : A topic that got a lot of attention and funding in recent years. I recommend Obsidian : it's free and has a large community of users. Copy well-made images for your personal archive since they are useful, but don’t ever forget to keep the attributions in your notes. Otherwise, you will forget the attribution and will risk plagiarism."
  }
]