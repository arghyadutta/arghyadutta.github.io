[
  {
    "title": "Hydrophobicity",
    "url": "https://arghyadutta.github.io/notebooks/hydrophobicity.html",
    "content": "Xi, E.; Patel, A. J. The Hydrophobic Effect, and Fluctuations: The Long and the Short of It. Proc. Natl. Acad. Sci. U.S.A. 2016 , 113 (17), 4549â€“4551. https://doi.org/10.1073/pnas.1603014113 . Kauzmann, W. Some Factors in the Interpretation of Protein Denaturation. In Advances in Protein Chemistry ; Elsevier, 1959; Vol. 14, pp 1â€“63. https://doi.org/10.1016/S0065-3233(08)60608-7 . Lazaridis, T. Hydrophobic Effect. In eLS ; John Wiley & Sons, Ltd, 2013. https://doi.org/10.1002/9780470015902.a0002974.pub2 . If I Get Time A result of interplay between entropy and enthalpy. Ben-Naim, A. Hydrophobic Interaction and Structural Changes in the Solvent. Biopolymers 1975 , 14 (7), 1337â€“1355. https://doi.org/10.1002/bip.1975.360140704 . Tanford, C. The Hydrophobic Effect and the Organization of Living Matter. Science 1978 , 200 (4345), 1012â€“1018. https://doi.org/10.1126/science.653353 . Berne, B. J.; Weeks, J. D.; Zhou, R. Dewetting and Hydrophobic Interaction in Physical and Biological Systems. Annual Review of Physical Chemistry 2009 , 60 (1), 85â€“103. https://doi.org/10.1146/annurev.physchem.58.032806.104445"
  },
  {
    "title": "Fiction",
    "url": "https://arghyadutta.github.io/notebooks/fiction.html",
    "content": "Recommended Wood, J. (2013), Becoming them , The New Yorker à¦†à¦®à¦¿ à¦—à§à¦°à¦¾à¦®à§‡à¦° à¦›à§‡à¦²à§‡, à¦•à¦¿à¦¨à§à¦¤à§ à¦¸à¦¤à§à¦¯à¦¿à¦•à¦¾à¦°à§‡à¦° à¦­à¦¾à¦²à§‹ à¦à¦•à¦Ÿà¦¾ à¦•à¦¿à¦¶à§‹à¦°à¦¸à¦¾à¦¹à¦¿à¦¤à§à¦¯ à¦¯à§‡ à¦—à§à¦°à¦¾à¦®-à¦¶à¦¹à¦°à§‡à¦° à¦¸à¦™à§à¦•à§€à¦°à§à¦£ à¦­à§‚à¦—à§‹à¦²à§‡à¦° à¦¸à§€à¦®à¦¾ à¦¹à§‡à¦²à¦¾à¦¯à¦¼ à¦…à¦¤à¦¿à¦•à§à¦°à¦® à¦•à¦°à§‡ à¦¯à§‡à¦¤à§‡ à¦ªà¦¾à¦°à§‡ à¦¤à¦¾à¦° à¦ªà§à¦°à¦®à¦¾à¦£ à¦¦à§‡à¦¯à¦¼ à¦à¦‡ à¦¬à¦‡: à¦–à¦¾à¦¸ à¦•à§‹à¦²à¦•à¦¾à¦¤à¦¾ à¦¶à¦¹à¦°à§‡à¦° à¦šà¦¾à¦° à¦•à¦¿à¦¶à§‹à¦°à§‡à¦° adventure (à¦à¦¬à¦‚, à¦†à¦° à¦¬à§‡à¦¶à¦¿ misadventure) à¦—à§à¦²à§‹ à¦¥à§‡à¦•à§‡ à¦®à¦œà¦¾ à¦ªà§‡à¦¤à§‡ à¦†à¦®à¦¾à¦° à¦•à§‹à¦¨ à¦…à¦¸à§à¦¬à¦¿à¦§à¦¾ à¦¹à¦¯à¦¼ à¦¨à¦¿à¥¤ à¦¹à§‡à¦®à§‡à¦¨à§à¦¦à§à¦°à¦•à§à¦®à¦¾à¦° à¦°à¦šà¦¨à¦¾à¦¬à¦²à§€à¦° à¦à¦‡ à¦–à¦£à§à¦¡à¦Ÿà¦¾ à¦ªà¦¡à¦¼à§‡ à¦†à¦®à¦¾à¦° à¦¬à§‡à¦¶ à¦­à¦¾à¦²à§‹ à¦²à¦¾à¦—à¦²à¥¤ à¦à¦° à¦®à¦§à§à¦¯à§‡ à¦ªà§à¦°à¦¥à¦® à¦¤à¦¿à¦¨à¦Ÿà¦¿ à¦‰à¦ªà¦¨à§à¦¯à¦¾à¦¸ à¦°à¦¯à¦¼à§‡à¦›à§‡ à¦¡à¦¾à¦•à¦¾à¦¤ à¦¦à§€à¦¨à¦¬à¦¨à§à¦§à§à¦•à§‡ à¦¨à¦¿à¦¯à¦¼à§‡à¥¤ à¦¦à§€à¦¨à¦¬à¦¨à§à¦§à§ à¦à¦®à¦¨ à¦à¦•à¦œà¦¨ à¦¡à¦¾à¦•à¦¾à¦¤ à¦¯à§‡, à¦¸à§Œà¦­à¦¾à¦—à§à¦¯à¦¬à¦¶à¦¤, à¦¨à¦¿à¦œà§‡à¦° à¦¨à¦¾à¦®à§‡à¦° à¦®à¦¾à¦¨à§‡ à¦¬à§‹à¦à§‡; à¦¤à¦¾à¦‡ à¦¤à¦¾à¦° à¦•à¦°à¦¾ à¦¡à¦¾à¦•à¦¾à¦¤à¦¿à¦•à§‡ à¦®à§‡à¦¨à§‡ à¦¨à¦¿à¦¤à§‡ à¦²à§‡à¦–à¦• à¦¬à¦¾ à¦ªà¦¾à¦ à¦• à¦•à¦¾à¦°à§‹à¦°à¦‡ à¦…à¦¸à§à¦¬à¦¿à¦§à¦¾ à¦¹à¦¯à¦¼ à¦¨à¦¾à¥¤ à¦¦à§€à¦¨à¦¬à¦¨à§à¦§à§à¦•à§‡ à¦§à¦°à¦¾à¦° à¦¬à§à¦¯à¦°à§à¦¥ à¦šà§‡à¦·à§à¦Ÿà¦¾ à¦•à¦°à§‡ à¦¬à§‡à¦°à¦¾à¦¯à¦¼ à¦—à§‹à¦¯à¦¼à§‡à¦¨à§à¦¦à¦¾ à¦ªà§à¦°à¦¶à¦¾à¦¨à§à¦¤à¦¬à¦¾à¦¬à§, à¦¯à¦¾à¦•à§‡ à¦¬à§à¦¦à§à¦§à¦¿à¦®à¦¾à¦¨ à¦¦à§€à¦¨à¦¬à¦¨à§à¦§à§ à¦¡à¦¾à¦•à§‡ à¦…à¦¶à¦¾à¦¨à§à¦¤à¦¬à¦¾à¦¬à§ à¦¬à¦²à§‡à¥¤ à¦à¦‡ à¦¦à§à¦œà¦¨à¦•à§‡ à¦¨à¦¿à¦¯à¦¼à§‡ à¦²à§‡à¦–à¦¾ à¦—à¦²à§à¦ªà¦—à§à¦²à§‹ à¦¬à§‡à¦¶ à¦¸à§à¦–à¦ªà¦¾à¦ à§à¦¯à¥¤ à¦¬à¦‡à¦¯à¦¼à§‡à¦° à¦¶à§‡à¦· à¦¦à§à¦Ÿà¦¿ à¦‰à¦ªà¦¨à§à¦¯à¦¾à¦¸ à¦œà¦¯à¦¼à¦¨à§à¦¤, à¦®à¦¾à¦¨à¦¿à¦• à¦“ à¦¸à§à¦¨à§à¦¦à¦°à¦¬à¦¾à¦¬à§à¦•à§‡ à¦¨à¦¿à¦¯à¦¼à§‡ à¦²à§‡à¦–à¦¾à¥¤ à¦à¦‡ à¦¦à§à¦Ÿà¦¿à¦¤à§‡ à¦°à¦•à§à¦¤à¦ªà¦¾à¦¤à§‡à¦° à¦•à¦¿à¦›à§ à¦¬à¦¾à¦¡à¦¼à¦¾à¦¬à¦¾à¦¡à¦¼à¦¿ à¦¥à¦¾à¦•à¦²à§‡à¦“ à¦¤à¦¾ à¦®à¦¾à¦¤à§à¦°à¦¾ à¦›à¦¾à¦¡à¦¼à¦¿à¦¯à¦¼à§‡ à¦¯à¦¾à¦¯à¦¼ à¦¨à¦¾ â€” à¦¤à¦¾à¦‡ à¦¸à¦¬ à¦®à¦¿à¦²à¦¿à¦¯à¦¼à§‡ à¦¬à¦‡à¦Ÿà¦¾ à¦ªà¦¡à¦¼à¦¤à§‡ à¦¬à§‡à¦¶ à¦­à¦¾à¦²à¦‡ à¦²à¦¾à¦—à§‡à¥¤ à¦†à¦œà¦•à¦¾à¦²à¦•à¦¾à¦° à¦›à§‡à¦²à§‡à¦®à§‡à¦¯à¦¼à§‡à¦°à¦¾ à¦ªà¦¡à¦¼à¦²à§‡ à¦¤à¦¾à¦¦à§‡à¦° à¦–à¦¾à¦°à¦¾à¦ª à¦²à¦¾à¦—à¦¬à§‡ à¦¬à¦²à§‡ à¦®à¦¨à§‡ à¦¹à¦¯à¦¼ à¦¨à¦¾à¥¤ Krystal, A. (2022), Whatâ€™s the Deal, Hummingbird? , The New Yorker â€¦à¦•à¦¿à¦¨à§à¦¤à§ à¦ªà¦¿à¦¸à¦¿à¦®à¦¾ à¦§à§‹à¦ªà¦¾à¦° à¦¹à¦¿à¦¸à§‡à¦¬à§‡à¦° à¦–à¦¾à¦¤à¦¾à¦¯à¦¼ à¦à¦‡à¦¸à¦¬ à¦¦à§‡à¦–à§‡ à¦­à§€à¦·à¦£ à¦°à§‡à¦—à§‡ à¦—à§‡à¦²! à¦°à§‡à¦—à§‡ à¦—à¦¿à¦¯à¦¼à§‡ à¦¹à¦¾à¦¤à§‡à¦° à¦•à¦¾à¦›à§‡ à¦†à¦° à¦•à¦¿à¦›à§ à¦¨à¦¾ à¦ªà§‡à¦¯à¦¼à§‡ à¦à¦•à¦Ÿà¦¾ à¦šà¦¾à¦²à¦•à§à¦®à¦¡à¦¼à§‹ à¦¨à¦¿à¦¯à¦¼à§‡ à¦«à§à¦šà§à¦¦à¦¾à¦•à§‡ à¦¤à¦¾à¦¡à¦¼à¦¾ à¦•à¦°à¦²à§‡à¥¤ à¦ à¦¿à¦• à¦¯à§‡à¦¨ à¦—à¦¦à¦¾ à¦¹à¦¾à¦¤à§‡ à¦¨à¦¿à¦¯à¦¼à§‡ à¦¶à¦¾à¦¡à¦¼à¦¿à¦ªà¦°à¦¾ à¦­à§€à¦® à¦¦à§Œà¦¡à¦¼à¦šà§à¦›à§‡à¥¤ à¦ªà¦¾à¦à¦šà¦–à¦¾à¦¨à¦¾ à¦§à§à¦¤à¦¿, à¦¸à¦¾à¦¤à¦–à¦¾à¦¨à¦¾ à¦¶à¦¾à¦¡à¦¼à¦¿ à¦-à¦¸à¦¬ à¦¹à¦¿à¦¸à§‡à¦¬à§‡ à¦¹à¦‡à¦¬à§‡ à¦•à¦¿à¦¬à¦¾? à¦-à¦œà¦—à¦¤à§‡ à¦œà§€à¦¬ à¦•à¦¤ à¦¬à§à¦¯à¦¾à¦¥à¦¾ à¦ªà¦¾à¦¯à¦¼, à¦¤à¦¾à¦‡ à¦­à¦¾à¦¬à¦¿ à¦†à¦®à¦¿ à¦°à¦¾à¦¤à§à¦°à¦¿-à¦¦à¦¿à¦¬à¦¾à¥¤ à¦°à¦¾à¦®à¦§à¦¨à§‡à¦° à¦“à¦‡ à¦¬à§ƒà¦¦à§à¦§ à¦—à¦¾à¦§à¦¾ à¦®à¦¨à¦Ÿà¦¿ à¦¤à¦¾à¦¹à¦¾à¦° à¦¬à¦¡à¦¼à¦‡ à¦¸à¦¾à¦¦à¦¾- à¦¸à§‡-à¦¬à§‡à¦šà¦¾à¦°à¦¾ à¦¤à¦¾à¦° à¦ªà¦¿à¦ à§‡à¦¤à§‡ à¦šà¦¾à¦ªà¦¾à¦¯à¦¼à§‡ à¦•à¦¤ à¦¶à¦¾à¦¡à¦¼à¦¿-à¦§à§à¦¤à¦¿-à¦ªà§à¦¯à¦¾à¦¨à§à¦Ÿ à¦²à¦‡à¦¯à¦¼à¦¾ à¦¯à¦¾à¦¯à¦¼- à¦®à¦¨à§‹à¦¦à§à¦–à§‡ à¦–à¦¾à¦²à¦¿ à¦¬à§‹à¦à¦¾ à¦Ÿà§‡à¦¨à§‡ à¦«à§‡à¦°à§‡ à¦—à¦¾à¦§à¦¾ à¦à¦•à¦–à¦¾à¦¨à¦¾ à¦§à§à¦¤à¦¿-à¦ªà§à¦¯à¦¾à¦¨à§à¦Ÿ à¦ªà¦°à¦¿à¦¤à§‡ à¦¨à¦¾ à¦ªà¦¾à¦¯à¦¼! à¦à¦‡ à¦¬à¦‡à¦¤à§‡ à¦ªà¦¾à¦“à¦¯à¦¼à¦¾ à¦¨à¦¾à¦°à¦¾à¦¯à¦¼à¦£ à¦—à¦™à§à¦—à§‹à¦ªà¦¾à¦§à§à¦¯à¦¾à¦¯à¦¼à§‡à¦° à¦…à¦§à¦¿à¦•à¦¾à¦‚à¦¶ à¦—à¦²à§à¦ªà¦—à§à¦²à¦¿à¦‡ à¦¨à¦¿à¦°à§à¦­à§‡à¦œà¦¾à¦² à¦¹à¦¾à¦¸à¦¿à¦°, à¦…à¦¨à¦¾à¦¬à¦¿à¦² à¦†à¦¨à¦¨à§à¦¦à§‡à¦° à¦‰à§à¦¸: à¦¨à¦¿à¦ƒà¦¸à¦¨à§à¦¦à§‡à¦¹à§‡ à¦†à¦®à¦¾à¦° à¦ªà§œà¦¾ à¦…à¦¨à§à¦¯à¦¤à¦® à¦¶à§à¦°à§‡à¦·à§à¦  à¦•à¦¿à¦¶à§‹à¦°à¦¸à¦¾à¦¹à¦¿à¦¤à§à¦¯à¥¤ Gangopadhyay, N. (2014). Samagra Kishor Sahitya (1st edition). Ananda Publishers. (Bengali) à¦¹à§‡à¦®à§‡à¦¨à§à¦¦à§à¦° à¦°à¦¾à¦¯à¦¼à§‡à¦° à¦²à§‡à¦–à¦¾ à¦¬à¦‡ à¦—à§à¦²à§‹ à¦à¦•à¦Ÿà¦¾à¦¨à¦¾ à¦ªà¦¡à¦¼à¦²à§‡ à¦­à¦¾à¦²à§‹ à¦²à¦¾à¦—à§‡ à¦¨à¦¾, à¦…à¦¨à§à¦¤à¦¤ à¦†à¦®à¦¾à¦°à¥¤ à¦¤à¦¾à¦‡ à¦“à¦¨à¦¾à¦° à¦¬à¦‡à¦—à§à¦²à§‹ à¦à¦•à¦Ÿà§ à¦°à¦¯à¦¼à§‡à¦¸à¦¯à¦¼à§‡ à¦ªà¦¡à¦¼à¦¿à¥¤ à¦¤à¦¾à¦‡ à¦à¦‡ à¦¬à¦‡ à¦•à§à¦²à¦¾à¦¸à§‡ à¦ªà§à¦°à¦¥à¦® à¦¹à¦¬à¦¾à¦° à¦¦à§Œà¦¡à¦¼à§‡ à¦•à§à¦²à¦¾à¦¨à§à¦¤ à¦†à¦œà¦•à§‡à¦° à¦•à¦¿à¦¶à§‹à¦°-à¦•à¦¿à¦¶à§‹à¦°à§€à¦¦à§‡à¦° à¦…à¦¬à¦¶à§à¦¯à¦ªà¦¾à¦ à§à¦¯à¥¤ à¦†à¦° à¦®à¦œà¦¾ à¦›à¦¾à¦¡à¦¼à¦¾à¦“ à¦à¦‡ à¦¬à¦‡ à¦¥à§‡à¦•à§‡ à¦¬à¦¾à¦¸à§à¦¤à¦¬-à¦œà§€à¦¬à¦¨à§‡ à¦•à¦¾à¦œà§‡ à¦²à¦¾à¦—à¦¾à¦¨à§‹à¦° à¦®à¦¤ à¦…à¦¨à§‡à¦• à¦•à¦¿à¦›à§ à¦¶à§‡à¦–à¦¾ à¦¯à¦¾à¦¯à¦¼à¥¤ à¦à¦•à¦Ÿà¦¿ à¦¨à¦®à§à¦¨à¦¾: More à¦¹à§‡à¦®à§‡à¦¨à§à¦¦à§à¦° à¦°à¦¾à¦¯à¦¼à§‡à¦° à¦²à§‡à¦–à¦¾ à¦¬à¦‡ à¦—à§à¦²à§‹ à¦à¦•à¦Ÿà¦¾à¦¨à¦¾ à¦ªà¦¡à¦¼à¦²à§‡ à¦­à¦¾à¦²à§‹ à¦²à¦¾à¦—à§‡ à¦¨à¦¾, à¦…à¦¨à§à¦¤à¦¤ à¦†à¦®à¦¾à¦°à¥¤ à¦¤à¦¾à¦‡ à¦“à¦¨à¦¾à¦° à¦¬à¦‡à¦—à§à¦²à§‹ à¦à¦•à¦Ÿà§ à¦°à¦¯à¦¼à§‡à¦¸à¦¯à¦¼à§‡ à¦ªà¦¡à¦¼à¦¿à¥¤ à¦¹à§‡à¦®à§‡à¦¨à§à¦¦à§à¦°à¦•à§à¦®à¦¾à¦° à¦°à¦šà¦¨à¦¾à¦¬à¦²à§€à¦° à¦à¦‡ à¦–à¦£à§à¦¡à¦Ÿà¦¾ à¦ªà¦¡à¦¼à§‡ à¦†à¦®à¦¾à¦° à¦¬à§‡à¦¶ à¦­à¦¾à¦²à§‹ à¦²à¦¾à¦—à¦²à¥¤ à¦à¦° à¦®à¦§à§à¦¯à§‡ à¦ªà§à¦°à¦¥à¦® à¦¤à¦¿à¦¨à¦Ÿà¦¿ à¦‰à¦ªà¦¨à§à¦¯à¦¾à¦¸ à¦°à¦¯à¦¼à§‡à¦›à§‡ à¦¡à¦¾à¦•à¦¾à¦¤ à¦¦à§€à¦¨à¦¬à¦¨à§à¦§à§à¦•à§‡ à¦¨à¦¿à¦¯à¦¼à§‡à¥¤ à¦¦à§€à¦¨à¦¬à¦¨à§à¦§à§ à¦à¦®à¦¨ à¦à¦•à¦œà¦¨ à¦¡à¦¾à¦•à¦¾à¦¤ à¦¯à§‡, à¦¸à§Œà¦­à¦¾à¦—à§à¦¯à¦¬à¦¶à¦¤, à¦¨à¦¿à¦œà§‡à¦° à¦¨à¦¾à¦®à§‡à¦° à¦®à¦¾à¦¨à§‡ à¦¬à§‹à¦à§‡; à¦¤à¦¾à¦‡ à¦¤à¦¾à¦° à¦•à¦°à¦¾ à¦¡à¦¾à¦•à¦¾à¦¤à¦¿à¦•à§‡ à¦®à§‡à¦¨à§‡ à¦¨à¦¿à¦¤à§‡ à¦²à§‡à¦–à¦• à¦¬à¦¾ à¦ªà¦¾à¦ à¦• à¦•à¦¾à¦°à§‹à¦°à¦‡ à¦…à¦¸à§à¦¬à¦¿à¦§à¦¾ à¦¹à¦¯à¦¼ à¦¨à¦¾à¥¤ à¦¦à§€à¦¨à¦¬à¦¨à§à¦§à§à¦•à§‡ à¦§à¦°à¦¾à¦° à¦¬à§à¦¯à¦°à§à¦¥ à¦šà§‡à¦·à§à¦Ÿà¦¾ à¦•à¦°à§‡ à¦¬à§‡à¦°à¦¾à¦¯à¦¼ à¦—à§‹à¦¯à¦¼à§‡à¦¨à§à¦¦à¦¾ à¦ªà§à¦°à¦¶à¦¾à¦¨à§à¦¤à¦¬à¦¾à¦¬à§, à¦¯à¦¾à¦•à§‡ à¦¬à§à¦¦à§à¦§à¦¿à¦®à¦¾à¦¨ à¦¦à§€à¦¨à¦¬à¦¨à§à¦§à§ à¦¡à¦¾à¦•à§‡ à¦…à¦¶à¦¾à¦¨à§à¦¤à¦¬à¦¾à¦¬à§ à¦¬à¦²à§‡à¥¤ à¦à¦‡ à¦¦à§à¦œà¦¨à¦•à§‡ à¦¨à¦¿à¦¯à¦¼à§‡ à¦²à§‡à¦–à¦¾ à¦—à¦²à§à¦ªà¦—à§à¦²à§‹ à¦¬à§‡à¦¶ à¦¸à§à¦–à¦ªà¦¾à¦ à§à¦¯à¥¤ à¦¬à¦‡à¦¯à¦¼à§‡à¦° à¦¶à§‡à¦· à¦¦à§à¦Ÿà¦¿ à¦‰à¦ªà¦¨à§à¦¯à¦¾à¦¸ à¦œà¦¯à¦¼à¦¨à§à¦¤, à¦®à¦¾à¦¨à¦¿à¦• à¦“ à¦¸à§à¦¨à§à¦¦à¦°à¦¬à¦¾à¦¬à§à¦•à§‡ à¦¨à¦¿à¦¯à¦¼à§‡ à¦²à§‡à¦–à¦¾à¥¤ à¦à¦‡ à¦¦à§à¦Ÿà¦¿à¦¤à§‡ à¦°à¦•à§à¦¤à¦ªà¦¾à¦¤à§‡à¦° à¦•à¦¿à¦›à§ à¦¬à¦¾à¦¡à¦¼à¦¾à¦¬à¦¾à¦¡à¦¼à¦¿ à¦¥à¦¾à¦•à¦²à§‡à¦“ à¦¤à¦¾ à¦®à¦¾à¦¤à§à¦°à¦¾ à¦›à¦¾à¦¡à¦¼à¦¿à¦¯à¦¼à§‡ à¦¯à¦¾à¦¯à¦¼ à¦¨à¦¾ â€” à¦¤à¦¾à¦‡ à¦¸à¦¬ à¦®à¦¿à¦²à¦¿à¦¯à¦¼à§‡ à¦¬à¦‡à¦Ÿà¦¾ à¦ªà¦¡à¦¼à¦¤à§‡ à¦¬à§‡à¦¶ à¦­à¦¾à¦²à¦‡ à¦²à¦¾à¦—à§‡à¥¤ à¦†à¦œà¦•à¦¾à¦²à¦•à¦¾à¦° à¦›à§‡à¦²à§‡à¦®à§‡à¦¯à¦¼à§‡à¦°à¦¾ à¦ªà¦¡à¦¼à¦²à§‡ à¦¤à¦¾à¦¦à§‡à¦° à¦–à¦¾à¦°à¦¾à¦ª à¦²à¦¾à¦—à¦¬à§‡ à¦¬à¦²à§‡ à¦®à¦¨à§‡ à¦¹à¦¯à¦¼ à¦¨à¦¾à¥¤ à¦•à§à¦°à¦¿à¦•à§‡à¦Ÿ à¦–à§‡à¦²à¦¤à§‡ à¦—à¦¿à¦¯à¦¼à§‡ à¦¦à§‡à¦°à¦¿ à¦•à¦°à§‡ à¦¬à¦¾à¦¡à¦¼à¦¿ à¦«à¦¿à¦°à§‡ à¦®à¦¾à¦¯à¦¼à§‡à¦° à¦¬à¦•à§à¦¨à¦¿ à¦–à¦¾à¦“à¦¯à¦¼à¦¾, à¦•à¦¿à¦‚à¦¬à¦¾ à¦œà¦¾à¦®à¦°à§à¦² à¦ªà¦¾à¦°à¦¤à§‡ à¦—à¦¿à¦¯à¦¼à§‡ à¦•à¦¾à¦ à¦ªà¦¿à¦à¦ªà¦¡à¦¼à§‡à¦° à¦•à¦¾à¦®à¦¡à¦¼ à¦–à¦¾à¦“à¦¯à¦¼à¦¾à¦° à¦®à¦¤ à¦›à§‹à¦Ÿà¦¬à§‡à¦²à¦¾à¦° à¦¬à¦¿à¦­à¦¿à¦¨à§à¦¨ à¦¸à§à¦®à§ƒà¦¤à¦¿à¦° à¦¸à¦¾à¦¥à§‡ à¦à¦‡ à¦¬à¦‡à¦Ÿà¦¾ à¦ªà¦¡à¦¼à¦¾à¦° à¦¸à§à¦–à¦¸à§à¦®à§ƒà¦¤à¦¿ à¦à¦®à¦¨à¦­à¦¾à¦¬à§‡ à¦œà¦¡à¦¼à¦¿à¦¯à¦¼à§‡ à¦†à¦›à§‡ à¦¯à§‡ à¦†à¦œ à¦†à¦° à¦†à¦®à¦¾à¦° à¦›à§‹à¦Ÿà¦¬à§‡à¦²à¦¾à¦•à§‡ à¦à¦‡ à¦¬à¦‡à¦Ÿà¦¾à¦° à¦¥à§‡à¦•à§‡ à¦†à¦²à¦¾à¦¦à¦¾ à¦•à¦°à§‡ à¦­à¦¾à¦¬à¦¤à§‡ à¦ªà¦¾à¦°à¦¿ à¦¨à¦¾à¥¤ à¦Ÿà§‡à¦¨à¦¿à¦¦à¦¾à¦° à¦¸à¦¾à¦¥à§‡ à¦†à¦®à¦¾à¦° à¦ªà§à¦°à¦¥à¦® à¦ªà¦°à¦¿à¦šà¦¯à¦¼ à¦•à§‹à¦¨ à¦à¦•à¦Ÿà¦¾ à¦•à§à¦²à¦¾à¦¸à§‡ à¦ªà§à¦°à¦¥à¦® à¦¹à¦¬à¦¾à¦° à¦¦à§Œà¦²à¦¤à§‡ à¦¦à¦¿à¦¦à¦¾à¦° à¦‰à¦ªà¦¹à¦¾à¦° à¦¦à§‡à¦“à¦¯à¦¼à¦¾ à¦Ÿà§‡à¦¨à¦¿à¦¦à¦¾ à¦¸à¦®à¦—à§à¦°à¦° à¦®à¦¾à¦§à§à¦¯à¦®à§‡à¥¤ à¦¹à§‡à¦®à§‡à¦¨à§à¦¦à§à¦°à¦•à§à¦®à¦¾à¦° à¦°à¦¾à¦¯à¦¼ à¦°à¦šà¦¨à¦¾à¦¬à¦²à§€ (à§§à§­) More on it à¦¨à¦¿à¦ƒà¦¸à¦¨à§à¦¦à§‡à¦¹à§‡ à¦†à¦®à¦¾à¦° à¦ªà§œà¦¾ à¦…à¦¨à§à¦¯à¦¤à¦® à¦¶à§à¦°à§‡à¦·à§à¦  à¦•à¦¿à¦¶à§‹à¦°à¦¸à¦¾à¦¹à¦¿à¦¤à§à¦¯à¥¤ à¦•à§à¦°à¦¿à¦•à§‡à¦Ÿ à¦–à§‡à¦²à¦¤à§‡ à¦—à¦¿à¦¯à¦¼à§‡ à¦¦à§‡à¦°à¦¿ à¦•à¦°à§‡ à¦¬à¦¾à¦¡à¦¼à¦¿ à¦«à¦¿à¦°à§‡ à¦®à¦¾à¦¯à¦¼à§‡à¦° à¦¬à¦•à§à¦¨à¦¿ à¦–à¦¾à¦“à¦¯à¦¼à¦¾, à¦•à¦¿à¦‚à¦¬à¦¾ à¦œà¦¾à¦®à¦°à§à¦² à¦ªà¦¾à¦°à¦¤à§‡ à¦—à¦¿à¦¯à¦¼à§‡ à¦•à¦¾à¦ à¦ªà¦¿à¦à¦ªà¦¡à¦¼à§‡à¦° à¦•à¦¾à¦®à¦¡à¦¼ à¦–à¦¾à¦“à¦¯à¦¼à¦¾à¦° à¦®à¦¤ à¦›à§‹à¦Ÿà¦¬à§‡à¦²à¦¾à¦° à¦¬à¦¿à¦­à¦¿à¦¨à§à¦¨ à¦¸à§à¦®à§ƒà¦¤à¦¿à¦° à¦¸à¦¾à¦¥à§‡ à¦à¦‡ à¦¬à¦‡à¦Ÿà¦¾ à¦ªà¦¡à¦¼à¦¾à¦° à¦¸à§à¦–à¦¸à§à¦®à§ƒà¦¤à¦¿ à¦à¦®à¦¨à¦­à¦¾à¦¬à§‡ à¦œà¦¡à¦¼à¦¿à¦¯à¦¼à§‡ à¦†à¦›à§‡ à¦¯à§‡ à¦†à¦œ à¦†à¦° à¦†à¦®à¦¾à¦° à¦›à§‹à¦Ÿà¦¬à§‡à¦²à¦¾à¦•à§‡ à¦à¦‡ à¦¬à¦‡à¦Ÿà¦¾à¦° à¦¥à§‡à¦•à§‡ à¦†à¦²à¦¾à¦¦à¦¾ à¦•à¦°à§‡ à¦­à¦¾à¦¬à¦¤à§‡ à¦ªà¦¾à¦°à¦¿ à¦¨à¦¾à¥¤ à¦Ÿà§‡à¦¨à¦¿à¦¦à¦¾à¦° à¦¸à¦¾à¦¥à§‡ à¦†à¦®à¦¾à¦° à¦ªà§à¦°à¦¥à¦® à¦ªà¦°à¦¿à¦šà¦¯à¦¼ à¦•à§‹à¦¨ à¦à¦•à¦Ÿà¦¾ à¦•à§à¦²à¦¾à¦¸à§‡ à¦ªà§à¦°à¦¥à¦® à¦¹à¦¬à¦¾à¦° à¦¦à§Œà¦²à¦¤à§‡ à¦¦à¦¿à¦¦à¦¾à¦° à¦‰à¦ªà¦¹à¦¾à¦° à¦¦à§‡à¦“à¦¯à¦¼à¦¾ à¦Ÿà§‡à¦¨à¦¿à¦¦à¦¾ à¦¸à¦®à¦—à§à¦°à¦° à¦®à¦¾à¦§à§à¦¯à¦®à§‡à¥¤ à¦†à¦®à¦¿ à¦—à§à¦°à¦¾à¦®à§‡à¦° à¦›à§‡à¦²à§‡, à¦•à¦¿à¦¨à§à¦¤à§ à¦¸à¦¤à§à¦¯à¦¿à¦•à¦¾à¦°à§‡à¦° à¦­à¦¾à¦²à§‹ à¦à¦•à¦Ÿà¦¾ à¦•à¦¿à¦¶à§‹à¦°à¦¸à¦¾à¦¹à¦¿à¦¤à§à¦¯ à¦¯à§‡ à¦—à§à¦°à¦¾à¦®-à¦¶à¦¹à¦°à§‡à¦° à¦¸à¦™à§à¦•à§€à¦°à§à¦£ à¦­à§‚à¦—à§‹à¦²à§‡à¦° à¦¸à§€à¦®à¦¾ à¦¹à§‡à¦²à¦¾à¦¯à¦¼ à¦…à¦¤à¦¿à¦•à§à¦°à¦® à¦•à¦°à§‡ à¦¯à§‡à¦¤à§‡ à¦ªà¦¾à¦°à§‡ à¦¤à¦¾à¦° à¦ªà§à¦°à¦®à¦¾à¦£ à¦¦à§‡à¦¯à¦¼ à¦à¦‡ à¦¬à¦‡: à¦–à¦¾à¦¸ à¦•à§‹à¦²à¦•à¦¾à¦¤à¦¾ à¦¶à¦¹à¦°à§‡à¦° à¦šà¦¾à¦° à¦•à¦¿à¦¶à§‹à¦°à§‡à¦° adventure (à¦à¦¬à¦‚, à¦†à¦° à¦¬à§‡à¦¶à¦¿ misadventure) à¦—à§à¦²à§‹ à¦¥à§‡à¦•à§‡ à¦®à¦œà¦¾ à¦ªà§‡à¦¤à§‡ à¦†à¦®à¦¾à¦° à¦•à§‹à¦¨ à¦…à¦¸à§à¦¬à¦¿à¦§à¦¾ à¦¹à¦¯à¦¼ à¦¨à¦¿à¥¤ à¦à¦‡ à¦¬à¦‡à¦¤à§‡ à¦ªà¦¾à¦“à¦¯à¦¼à¦¾ à¦¨à¦¾à¦°à¦¾à¦¯à¦¼à¦£ à¦—à¦™à§à¦—à§‹à¦ªà¦¾à¦§à§à¦¯à¦¾à¦¯à¦¼à§‡à¦° à¦…à¦§à¦¿à¦•à¦¾à¦‚à¦¶ à¦—à¦²à§à¦ªà¦—à§à¦²à¦¿à¦‡ à¦¨à¦¿à¦°à§à¦­à§‡à¦œà¦¾à¦² à¦¹à¦¾à¦¸à¦¿à¦°, à¦…à¦¨à¦¾à¦¬à¦¿à¦² à¦†à¦¨à¦¨à§à¦¦à§‡à¦° à¦‰à§à¦¸: ...à¦Ÿà§‡à¦¨à¦¿à¦¦à¦¾ à¦¤à¦¬à§ à¦¹à¦¾à¦à¦¡à¦¼à¦¿à¦Ÿà¦¾à¦•à§‡ à¦›à¦¾à¦¡à¦¼à§‡ à¦¨à¦¾à¥¤ à¦¶à§‡à¦·à¦•à¦¾à¦²à§‡ à¦®à§à¦–à§‡à¦° à¦“à¦ªà¦° à¦¤à§à¦²à§‡ à¦šà§‹à¦ à¦•à¦°à§‡ à¦°à¦¸à¦Ÿà¦¾ à¦ªà¦°à§à¦¯à¦¨à§à¦¤ à¦¨à¦¿à¦•à§‡à¦¶ à¦•à¦°à§‡ à¦¦à¦¿à¦²à§‡à¥¤ à¦¤à¦¾à¦°à¦ªà¦° à¦¨à¦¾à¦•-à¦Ÿà¦¾à¦• à¦•à§à¦à¦šà¦•à§‡ à¦¬à¦²à¦²à§‡, à¦¦à§à¦¤à§à¦¤à§‹à¦°, à¦—à§‹à¦Ÿà¦¾à¦•à¦¯à¦¼à§‡à¦• à¦¡à§‡à¦¯à¦¼à§‹ à¦ªà¦¿à¦à¦ªà¦¡à¦¼à§‡à¦“ à¦–à§‡à¦¯à¦¼à§‡ à¦«à§‡à¦²à¦²à§à¦® à¦°à§‡! à¦œà§à¦¯à¦¾à¦¨à§à¦¤à¦“ à¦›à¦¿à¦² à¦¦à§â€™-à¦¤à¦¿à¦¨à¦Ÿà§‡! à¦ªà§‡à¦Ÿà§‡à¦° à¦­à§‡à¦¤à¦°à§‡ à¦—à¦¿à¦¯à¦¼à§‡ à¦•à¦¾à¦®à¦¡à¦¼à¦¾à¦¬à§‡ à¦¨à¦¾ à¦¤à§‹? à¦¹à¦¾à¦¬à§à¦² à¦¬à¦²à¦²à§‡, à¦•à¦¾à¦®à¦¡à¦¼à¦¾à¦‡à¦¤à§‡à¦“ à¦ªà¦¾à¦°à§‡à¥¤ à¦•à¦¾à¦®à¦¡à¦¼à¦¾à¦• à¦—à§‡, à¦¬à¦¯à¦¼à§‡ à¦—à§‡à¦²! à¦à¦•à¦¬à¦¾à¦° à¦­à§€à¦®à¦°à§à¦²-à¦¸à§à¦¦à§à¦§ à¦à¦•à¦Ÿà¦¾ à¦œà¦¾à¦®à¦°à§à¦² à¦–à§‡à¦¯à¦¼à§‡ à¦«à§‡à¦²à§‡à¦›à¦¿à¦²à§à¦®, à¦¤à¦¾ à¦¸à§‡-à¦‡ à¦¯à¦–à¦¨ à¦•à¦¿à¦›à§ à¦•à¦°à¦¤à§‡ à¦ªà¦¾à¦°à¦²à§‡ à¦¨à¦¾, à¦¤à¦–à¦¨ à¦•â€™à¦Ÿà¦¾ à¦ªà¦¿à¦à¦ªà¦¡à¦¼à§‡à¦¤à§‡ à¦†à¦° à¦•à§€ à¦•à¦°à¦¬à§‡! à¦¤à¦¾à¦‡ à¦à¦‡ à¦¬à¦‡ à¦•à§à¦²à¦¾à¦¸à§‡ à¦ªà§à¦°à¦¥à¦® à¦¹à¦¬à¦¾à¦° à¦¦à§Œà¦¡à¦¼à§‡ à¦•à§à¦²à¦¾à¦¨à§à¦¤ à¦†à¦œà¦•à§‡à¦° à¦•à¦¿à¦¶à§‹à¦°-à¦•à¦¿à¦¶à§‹à¦°à§€à¦¦à§‡à¦° à¦…à¦¬à¦¶à§à¦¯à¦ªà¦¾à¦ à§à¦¯à¥¤ à¦†à¦° à¦®à¦œà¦¾ à¦›à¦¾à¦¡à¦¼à¦¾à¦“ à¦à¦‡ à¦¬à¦‡ à¦¥à§‡à¦•à§‡ à¦¬à¦¾à¦¸à§à¦¤à¦¬-à¦œà§€à¦¬à¦¨à§‡ à¦•à¦¾à¦œà§‡ à¦²à¦¾à¦—à¦¾à¦¨à§‹à¦° à¦®à¦¤ à¦…à¦¨à§‡à¦• à¦•à¦¿à¦›à§ à¦¶à§‡à¦–à¦¾ à¦¯à¦¾à¦¯à¦¼à¥¤ à¦à¦•à¦Ÿà¦¿ à¦¨à¦®à§à¦¨à¦¾: à¦¹à§à¦, à¦•à¦¬à¦¿ à¦¹à¦“à¦¯à¦¼à¦¾ à¦–à§à¦¬ à¦–à¦¾à¦°à¦¾à¦ªà¥¤ à¦†à¦®à¦¾à¦° à¦ªà¦¿à¦¸à¦¤à§à¦¤à§‹ à¦­à¦¾à¦‡ à¦«à§à¦šà§à¦¦à¦¾ à¦à¦•à¦¬à¦¾à¦° à¦•à¦¬à¦¿ à¦¹à¦¯à¦¼à§‡à¦›à¦¿à¦²à§‹à¥¤ à¦¦à¦¿à¦¨à¦°à¦¾à¦¤ à¦•à¦¬à¦¿à¦¤à¦¾ à¦²à¦¿à¦–à¦¤à¥¤ à¦à¦•à¦¦à¦¿à¦¨ à¦°à¦¾à¦®à¦§à¦¨ à¦§à§‹à¦ªà¦¾à¦° à¦–à¦¾à¦¤à¦¾à¦¯à¦¼ à¦•à¦¬à¦¿à¦¤à¦¾ à¦•à¦°à§‡ à¦²à¦¿à¦–à¦² : à¦ªà¦¾à¦à¦šà¦–à¦¾à¦¨à¦¾ à¦§à§à¦¤à¦¿, à¦¸à¦¾à¦¤à¦–à¦¾à¦¨à¦¾ à¦¶à¦¾à¦¡à¦¼à¦¿ à¦-à¦¸à¦¬ à¦¹à¦¿à¦¸à§‡à¦¬à§‡ à¦¹à¦‡à¦¬à§‡ à¦•à¦¿à¦¬à¦¾? à¦-à¦œà¦—à¦¤à§‡ à¦œà§€à¦¬ à¦•à¦¤ à¦¬à§à¦¯à¦¾à¦¥à¦¾ à¦ªà¦¾à¦¯à¦¼, à¦¤à¦¾à¦‡ à¦­à¦¾à¦¬à¦¿ à¦†à¦®à¦¿ à¦°à¦¾à¦¤à§à¦°à¦¿-à¦¦à¦¿à¦¬à¦¾à¥¤ à¦°à¦¾à¦®à¦§à¦¨à§‡à¦° à¦“à¦‡ à¦¬à§ƒà¦¦à§à¦§ à¦—à¦¾à¦§à¦¾ à¦®à¦¨à¦Ÿà¦¿ à¦¤à¦¾à¦¹à¦¾à¦° à¦¬à¦¡à¦¼à¦‡ à¦¸à¦¾à¦¦à¦¾- à¦¸à§‡-à¦¬à§‡à¦šà¦¾à¦°à¦¾ à¦¤à¦¾à¦° à¦ªà¦¿à¦ à§‡à¦¤à§‡ à¦šà¦¾à¦ªà¦¾à¦¯à¦¼à§‡ à¦•à¦¤ à¦¶à¦¾à¦¡à¦¼à¦¿-à¦§à§à¦¤à¦¿-à¦ªà§à¦¯à¦¾à¦¨à§à¦Ÿ à¦²à¦‡à¦¯à¦¼à¦¾ à¦¯à¦¾à¦¯à¦¼- à¦®à¦¨à§‹à¦¦à§à¦–à§‡ à¦–à¦¾à¦²à¦¿ à¦¬à§‹à¦à¦¾ à¦Ÿà§‡à¦¨à§‡ à¦«à§‡à¦°à§‡ à¦—à¦¾à¦§à¦¾ à¦à¦•à¦–à¦¾à¦¨à¦¾ à¦§à§à¦¤à¦¿-à¦ªà§à¦¯à¦¾à¦¨à§à¦Ÿ à¦ªà¦°à¦¿à¦¤à§‡ à¦¨à¦¾ à¦ªà¦¾à¦¯à¦¼! â€¦à¦•à¦¿à¦¨à§à¦¤à§ à¦ªà¦¿à¦¸à¦¿à¦®à¦¾ à¦§à§‹à¦ªà¦¾à¦° à¦¹à¦¿à¦¸à§‡à¦¬à§‡à¦° à¦–à¦¾à¦¤à¦¾à¦¯à¦¼ à¦à¦‡à¦¸à¦¬ à¦¦à§‡à¦–à§‡ à¦­à§€à¦·à¦£ à¦°à§‡à¦—à§‡ à¦—à§‡à¦²! à¦°à§‡à¦—à§‡ à¦—à¦¿à¦¯à¦¼à§‡ à¦¹à¦¾à¦¤à§‡à¦° à¦•à¦¾à¦›à§‡ à¦†à¦° à¦•à¦¿à¦›à§ à¦¨à¦¾ à¦ªà§‡à¦¯à¦¼à§‡ à¦à¦•à¦Ÿà¦¾ à¦šà¦¾à¦²à¦•à§à¦®à¦¡à¦¼à§‹ à¦¨à¦¿à¦¯à¦¼à§‡ à¦«à§à¦šà§à¦¦à¦¾à¦•à§‡ à¦¤à¦¾à¦¡à¦¼à¦¾ à¦•à¦°à¦²à§‡à¥¤ à¦ à¦¿à¦• à¦¯à§‡à¦¨ à¦—à¦¦à¦¾ à¦¹à¦¾à¦¤à§‡ à¦¨à¦¿à¦¯à¦¼à§‡ à¦¶à¦¾à¦¡à¦¼à¦¿à¦ªà¦°à¦¾ à¦­à§€à¦® à¦¦à§Œà¦¡à¦¼à¦šà§à¦›à§‡à¥¤ à¦¤à¦¾à¦‡ à¦†à¦° à¦¦à§‡à¦°à¦¿ à¦¨à¦¾ à¦•à¦°à§‡ à¦¬à¦‡à¦Ÿà¦¾ à¦ªà¦¡à¦¼à§‡ à¦«à§‡à¦²à§à¦¨ à¦†à¦° à¦¯à¦¦à¦¿ à¦ªà¦¡à¦¼à¦¾ à¦¹à¦¯à¦¼à§‡ à¦—à¦¿à¦¯à¦¼à§‡ à¦¥à¦¾à¦•à§‡ à¦¤à§‹ à¦†à¦°à¦“ à¦à¦•à¦¬à¦¾à¦° à¦ªà¦¡à¦¼à§‡ à¦¨à¦¿à¦¨; à¦ªà§à¦°à¦¾à¦£à¦–à§‹à¦²à¦¾ à¦¹à¦¾à¦¸à¦¿ à¦†à¦ªà¦¨à¦¾à¦•à§‡ à¦¸à§à¦¸à§à¦¥ à¦°à¦¾à¦–à¦¬à§‡à¥¤"
  },
  {
    "title": "Open Source: Ideas and Tools",
    "url": "https://arghyadutta.github.io/notebooks/openSource.html",
    "content": "Find out publishers' copyright policies. Writings and rehearsals by Nathan Schneider Simard, M.-A., Butler, L.-A., Alperin, J. P., & Haustein, S. (2024). We need to rethink the way we identify diamond open access journals in quantitative science studies . Quantitative Science Studies, 5(4), 1042â€“1046 . Interesting work regarding free and open source software Open policy finder If I Get Time"
  },
  {
    "title": "Funny",
    "url": "https://arghyadutta.github.io/notebooks/funny.html",
    "content": "Steve Plimptonâ€™s collection of quotes A map of every city in Europe Cartoons by Sidney Harris Joel Grusâ€”Fizz Buzz in Tensorflow"
  },
  {
    "title": "Socialism",
    "url": "https://arghyadutta.github.io/notebooks/socialism.html",
    "content": "PhilosophyInsights (Director). (2018, May 23). Stephen Hicks: How Failed Marxist Predictions Led to the Postmodern Left YouTube One notable feature of this book is that it scarcely discussed about Marxism as implemented in Russia, North Vietnam, and China. Instead we get a thorough discussion of Cuban and Swedenâ€™s socialism, which is interesting! Also the enrichment, and fragmentation, of Socialist ideal after the rise of Feminist and Left-Green movement were not left. The author is not oblivious to the issues of the system, but he doesn't shy away from pointing out the strengths. Recommended! Edwards, J., & Leiter, B. (2025). Marx . Routledge, Taylor & Francis group. My thoughts A lucid and thoughtful book; the author has done an impressive job. After describing the basic tenets of Socialism, the author discussed about the existent Socialist tradition before Marx came up with his scientific socialism. I liked the informative analysis on the early Utopians like the popular, but theoretically not so convincing, Etienne Cabet and more sound theorists like Henri de Saint-Simon, Charles Fourier, and Robert Owens. Then the Anarchist theories of Proudhon and Bakunin were also discussed in some detail. This studies were interspersed with the authorâ€™s thoughtful comment throughout, which I liked. And here is one slightly unnerving quote from Proudhon: To be governed is to be watched over, inspected, spied on, directed, legislated at, regulated, docketed, indoctrinated, preached at, controlled, assessed, weighed, censored, ordered about, by men who have neither the right nor the knowledge nor the virtueâ€¦. Thatâ€™s government, thatâ€™s its justice, thatâ€™s its morality! One notable feature of this book is that it scarcely discussed about Marxism as implemented in Russia, North Vietnam, and China. Instead we get a thorough discussion of Cuban and Swedenâ€™s socialism, which is interesting! Also the enrichment, and fragmentation, of Socialist ideal after the rise of Feminist and Left-Green movement were not left. The author is not oblivious to the issues of the system, but he doesn't shy away from pointing out the strengths. Recommended! Russell, B. (1956). Why I am Not a Communist . In Portraits from Memory. https://www.rjgeib.com/thoughts/opiate/why.html Arrow offers a careful analysis of capitalism and socialism, without asking you to join a camp. Gellately sometimes has placed opinions, which are not well supported by data presented in the text, in between well-argued points. Though they do not contradict his claims, I found that problematic. Also while describing the show trials, he did not present the allegations, however superficial they may be, of the Communist party against its members. The clashes between the Nazis and the Communist party members of Germany are somewhat less described. Overall, I will recommend this book to anyone interested in the modus operandi of totalitarian regimes. Newman, M. Socialism: A Very Short Introduction . Oxford University Press, 2005. The Communist party received enormous support from the citizens, mainly urban population. At the time of show trial of Nikolai Bukharin, after complications regarding the New Economic Policy, many prominent literary figures singed an open letter saying \"We demand the spies' execution! We shall not allow the enemies of the Soviet Union to live\". The writers include, among others, Mikhail Sholokov, Alexei Tolstoy, and surprisingly, Vasily Grossman, who was once saved from imprisonment by Bukharin's direct help. Grossman demanded \"No mercy to the Trotskyite degenerates, the murderous accomplishes of fascism.\" I mean, wow. In the first part, he presents some arguments to convince the reader that Lenin, in the short time he was in power, had shown remarkable similarity with Stalin: punishing and 'liquidating' dissenters, being ruthless with the concerns of common Russians, specially peasants, overruling the possibility of a democracyâ€”at times berating itâ€”, and taking brutal steps to implement communist policies. Thorough summary; recommended. Not entirely on socialism, but it touches on Marx's alienation. I think the author is hopelessly romanticâ€”but then whatâ€™s life if one can't even dream? After describing the basic tenets of Socialism, the author discussed about the existent Socialist tradition before Marx came up with his scientific socialism. I liked the informative analysis on the early Utopians like the popular, but theoretically not so convincing, Etienne Cabet and more sound theorists like Henri de Saint-Simon, Charles Fourier, and Robert Owens. Then the Anarchist theories of Proudhon and Bakunin were also discussed in some detail. This studies were interspersed with the authorâ€™s thoughtful comment throughout, which I liked. And here is one slightly unnerving quote from Proudhon: To be governed is to be watched over, inspected, spied on, directed, legislated at, regulated, docketed, indoctrinated, preached at, controlled, assessed, weighed, censored, ordered about, by men who have neither the right nor the knowledge nor the virtueâ€¦. Thatâ€™s government, thatâ€™s its justice, thatâ€™s its morality! A lucid and thoughtful book; the author has done an impressive job. A mostly cogent analysis of the similarities and dissimilarities between the totalitarian regimes of Lenin, Stalin, and Hitlerâ€”and he anticipated that the inclusion of Lenin with the other two will shock many. In the subsequent sections, he shows parallels between Hitler and Stalin. His account of Stalin is particularly well written. While dekulakizing USSR, after the number of rich, and yet unpunished, peasants reached a really low number, it became difficult to determine \"who is a kulak?\". To settle that it was decided that any family with two samovars or a \"status symbol\" was to be considered as a kulak. One astonishing fact about that period is recent analyses show that the cheap slave labour by Gulag inmates was actually counter-productive for Russian economy. Like the common Russians, the party members also faced Stalin's wrath. According to Stalin, if any member committed suicide to evade a show trial, he \"is covering his track and deceiving the party for one last time. Suicide is simply a method to spit on the party for the last time\". Arrow, K. J. (1978). A Cautious Case for Socialism . Dissent Magazine Ferretter, L. (2006). Louis Althusser (R. Eaglestone, Ed.; 1st edition). Routledge. Recommended If I get time More A mostly cogent analysis of the similarities and dissimilarities between the totalitarian regimes of Lenin, Stalin, and Hitlerâ€”and he anticipated that the inclusion of Lenin with the other two will shock many. In the first part, he presents some arguments to convince the reader that Lenin, in the short time he was in power, had shown remarkable similarity with Stalin: punishing and 'liquidating' dissenters, being ruthless with the concerns of common Russians, specially peasants, overruling the possibility of a democracyâ€”at times berating itâ€”, and taking brutal steps to implement communist policies. In the subsequent sections, he shows parallels between Hitler and Stalin. His account of Stalin is particularly well written. While dekulakizing USSR, after the number of rich, and yet unpunished, peasants reached a really low number, it became difficult to determine \"who is a kulak?\". To settle that it was decided that any family with two samovars or a \"status symbol\" was to be considered as a kulak. One astonishing fact about that period is recent analyses show that the cheap slave labour by Gulag inmates was actually counter-productive for Russian economy. Like the common Russians, the party members also faced Stalin's wrath. According to Stalin, if any member committed suicide to evade a show trial, he \"is covering his track and deceiving the party for one last time. Suicide is simply a method to spit on the party for the last time\". The Communist party received enormous support from the citizens, mainly urban population. At the time of show trial of Nikolai Bukharin, after complications regarding the New Economic Policy, many prominent literary figures singed an open letter saying \"We demand the spies' execution! We shall not allow the enemies of the Soviet Union to live\". The writers include, among others, Mikhail Sholokov, Alexei Tolstoy, and surprisingly, Vasily Grossman, who was once saved from imprisonment by Bukharin's direct help. Grossman demanded \"No mercy to the Trotskyite degenerates, the murderous accomplishes of fascism.\" I mean, wow. Gellately sometimes has placed opinions, which are not well supported by data presented in the text, in between well-argued points. Though they do not contradict his claims, I found that problematic. Also while describing the show trials, he did not present the allegations, however superficial they may be, of the Communist party against its members. The clashes between the Nazis and the Communist party members of Germany are somewhat less described. Overall, I will recommend this book to anyone interested in the modus operandi of totalitarian regimes. Gellately, R. (2008). Lenin, Stalin, and Hitler: The Age of Social Catastrophe (Reprint edition). Vintage. Mommsen, P. (2025, March 3). The Quest to Emancipate Labor. Plough Gilabert, P., & Oâ€™Neill, M. (2024). Socialism . In E. N. Zalta & U. Nodelman (Eds.). The Stanford Encyclopedia of Philosophy"
  },
  {
    "title": "Physics Free Books",
    "url": "https://arghyadutta.github.io/notebooks/physicsFreeBooks.html",
    "content": "Online course on topology in condensed matter Entropy by John Baez Group Theory in Physics: An Introduction with Mathematica by Balasubramanian Ananthanarayan, Souradeep Das, Amitabha Lahiri, Suhas Sheikh, Sarthak Talukdar Electrodynamics by Phil Nelson (excellent book!) Lecture notes on Soft Matter and Interfaces by LydÃ©ric Bocquet Lecture notes on Statistical Physics by LydÃ©ric Bocquet Understanding the Properties of Matter by Michael de Podesta Feynman lectures"
  },
  {
    "title": "Machine Learning: Free Books",
    "url": "https://arghyadutta.github.io/notebooks/mlFreeBooks.html",
    "content": "The Little Book of Deep Learning by FranÃ§ois Fleuret. He formatted the book for reading on smartphonesâ€”such innovative people! Deep Learning for Molecules and Materials by Andrew D. White Interpretable Machine Learning by Christoph Molnar Information Theory, Inference, and Learning Algorithms by David MacKay Model-Based Machine Learning by John Winn An Introduction to Statistical Learning by Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani The Elements of Statistical Learning by Trevor Hastie, Robert Tibshirani, Jerome Friedman"
  },
  {
    "title": "Non-linear Dynamics",
    "url": "https://arghyadutta.github.io/notebooks/nld.html",
    "content": "Excellent! Recommended Krishnaswami, G. S. (2020). Notes on Nonlinear Dynamics . PDF Strogatz, S. H. (2015). Nonlinear dynamics and chaos: With applications to physics, biology, chemistry, and engineering (Second edition). Westview Press. Percival, I.; Richards, D. Introduction to Dynamics; Cambridge University Press: Cambridgeâ€¯; New York, 1982. Ross, S. L. (2004). Differential equations (Third edition, Wiley student edition). Wiley India. The chapter on nonlinear differential equations is especially neat. I liked the idea of introducing critical points and paths first for linear systems, and then for nonlinear systems. Concise."
  },
  {
    "title": "Stoicism",
    "url": "https://arghyadutta.github.io/notebooks/stoicism.html",
    "content": "If I get time Recommended Another run-of-the-mill, clichÃ©-filled self-help book written to optimize every second of your life. Don't buy it. Seneca, A. L. (2000). Letters From A Stoic (Reprint edition). Penguin UK. Long, A. A. (2004). Epictetus: A Stoic and Socratic Guide to Life (1st edition). Clarendon Press. Holiday, R. (2016). The Daily Stoic: 366 Meditations on Wisdom, Perseverance, and the Art of Living. Profile Books. Epictetus. (2018). How to Be Free: An Ancient Guide to the Stoic Life (A. Long, Trans.; Early Printing edition). Princeton University Press. Farnsworth is a careful writer, and he lays down the main tenets of Stoicism with ample quotes and nuanced analysis. Not recommended Farnsworth, W. (2018). The Practicing Stoic: A Philosophical Userâ€™s Manual . David R. Godine, Publisher."
  },
  {
    "title": "Academia: Issues",
    "url": "https://arghyadutta.github.io/notebooks/academiaIssues.html",
    "content": "Skinner, B. (2019) What It Means, and Doesnâ€™t Mean, to Get a Job in Physics . Gravity and Levity Recommended Sierra, C. A. (n.d.). Reject Nature: Elite journals and the defeat of science . Retrieved December 6, 2021, from his blog \". Academia, the Good Side The unsavory side of academia. See also Curry, S. Sick of Impact Factors . Reciprocal Space . Retrieved December 6, 2021. Lin, J. (2010). Unraveling tenure at MIT . The Tech Lenz, M. (2020). The adversarial culture in philosophy does not serve the truth Aeon Ideas. Aeon. Nothing we make, alas, is perfect. To be fair, I've published in Nature family of journalsâ€”but never as a corresponding author. I think Sierra's is argument is sound."
  },
  {
    "title": "Foundations of Quantum mechanics",
    "url": "https://arghyadutta.github.io/notebooks/qMechFoundations.html",
    "content": "Courses on YouTube Recommended Asher Peres's book goes deep early on and is easily the most difficult of all the books mentioned in this list. It's very rewarding, though. Foundations of quantum mechanics is a great subject, until you want to publish, of course. Norsen's accessible and mildly opinionated book provides a good introduction. Foundations of Quantum Mechanics by Florian Marquardt . Norsen, T. (2017). Foundations of Quantum Mechanics: An Exploration of the Physical Meaning of Quantum Theory (1st ed.). Springer International Publishing Another series, by Rob Spekkens, on Foundations of Quantum Mechanics . Fun slides. Peres, A. (2010). Quantum theory: Concepts and methods . Kluwer Acad. Publ."
  },
  {
    "title": "Bayesian Methods",
    "url": "https://arghyadutta.github.io/notebooks/bayesian.html",
    "content": "van de Schoot, R.; Depaoli, S.; King, R.; Kramer, B.; MÃ¤rtens, K.; Tadesse, M. G.; Vannucci, M.; Gelman, A.; Veen, D.; Willemsen, J.; Yau, C. Bayesian Statistics and Modelling . Nat Rev Methods Primers 2021, 1 (1), 1â€“26 . Recommended The most beginner friendly introduction to Bayesian statistics that I've came across. Bayesian methods course by Richard McElreath Eddy, S. R. What Is Bayesian Statistics? Biotechnol 2004, 22 (9), 1177â€“1178 . Neat description of Bayesian ideas (it's a neuroscience book). Online Resources Statistics for application by Phillippe Rigollet Titelbaum, M. G. How to Think like a Bayesian . Psyche. 2024 . (accessed 2024-01-29). Ma, W. J.; Kording, K.; Goldreich, D. Bayesian Models of Perception and Action: An Introduction ; The MIT Press: London, England, 2023"
  },
  {
    "title": "Biology",
    "url": "https://arghyadutta.github.io/notebooks/biology.html",
    "content": "Recommended A unique book, like Phillips's other ones. I haven't read another 'dictionary' quite like it. Outstanding content and Harold is a master of prose. Much better than standard popular science books. Medawar, P. B., & Medawar, J. S. (1985). Aristotle to Zoos: A Philosophical Dictionary of Biology . Oxford University Press. Milo, R., & Phillips, R. (2016). Cell biology by the numbers. Garland Science, Taylor & Francis Group. ( Online free version ) Harold, F. M. (2001). The Way of the Cell: Molecules, Organisms, and the Order of Life . Oxford University Press. An Owner's Guide to the Human Genome: an introduction to human population genetics, variation and disease by Jonathan Pritchard"
  },
  {
    "title": "Buddha and Buddhism",
    "url": "https://arghyadutta.github.io/notebooks/buddhism.html",
    "content": "Kornfield, J. (2005). The Dhammapada: A New Translation of the Buddhist Classic with Annotations (G. Fronsdal, Trans.). Shambhala. Recommended The Dhammapada: Verses and Stories Bhattacharya, K. (1975). On the Bramhan in Buddhist Literature. Oriental Journal Tirupati, XVIII. If I Get Time Rahula, W. (1974). What the Buddha Taught: Revised and Expanded Edition with Texts from Suttas and Dhammapada (Revised edition). Grove Press."
  },
  {
    "title": "Loschmidt Paradox",
    "url": "https://arghyadutta.github.io/notebooks/loschmidtParadox.html",
    "content": "Consider the time evolution of a system $N$ particles. In the $6N$-dimensional phase space, a point $(q_i, p_i)\\equiv(q_1,p_1,q_2,p_2,\\cdots,q_{3N},p_{3N})$ denote one microstate. Loschmidt argued that for a system starting from $(q_i(t_0),p_i(t_0))$ and evolving to $(q_i(t_f),p_f(t_f))$, if we reverse the momenta of all particles at $t_f$â€”i.e., if we can prepare a system described by $(q_i(t_0),-p_i(t_0))$â€”then the system should evolve back to the initial state at a later time because the laws of classical mechanics are time reversible. If true, this implies the amazing phenomenon of a gas spontaneously returning to a smaller volume from a larger volume. Recommended Greiner, W., Neise, L., & StÃ¶cker, H. (1995). Thermodynamics and Statistical Mechanics. Springer. (pp. 43â€“46) Boltzmann solved this paradox by noting that: As a concrete example, the number of microstate of a system with $N$ particles is proportional to $V^N$. So if you halve the volume, the available microstates decrease by $1/2^N$. So, in the thermodynamic limit, the probability of a gas spontaneously collapsing to half of its volume is $1/2^N \\rightarrow 0$. An overwhelmingly large number of microstates are compatible with a macrostate, specified by a few state quantities like pressure, volume, and temperature. That means the initial state typically evolves to one of numerous possible microstates which cannot be distinguished macroscopically. That means the exactly one final state that allows the system to go back to the initial state is the actual final state can occur with a miniscule probability. Swendsen, R. H. (2008). Explaining irreversibility . American Journal of Physics, 76(7), 643â€“648 . Moreover, all the microstates with same total energy can be found with equal probability."
  },
  {
    "title": "Books and Libraries",
    "url": "https://arghyadutta.github.io/notebooks/bookNLibrary.html",
    "content": "If I get time Piper, A. (2009). Dreaming in books: The making of the bibliographic imagination in the Romantic age . University of Chicago press. CarriÃ¨re, J.-C., & Eco, U. (2011). This is Not the End of the Book . Harvill Secker. Is it okay to appreciate creations by imperfect human beings? Morton offers a nuanced perspective. Eco is interesting, as usual. Meanwhile, CarriÃ¨re: And anyway, what about the cultures that havenâ€™t developed what we call philosophy? Thatâ€™s what I meant just now by saying that anthropology is just as important. The notion of a â€˜philosophical conceptâ€™, for instance, is a purely Western one. Try explaining â€˜conceptâ€™ to an Indianâ€”even a highly sophisticated oneâ€”or â€˜transcendenceâ€™ to a Chinese person! (p. 233 in the eBook) Really, my man? (To be fair, he wrote a long French play based on the Indian epic Mahabharata, as he was \"completely enchanted\" by it. So, maybe he meant something else here? I don't know.) Piper, A. (2018). Enumerations: Data and literary study . The University of Chicago Press. Morton, B. (2019, January 8). Virginia Woolf? Snob! Richard Wright? Sexist! Dostoyevsky? Anti-Semite! The New York Times . Piper, A. (2012). Book was there: Reading in electronic times . University of Chicago Press."
  },
  {
    "title": "Time's Arrow",
    "url": "https://arghyadutta.github.io/notebooks/timesArrow.html",
    "content": "If I Get Time Arrow of Time and Irreversibility"
  },
  {
    "title": "On music",
    "url": "https://arghyadutta.github.io/notebooks/music.html",
    "content": "Recommended Dhar, S. (2017). Raga'N Josh . The Orient Blackswan. Ayyangar, R. R. (2019). History of South Indian (Carnatic) Music , Third edition. Vipanci Charitable Trust. Chatterjee, S. (2023). Choral Voices: Ethnographic Imaginations of Sound and Sacrality . Bloomsbury Publishing USA. Ethnographic studies of choirs in Shillong and Goa to explore choral singing and postcolonial musical practices, highlighting the interplay of individual and collective identities. The tone is academic, yet it's accessible and well-writtenâ€”a fairly rare occurrence. TMK has strong opinions, and I sometimes disagree with him, but the book is informative, empathetic, and well-written. Recommended. Incidentally, an organizationâ€”Kaahonâ€”has a YouTube channel and they produced a couple of good documentaries on instrument makers from Bengal, like this one on Rudra Veena makers and this one on Sitar makers. Their tales are similar to the mridangam makersâ€”especially the lack of recognition. The list is chronologically ordered. Krishna, T. M. (2022). Sebastian & Sons: A Brief History of Mrdangam Makers . Westland Publications Limited. Sheila Dharâ€™s witty reflections on the world of Indian classical music. Some descriptions are unforgettable, like when Pandit Pran Nath tuned a Tanpura for over an hour until it sounded just perfect. Informative and sometimes fun, but the overall snobbish tone ruined it for me. Mukhaopadhyay, K. P. (2014). Kudrat Rangibirangi (1st edition). Ananda Publishers. (In Bengali)"
  },
  {
    "title": "Relativity: Special and General Theory",
    "url": "https://arghyadutta.github.io/notebooks/relativity.html",
    "content": "Schutz, B. F. (2009) A First Course in General Relativity , 2nd ed.; Cambridge University Press. Recommended If I get time Coleman, S. (2021) Sidney Colemanâ€™s Lectures on Relativity , ; Griffiths, D. J., Derbes, D., Sohn, R. B., Eds.; Cambridge University Press Really good introductory book. Especially the first few chapters introducing the idea of tensors are excellent. Guidry, M. (2019) Modern General Relativity: Black Holes, Gravitational Waves, and Cosmology ; Cambridge university press It should not be the first book, but it's well-written and full of insights."
  },
  {
    "title": "Mathematics for Quantum Mechanics",
    "url": "https://arghyadutta.github.io/notebooks/qMechMath.html",
    "content": "Normal operator : $A$ is normal if $A^\\dagger A = A A^\\dagger$. A normal matrix is Hermitian if and only if it has real eigenvalues. Eigenvectors of an Hermitian operators with distinct eigenvalues are orthogonal. Bases Savov, I. (2020). No Bullshit Guide to Linear Algebra (2nd V2.2 ed. edition). Minireference Co. Singular Value Decomposition A unitary operator is a normal operator with $A^\\dagger A = A A^\\dagger=I$. Since $U$ is normal, it's diagonalizable, too. $U$ preserves inner product $(U\\ket{v}, U\\ket{w}) = \\ket{v}\\ket{w}$. $U = \\sum_i \\ket{w_i}\\bra{v_i}$ Spanning set need not be unique . $\\begin{smallmatrix}1 \\\\1 \\end{smallmatrix}$ and $\\begin{smallmatrix}1 \\\\-1 \\end{smallmatrix}$ span $\\mathbb{C}^2$, too. Also, $\\hat{i}$ and $\\hat{j}$ and $45$Â° anti-clockwise-rotated vectors $(\\hat{i}+\\hat{j})$ and $(-\\hat{i}+\\hat{j})$ both spans $\\mathbb{R}^2$. $\\{I,X,Y,Z\\}$ spans $\\mathcal{M}_2(\\mathbb{C})$. If $\\mathcal{V}$ and $\\mathcal{W}$ are $m$ and $n$ dimensional vector spaces, then $\\mathcal{V} \\otimes \\mathcal{W}$ (read as 'V tensor W') is an $mn$ dimensional vector space. Functions of Operators The vectors in $\\mathcal{B}$ uniquely combine to generate vectors in $\\mathcal{V}$. (Of course! $2\\hat{i}+3\\hat{j}$ is a unique vector in the $\\hat{i}, \\hat{j}$ basis. For a proof, see p. 21 of Johnston 2021). Tensor Products $P=\\sum_{i=1}^k \\ket{i}\\bra{i}$ $\\frac{1}{\\sqrt{2}}(\\ket{0}-\\ket{1}) = \\frac{1}{\\sqrt{2}}\\begin{bmatrix}1 \\\\-1\\end{bmatrix} \\equiv \\ket{-}$ Projector Operator Trace Some Useful Relations (incomplete, like other good things in life) Closure under addition: $\\ket{v} + \\ket{w} \\in \\mathcal{V}$ Simultaneous Diagonalization Theorem Associativity: $(\\ket{v}+\\ket{w})+\\ket{x}=\\ket{v}+(\\ket{w}+\\ket{x})$ Examples of vector spaces include $\\mathbb{R}^n$, $\\mathbb{C}^n$, $\\mathcal{M}_{m,n}(\\mathbb{F})$ (space of $m\\times n$ matrices with elements from the field $\\mathbb{F}$). One important point is when it is said that $\\mathcal{V}$ is a vector space over a field $\\mathbb{F}$, it means that the scalars used in the scalar multiplication comes from $\\mathbb{F}$ (also known as a ground field ), not that the elements of the vectors are from $\\mathbb{F}$. For example Hermitian matrices can be defined over $\\mathbb{R}$ or $\\mathbb{C}$, though its elements are from $\\mathbb{C}$. This is important since, for example, the space of $n\\times n$ Hermitian matrices $\\mathcal{M}^{\\rm H}_n$ is not a vector space over $\\mathbb{C}$, but it is a vector space over $\\mathbb{R}$. Example: $A=\\begin{pmatrix} 0 1 \\\\ 1 0 \\end{pmatrix} \\in \\mathcal{M}^{\\rm H}_2$ but since $(iA)^\\dagger\\neq iA$, it's not Hermitian, so $\\mathcal{M}^{\\rm H}_2$ isn't closed under scalar multiplication over $\\mathbb{C}$. Linear Dependence and Independence Outer-product representation reduces to diagonal representation (eigendecomposition/orthonormal decomposition) when $\\ket{v_i}, \\ket{w_j}$ are orthonormal eigenvalues of $A$. Distributivity: $(c+d)\\ket{v}=c\\ket{v}+d\\ket{v}$ Levi-Civita Incomplete, brief, and evolving \"lookup table\" on quantum mechanics for personal use, closely following Nielsen and Chuang (2010) and others. There may be typos (even conceptual errors!). If you find one, please tell me about it (argphy@gmail.com). Also, feel free to use the material for your own use. Linear operators on $\\mathcal{V}\\otimes \\mathcal{W}$ : If $A: \\mathcal{V} \\rightarrow \\mathcal{V}'$ and $B: \\mathcal{W} \\rightarrow \\mathcal{W}'$ are two linear operators, then $A\\otimes B: \\mathcal{V}\\otimes \\mathcal{W} \\rightarrow \\mathcal{V}' \\otimes \\mathcal{W}'$ is defined as A self-adjoint/Hermitian operator satisfies $A^\\dagger=A$. Orthogonal Vectors Sadun (2008) has a neat discussion in section 6.3 titled 'Bras, Kets, and Duality' pp. 152â€“156 Inner product in a tensor-product space Distributivity: $c(\\ket{v}+\\ket{w}) = c\\ket{v} +c\\ket{w}$ If $\\mathcal{V}$ is a vector space and $\\mathcal{S} \\subseteq \\mathcal{V}$, then $\\mathcal{S}$ is a subspace iff it is closed under addition and scalar multiplication. (You need not check all ten properties. See p. 9, Johnston (2021) for a proof). Cohen Tannoudji (2020) (pp. 103â€“108) and Mermin (2007) pp. 159â€“164 for careful definition of bra vectors Spectral Decomposition Existence of a 'zero vector' $\\ket{0}$ such that $\\ket{v}+\\ket{0}=\\ket{v}$ Cohen-Tannoudji, C., Diu, B., & LaloÃ«, F. (2020). Quantum mechanics. Volume 1: Basic concepts, tools, and applications (S. Reid Hemley, N. Ostrowsky, & D. Ostrowsky, Trans.; Second edition). Wiley-VCH Verlag GmbH & Co. KGaA. $\\rm{Tr}(A)=\\sum_i A_{ii}$ Linear Operators $\\frac{1}{\\sqrt{2}}(\\ket{0}-i\\ket{1} = \\frac{1}{\\sqrt{2}}\\begin{bmatrix}1 \\\\-i\\end{bmatrix} \\equiv \\ket{y_-}$ Types of Linear Operators Outer-product and Diagonal Representation A set of vectors $\\mathcal{B} \\subseteq \\mathcal{V}$ is linearly dependent if there exists a set of scalars $c_1,c_2,\\cdots,c_k$, not all zeros, such that $c_i\\ket{v}*i=\\ket{0}$. $\\mathcal{B}$ is linearly independent if it is not linearly dependent. (It's funny ğŸ˜„, but see the example for what is means.) Subspace $\\rm{Tr}(A\\ket{\\psi}\\bra{\\psi})=\\bra{\\psi}{A}\\ket{\\psi}$ Useful Kronecker product as matrix representation for $A\\otimes B$ Identity and Pauli Matrices Computational Basis Nielsen, M. A., & Chuang, I. L. (2010). Quantum computation and quantum information (10th anniversary ed). Cambridge University Press. Inner Product If $\\ket{i}$ and $\\ket{j}$ are orthonormal bases in $\\mathcal{V}$ and $\\mathcal{W}$ then $\\ket{i}\\otimes \\ket{j}$ is a basis for $\\mathcal{V}\\otimes \\mathcal{W}$. A bra vector is a linear functional $\\mathcal{V} \\rightarrow \\mathbb{C}$; i.e. it takes a ket vector to a complex number: $(\\bra{v})\\ket{w} = \\braket{v}{w}$. Cohen Tannoudji (2020) showed that the space of all such linear functionals forms another vector space $V^*$â€”known as the dual space of $V$ (pp. 103â€“108). A bra vector is an element of $V^*$ . A bra vector can also be represented as an adjoint vector: $\\ket{v}^\\dagger \\equiv \\bra{v}$. Sadun, L. A. (2008). Applied linear algebra: The decoupling principle (2nd ed). American Mathematical Society. Notation: $\\ket{\\psi}^{\\otimes 2} \\equiv \\ket{\\psi}\\otimes \\ket{\\psi}$ $\\ket{v}\\otimes(\\ket{w_1}+\\ket{w_2}) = \\ket{v}\\otimes\\ket{w_1} + \\ket{v}\\otimes\\ket{w_2}$ Similarity Transformation $\\frac{1}{\\sqrt{2}}(\\ket{0}+\\ket{1}) = \\frac{1}{\\sqrt{2}}\\begin{bmatrix} 1 \\\\1 \\end{bmatrix} \\equiv\\ket{+}$ Johnston, N. (2021). Advanced Linear and Matrix Algebra . Springer International Publishing. Mermin, N. D. (2007). Quantum Computer Science: An Introduction . Cambridge University Press. Constructing a basis can be very non-trivial for infinite-dimensional vector spaces. (More on this on pp. 29â€“30, Johnston 2021) Special case: Measurement operator $M=\\sum_i\\lambda_i P_i$ Span Tensor products of (Hermitian, unitary, positive, projector) operators retain those properties. $P^2 =P$ Some properties of the adjoint operator: Recommended $\\rm{Tr}(A+B)=\\rm{Tr}(A)+\\rm{Tr}(B)$ A function $(,):\\mathcal{V} \\times \\mathcal{V} \\rightarrow \\mathbb{C}$. Some properties: When we represent a vector in some basis, we order the basis set so that the vector can be written in terms of its 'coordinates' $[\\ket{v}]_\\mathcal{B} \\equiv (c_1,\\cdots,c_n)$. The coordinates depend on the bases . A map between two vector spaces $\\mathcal{V}$ and $\\mathcal{W}$, $A: \\mathcal{V} \\rightarrow \\mathcal{W}$, that preserves linearity $A (c_1 \\ket{v_1} + c_2 \\ket{v_2}) = c_1 A \\ket{v_1} + c_2 A \\ket{v_2}$. Generally, it has a matrix representation given by $A\\ket{v_j}=\\sum_i A_{ij}\\ket{w_i}$. The representation depends on the basis. $c(d\\ket{v})=(cd)\\ket{v}$ All *finite* linear combinations of vectors of $\\mathcal{B} \\subseteq \\mathcal{V}$. $\\rm{span}(\\mathcal{B})$ is a subspace of $\\mathcal{V}$. If $\\rm{span}(\\mathcal{B})=\\mathcal{V}$ then $\\mathcal{V}$ is said to be spanned by $\\mathcal{B}$. For example, a vector spans a line and two non-parallel vectors spans $\\mathbb{R}^2$. Let $\\mathcal{V}$ be a set of elements (vectors) with two operations, addition and multiplication with a scalar from a field $\\mathbb{F}$ (typically $\\mathbb{C}$ in QM). $\\mathcal{V}$ is a called vector space if for vectors $\\ket{v},\\ket{w},\\ket{x} \\in \\mathcal{V}$ and scalars $c,d \\in \\mathbb{F}$, the following ten conditions are satisfied: Orthonormal Basis Set Example: Two non-parallel vectors on a plane are linearly independent, three of more of them on a plane are linearly dependent. Notice that an infinite set of vectors can be linearly independentâ€”you only need to show that there are not any finite linear combination of them such that all the scalars used in the combination are zeros. Think about the set $\\{1,x,x^2,\\cdots\\}$. Though the set contains infinitely many elements, it is linearly independent. Because if $\\sum_{i=0}^p c_i x^i=0$ then all $c_i$s are equal to 0. (set $x=0 \\implies c_0=0$; take derivatives and show $c_1=0$ and so on. See p. 17 of Johnston (2021)). So we cannot find any linear combination which is equal to 0 with at least one non-zero coefficient, implying that the set is not linearly dependent. So it's linearly independent! Closure under scalar multiplication: $c\\ket{v} \\in \\mathcal{V}$ Eigenvectors Interestingly, for any ket vector in $V$, there exists a bra vector in $V^*$; but for any bra vector, there may not exist a ket vector if the ket belongs to an infinite-dimensional Hilbert space (Cohen Tannoudji 2020 showed a counter-example). For finite-dimensional vector spaces, they are of the same size, and so you'll always get kets corresponding to bras, though. $\\frac{1}{\\sqrt{2}}(\\ket{0}+i\\ket{1}) = \\frac{1}{\\sqrt{2}}\\begin{bmatrix}1 \\\\i \\end{bmatrix} \\equiv \\ket{y_+}$ Gramâ€“Schmidt method is used to make a linearly independent set of vectors an orthonormal basis set. This is done by normalizing the first vector from the linearly independent set to make it the first basis element, and then progressively making the other vectors orthonormal by removing the projections. Wikipedia has a neat illustration, check it. Also Savov (2020) explained it well. Todo: Add a better synopsis later. $1\\ket{v}=\\ket{v}$ Example: the set of non-invertible $2 \\times 2$ matrices is *not* a subspace of $\\mathcal{M}_2$. $\\begin{smallmatrix}1 0 \\\\ 0 0\\end{smallmatrix}$ and $\\begin{smallmatrix} 0 0 \\\\ 0 1\\end{smallmatrix}$ are non-invertible but their sum $\\begin{smallmatrix}1 0 \\\\ 0 1\\end{smallmatrix}$ is invertible. Polar Decomposition Vector Space A basis of a vector space $\\mathcal{V}$ is a set of vectors in $\\mathcal{V}$ that spans it and is linearly independent. Example, $\\begin{smallmatrix} 1 \\\\ 0 \\end{smallmatrix}$ and $\\begin{smallmatrix} 0 \\\\1 \\end{smallmatrix}$ spans $\\mathbb{C}^2$. Why you may not want to read this page $P_i P_j = \\delta_{ij}P_i$ For a positive operator : $(\\ket{v},A\\ket{v})\\geq 0$. If it is positive, then $A$ is called positive definite. $A^\\dagger A$ is positive for any operator $A$. $z(\\ket{v}\\otimes\\ket{w})=(z\\ket{v})\\otimes\\ket{w}=\\ket{v}\\otimes(z\\ket{w})$ Linear Combination $\\rm{Tr}(zA)=z\\rm{Tr}(A)$ Any vector of the form $\\sum^k_{i=1} c_i \\ket{v_i}$ is called a linear combination. Interesting: The Identity matrix *cannot* be written as a linear combination of Pauli matrices. The number of vectors in a basis sets the dimension of the vector space. If $\\mathcal{V}$ has a basis with $n$ vectors, then any set of $m>n$ vectors will be linearly dependent and any set of $m < n$ vectors cannot span $\\mathcal{V}$. (p. 27, Johnston 2021) Interestingly, if the eigenvectors of $A: \\mathcal{V} \\rightarrow \\mathcal{V}$ have non-degenerate eigenvalues, then they form a basis (eigenbasis) $A = \\sum_i \\lambda_i \\ket{i}\\bra{i}$. Example: our favorite Hamiltonian operator and it's energy eigenvalues: $H=\\sum_E E\\ket{E}\\bra{E}$ $P^\\dagger=P$ A basis set whose elements are orthonormal $\\bra{i},\\ket{j}=\\delta_{ij}$ (like unit vectors). They are complete: $\\sum_i \\ket{i}\\bra{i} = I$. $(\\ket{v_1}+\\ket{v_2})\\otimes\\ket{w}=\\ket{v_1}\\otimes\\ket{w}+\\ket{v_2}\\otimes\\ket{w}$ First, a definition. The adjoint/Hermitian conjugate of an operator $A$ is the unique operator $A^\\dagger$ that satisfies the relation $(\\ket{v},A\\ket{w})=(A^\\dagger \\ket{v},\\ket{w})$ for any $\\ket{v}$ and $\\ket{w}$. Bra Vectors $\\rm{Tr}(AB)=\\rm{Tr}(BA)$ Commutativity: $\\ket{v} + \\ket{w} = \\ket{w} + \\ket{v}$ It obeys all properties of linear operators. Existence of an 'inverse': a vector $-\\ket{v}$ such that $\\ket{v}+(-\\ket{v})=\\ket{0}$"
  },
  {
    "title": "Sanskrit",
    "url": "https://arghyadutta.github.io/notebooks/sanskrit.html",
    "content": "Recommended Ruppel, A. M. The Cambridge Introduction to Sanskrit , 1st ed.; Cambridge University Press, 2017. https://doi.org/10.1017/9781107088283 ."
  },
  {
    "title": "Fountain Pen, Pencil, Paper",
    "url": "https://arghyadutta.github.io/notebooks/stationery.html",
    "content": "Fountain Pen Network Lamy 2000 And The Origins Of Lamy Design . (2012, August 11). The Fountain Pen Network In case you're serious about Lamy 2000. I have read (and spent) a bit too much on fountain pens, pencils, and paperâ€”in general, writing paraphernalia. Maybe someday I'll write about them, but not today. Today, I am busy filling out electronic forms, Word files, and Excel sheets that my workplace is asking for."
  },
  {
    "title": "Evaluating Clustering Performance",
    "url": "https://arghyadutta.github.io/notebooks/clustPerformance.html",
    "content": " InÂ [2]: from sklearn.metrics.cluster import pair_confusion_matrix from sklearn import metrics import numpy as np C = pair_confusion_matrix ([ 0 , 0 , 1 , 1 ], [ 0 , 0 , 1 , 2 ]) print ( C ) TN = C [ 0 , 0 ] FP = C [ 0 , 1 ] FN = C [ 1 , 0 ] TP = C [ 1 , 1 ] [[8 0]\n [2 2]] Pair confusion matrix Â¶ Two clusters can be compared using the pair-confusion Matrix. InÂ [1]: from sklearn import metrics labels_true = [ 0 , 0 , 0 , 1 , 1 , 1 ] labels_pred = [ 0 , 0 , 1 , 1 , 2 , 2 ] print ( metrics . homogeneity_score ( labels_true , labels_pred )) print ( metrics . completeness_score ( labels_true , labels_pred )) print ( metrics . v_measure_score ( labels_true , labels_pred , beta = 0.6 )) 0.6666666666666669\n0.420619835714305\n0.5467344787062375 https://github.com/Hoosier-Clusters/clusim of Gates et al. with their package For CHI and DBI formulas, check the referenced scikit doc page. from clusim.clustering import Clustering import clusim.sim as sim true_labels = [ 1 , 1 , 1 , 2 , 2 , 2 , 3 , 3 , 3 ] predicted_labels = [ 1 , 2 , 2 , 3 , 3 , 1 , 1 , 1 , 1 ] single_cluster_labels = [ 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 ] completely_fragmented_labels = [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 ] # Their Data is Differently Formatted. true_clustering = Clustering () . from_membership_list ( true_labels ) predicted_clustering = Clustering () . from_membership_list ( predicted_labels ) predicted_single_cluster = Clustering () . from_membership_list ( single_cluster_labels ) predicted_completely_fragmented = Clustering () . from_membership_list ( completely_fragmented_labels ) for _ in [ predicted_clustering , predicted_single_cluster , predicted_completely_fragmented , ]: print ( f \"FMI = { sim . fowlkes_mallows_index ( true_clustering , _ ) } , NMI = { sim . nmi ( true_clustering , _ ) } , elem-cent = { sim . element_sim ( true_clustering , _ ) } \" ) # The Package Can Compute Many Scores such As... (code from Their Documentation https://hoosier-clusters.github.io/clusim/html/clusim.html) row_format2 = \" {:>25} \" * ( 2 ) for simfunc in sim . available_similarity_measures : print ( row_format2 . format ( simfunc , eval ( \"sim.\" + simfunc + \"(true_clustering, predicted_clustering)\" ) ) ) Fowlkesâ€“Mallows Index Â¶ Building on the pair-confusion matrix, FMI, a measure for cluster similarity, is computed using the elements of $C$ as\n$$FMI = \\frac{TP}{\\sqrt{(TP + FP) (TP + FN)}}.$$ FMI is useful because it gives a numberâ€”and not a matrix like the pair confusion matrixâ€”for quickly comparing two clusterings. Element-centric similarity and issues with FMI and NMI Â¶ Gates et al. raise objections against using Fowlkesâ€“Mallows Index and NMI, specifically NMI; they proposed a new one. References: A. J. Gates et al. Element-Centric Clustering Comparison Unifies Overlaps and Hierarchy. Sci Rep 2019, 9 (1), 8574. DOI . A. J. Gates, Y.-Y. Ahn. CluSim: A Python Package for Calculating Clustering Similarity. Journal of Open Source Software 2019, 4 (35), 1264. DOI . https://github.com/Hoosier-Clusters/clusim of Gates et al. with their package Davies-Bouldin Index Â¶ 0 meaning overlapping clusters, and comparing how well the predicted clusters compare with the ground truth and V-Measure Â¶ Arghya Dutta Notebooks InÂ [3]: Two clusters can be compared using the pair-confusion Matrix. Mutual-information-based similarity score Â¶ InÂ [4]: InÂ [4]: from clusim.clustering import Clustering import clusim.sim as sim true_labels = [ 1 , 1 , 1 , 2 , 2 , 2 , 3 , 3 , 3 ] predicted_labels = [ 1 , 2 , 2 , 3 , 3 , 1 , 1 , 1 , 1 ] single_cluster_labels = [ 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 ] completely_fragmented_labels = [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 ] # Their Data is Differently Formatted. true_clustering = Clustering () . from_membership_list ( true_labels ) predicted_clustering = Clustering () . from_membership_list ( predicted_labels ) predicted_single_cluster = Clustering () . from_membership_list ( single_cluster_labels ) predicted_completely_fragmented = Clustering () . from_membership_list ( completely_fragmented_labels ) for _ in [ predicted_clustering , predicted_single_cluster , predicted_completely_fragmented , ]: print ( f \"FMI = { sim . fowlkes_mallows_index ( true_clustering , _ ) } , NMI = { sim . nmi ( true_clustering , _ ) } , elem-cent = { sim . element_sim ( true_clustering , _ ) } \" ) # The Package Can Compute Many Scores such As... (code from Their Documentation https://hoosier-clusters.github.io/clusim/html/clusim.html) row_format2 = \" {:>25} \" * ( 2 ) for simfunc in sim . available_similarity_measures : print ( row_format2 . format ( simfunc , eval ( \"sim.\" + simfunc + \"(true_clustering, predicted_clustering)\" ) ) ) FMI = 0.4811252243246881, NMI = 0.5451600159416435, elem-cent = 0.5407407407407406\nFMI = 0.5, NMI = 0.0, elem-cent = 0.33333333333333326\nFMI = 0.0, NMI = 0.6666666666666665, elem-cent = 0.33333333333333326\n            jaccard_index                   0.3125\n               rand_index       0.6944444444444444\n            adjrand_index      0.26666666666666655\n    fowlkes_mallows_index       0.4811252243246881\n                 fmeasure      0.47619047619047616\n             purity_index       0.7777777777777777\n     classification_error      0.22222222222222232\n        czekanowski_index      0.47619047619047616\n               dice_index      0.47619047619047616\n           sorensen_index      0.47619047619047616\n    rogers_tanimoto_index       0.5319148936170213\n          southwood_index      0.45454545454545453\n      pearson_correlation      0.00102880658436214\n         corrected_chance      0.16994265720286564\n      sample_expected_sim      0.10526315789473684\n                      nmi       0.5451600159416435\n                       mi       0.8233232815796736\n                   adj_mi       0.3410389011275906\n                      rmi       0.1464053299155769\n                       vi       1.3738364418444755\n       geometric_accuracy       0.7777777777777778\n          overlap_quality                     -0.0\n                     onmi       0.6303315236619905\n              omega_index      0.26666666666666655 InÂ [2]: FMI = TP / np . sqrt (( TP + FP ) * ( TP + FN )) print ( FMI ) # You Can also Compute FMI Using Scikit-learn. The Results Match, of Course. print ( metrics . fowlkes_mallows_score ([ 0 , 0 , 1 , 1 ], [ 0 , 0 , 1 , 2 ])) # Also Two Same Partitions Will Have no Off-diagonal Elements in the Pair Confusion Matrix and a FMI Score of 1. C = pair_confusion_matrix ([ 0 , 0 , 1 , 1 ], [ 0 , 0 , 1 , 1 ]) print ( C ) TN = C [ 0 , 0 ] FP = C [ 0 , 1 ] FN = C [ 1 , 0 ] TP = C [ 1 , 1 ] FMI = TP / np . sqrt (( TP + FP ) * ( TP + FN )) print ( FMI ) print ( metrics . fowlkes_mallows_score ([ 0 , 0 , 1 , 1 ], [ 0 , 0 , 1 , 1 ])) InÂ [2]: from sklearn.metrics.cluster import pair_confusion_matrix from sklearn import metrics import numpy as np C = pair_confusion_matrix ([ 0 , 0 , 1 , 1 ], [ 0 , 0 , 1 , 2 ]) print ( C ) TN = C [ 0 , 0 ] FP = C [ 0 , 1 ] FN = C [ 1 , 0 ] TP = C [ 1 , 1 ] $c = 1 - \\frac{H(K|C)}{H(K)}$ $n_{c,k}$: number of samples from class $c$ assigned to cluster $k$. $-1$ meaning incorrect clustering, $n_c$, $n_k$: number of samples in class $c$ and cluster $k$, respectively. Homogeneity Score Â¶ Scikit mentions a drawback: Higher $h$ means better clusters. $h$ takes values in $[0,1]$. Homogeneity Score Â¶ Homogeneity score ($h$) is defined as\n$$\n\\begin{align*}\n&h = 1 - \\frac{H(C|K)}{H(C)}\\;\\text{where}\\\\\n&H(C) = - \\sum_{c=1}^{|C|} \\frac{n_c}{n} \\cdot \\log\\left(\\frac{n_c}{n}\\right)\\\\\n&H(C|K) = - \\sum_{c=1}^{|C|} \\sum_{k=1}^{|K|} \\frac{n_{c,k}}{n}\\cdot \\log\\left(\\frac{n_{c,k}}{n_k}\\right)\n\\end{align*}\n$$ $n$: total number of samples. $n_c$, $n_k$: number of samples in class $c$ and cluster $k$, respectively. $n_{c,k}$: number of samples from class $c$ assigned to cluster $k$. Higher $h$ means better clusters. $h$ takes values in $[0,1]$. FMI = 0.4811252243246881, NMI = 0.5451600159416435, elem-cent = 0.5407407407407406\nFMI = 0.5, NMI = 0.0, elem-cent = 0.33333333333333326\nFMI = 0.0, NMI = 0.6666666666666665, elem-cent = 0.33333333333333326\n            jaccard_index                   0.3125\n               rand_index       0.6944444444444444\n            adjrand_index      0.26666666666666655\n    fowlkes_mallows_index       0.4811252243246881\n                 fmeasure      0.47619047619047616\n             purity_index       0.7777777777777777\n     classification_error      0.22222222222222232\n        czekanowski_index      0.47619047619047616\n               dice_index      0.47619047619047616\n           sorensen_index      0.47619047619047616\n    rogers_tanimoto_index       0.5319148936170213\n          southwood_index      0.45454545454545453\n      pearson_correlation      0.00102880658436214\n         corrected_chance      0.16994265720286564\n      sample_expected_sim      0.10526315789473684\n                      nmi       0.5451600159416435\n                       mi       0.8233232815796736\n                   adj_mi       0.3410389011275906\n                      rmi       0.1464053299155769\n                       vi       1.3738364418444755\n       geometric_accuracy       0.7777777777777778\n          overlap_quality                     -0.0\n                     onmi       0.6303315236619905\n              omega_index      0.26666666666666655 [[8 0]\n [2 2]] Gates et al. raise objections against using Fowlkesâ€“Mallows Index and NMI, specifically NMI; they proposed a new one. The Silhouette Coefficient is generally higher for convex clusters than other concepts of clusters, such as density based clusters like those obtained through DBSCAN. The second approach is the only option if no ground truths are available. $a$: mean distance between a sample all other points in the same cluster The harmonic mean of the homogeneity score and completeness scores: $v = 2 \\cdot \\frac{h \\cdot c}{h + c}$. $=0$ (random labeling) InÂ [3]: FMI = TP / np . sqrt (( TP + FP ) * ( TP + FN )) print ( FMI ) # You Can also Compute FMI Using Scikit-learn. The Results Match, of Course. print ( metrics . fowlkes_mallows_score ([ 0 , 0 , 1 , 1 ], [ 0 , 0 , 1 , 2 ])) # Also Two Same Partitions Will Have no Off-diagonal Elements in the Pair Confusion Matrix and a FMI Score of 1. C = pair_confusion_matrix ([ 0 , 0 , 1 , 1 ], [ 0 , 0 , 1 , 1 ]) print ( C ) TN = C [ 0 , 0 ] FP = C [ 0 , 1 ] FN = C [ 1 , 0 ] TP = C [ 1 , 1 ] FMI = TP / np . sqrt (( TP + FP ) * ( TP + FN )) print ( FMI ) print ( metrics . fowlkes_mallows_score ([ 0 , 0 , 1 , 1 ], [ 0 , 0 , 1 , 1 ])) Fowlkesâ€“Mallows Index Â¶ Mutual-information-based similarity score Â¶ Let's say, we have 2 sets of labels, $U$ and $V$, for $N$ objects. If $P(i)=|U_i|/N$ and $P'(j)=|V_i|/N$ are the probabilities that a randomly-picked object from $U$ ($V$) falls into class $U_i$ ($V_j$), then the entropies and the mutual information between $U$ and $V$ are computed as\n$$\n\\begin{aligned}\nH(U) &= - \\sum_{i=1}^{|U|}P(i)\\log(P(i))\\\\\nH(V) &= - \\sum_{j=1}^{|V|}P'(j)\\log(P'(j))\\\\\n\\text{MI}(U, V) &= \\sum_{i=1}^{|U|}\\sum_{j=1}^{|V|}P(i, j)\\log\\left(\\frac{P(i,j)}{P(i)P'(j)}\\right)\\\\\n&= \\sum_{i=1}^{|U|} \\sum_{j=1}^{|V|} \\frac{|U_i \\cap V_j|}{N}\\log\\left(\\frac{N|U_i \\cap V_j|}{|U_i||V_j|}\\right)\\\\\n\\text{NMI}(U, V) &= \\frac{\\text{MI}(U, V)}{\\text{mean}(H(U), H(V))}\\\\\n\\text{AMI} &= \\frac{\\text{MI} - E[\\text{MI}]}{\\text{mean}(H(U), H(V)) - E[\\text{MI}]}\n\\end{aligned}\n$$ AMI is adjusted against chance. NMI and MI are not. AMI $<0$: bad (i.e. independent labeling) $=0$ (random labeling) $\\simeq 1$ (agreement between the ground truth and predicted labels) References: C. D. Manning, P. Raghavan, H. SchÃ¼tze. Introduction to Information Retrieval; Cambridge University Press: New York, 2008. pp. 356â€“359 (Good discussion.) InÂ [1]: InÂ [4]: from clusim.clustering import Clustering import clusim.sim as sim true_labels = [ 1 , 1 , 1 , 2 , 2 , 2 , 3 , 3 , 3 ] predicted_labels = [ 1 , 2 , 2 , 3 , 3 , 1 , 1 , 1 , 1 ] single_cluster_labels = [ 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 ] completely_fragmented_labels = [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 ] # Their Data is Differently Formatted. true_clustering = Clustering () . from_membership_list ( true_labels ) predicted_clustering = Clustering () . from_membership_list ( predicted_labels ) predicted_single_cluster = Clustering () . from_membership_list ( single_cluster_labels ) predicted_completely_fragmented = Clustering () . from_membership_list ( completely_fragmented_labels ) for _ in [ predicted_clustering , predicted_single_cluster , predicted_completely_fragmented , ]: print ( f \"FMI = { sim . fowlkes_mallows_index ( true_clustering , _ ) } , NMI = { sim . nmi ( true_clustering , _ ) } , elem-cent = { sim . element_sim ( true_clustering , _ ) } \" ) # The Package Can Compute Many Scores such As... (code from Their Documentation https://hoosier-clusters.github.io/clusim/html/clusim.html) row_format2 = \" {:>25} \" * ( 2 ) for simfunc in sim . available_similarity_measures : print ( row_format2 . format ( simfunc , eval ( \"sim.\" + simfunc + \"(true_clustering, predicted_clustering)\" ) ) ) Evaluation of quality of clusters is often done in two ways: Calinski Harabasz Index Â¶ Completeness Score Â¶ Silhouette coefficient Â¶ For a single sample it is $$\ns = \\frac{b - a}{\\max(a, b)}.\n$$ $a$: mean distance between a sample all other points in the same cluster $b$: mean distance between a sample all other points in the next nearest cluster . $s$ takes values in $[-1,1]$ with $-1$ meaning incorrect clustering, 0 meaning overlapping clusters, and 1 meaning highly-dense, well-separated clusters. So, a higher silhouette score means better defined clusters. Important : Scikit returns the mean of all Silhouette coefficients of the samples. Scikit mentions a drawback: The Silhouette Coefficient is generally higher for convex clusters than other concepts of clusters, such as density based clusters like those obtained through DBSCAN. AMI $<0$: bad (i.e. independent labeling) $=0$ (random labeling) $\\simeq 1$ (agreement between the ground truth and predicted labels) 0.7071067811865475\n0.7071067811865476\n[[8 0]\n [0 4]]\n1.0\n1.0 InÂ [1]: from sklearn import metrics labels_true = [ 0 , 0 , 0 , 1 , 1 , 1 ] labels_pred = [ 0 , 0 , 1 , 1 , 2 , 2 ] print ( metrics . homogeneity_score ( labels_true , labels_pred )) print ( metrics . completeness_score ( labels_true , labels_pred )) print ( metrics . v_measure_score ( labels_true , labels_pred , beta = 0.6 )) checking if the generated clusters are consistent. 1 meaning highly-dense, well-separated clusters. Silhouette coefficient Â¶ Let's say, we have 2 sets of labels, $U$ and $V$, for $N$ objects. If $P(i)=|U_i|/N$ and $P'(j)=|V_i|/N$ are the probabilities that a randomly-picked object from $U$ ($V$) falls into class $U_i$ ($V_j$), then the entropies and the mutual information between $U$ and $V$ are computed as\n$$\n\\begin{aligned}\nH(U) &= - \\sum_{i=1}^{|U|}P(i)\\log(P(i))\\\\\nH(V) &= - \\sum_{j=1}^{|V|}P'(j)\\log(P'(j))\\\\\n\\text{MI}(U, V) &= \\sum_{i=1}^{|U|}\\sum_{j=1}^{|V|}P(i, j)\\log\\left(\\frac{P(i,j)}{P(i)P'(j)}\\right)\\\\\n&= \\sum_{i=1}^{|U|} \\sum_{j=1}^{|V|} \\frac{|U_i \\cap V_j|}{N}\\log\\left(\\frac{N|U_i \\cap V_j|}{|U_i||V_j|}\\right)\\\\\n\\text{NMI}(U, V) &= \\frac{\\text{MI}(U, V)}{\\text{mean}(H(U), H(V))}\\\\\n\\text{AMI} &= \\frac{\\text{MI} - E[\\text{MI}]}{\\text{mean}(H(U), H(V)) - E[\\text{MI}]}\n\\end{aligned}\n$$ $n$: total number of samples. Pair confusion matrix Â¶ Building on the pair-confusion matrix, FMI, a measure for cluster similarity, is computed using the elements of $C$ as\n$$FMI = \\frac{TP}{\\sqrt{(TP + FP) (TP + FN)}}.$$ There are several coefficients that quantify the quality of clustering. Some that do not need a ground truth are Silhouette coefficient, Calinskiâ€“Harabasz index, and Daviesâ€“Bouldin index. Others such as homogeneity Score, completeness score, and V Measure need a ground truth. from sklearn.metrics.cluster import pair_confusion_matrix from sklearn import metrics import numpy as np C = pair_confusion_matrix ([ 0 , 0 , 1 , 1 ], [ 0 , 0 , 1 , 2 ]) print ( C ) TN = C [ 0 , 0 ] FP = C [ 0 , 1 ] FN = C [ 1 , 0 ] TP = C [ 1 , 1 ] Notebooks from sklearn import metrics labels_true = [ 0 , 0 , 0 , 1 , 1 , 1 ] labels_pred = [ 0 , 0 , 1 , 1 , 2 , 2 ] print ( metrics . homogeneity_score ( labels_true , labels_pred )) print ( metrics . completeness_score ( labels_true , labels_pred )) print ( metrics . v_measure_score ( labels_true , labels_pred , beta = 0.6 )) Important : Scikit returns the mean of all Silhouette coefficients of the samples. V-Measure Â¶ The harmonic mean of the homogeneity score and completeness scores: $v = 2 \\cdot \\frac{h \\cdot c}{h + c}$. It's implemented in scikit as $v = \\frac{(1 + \\beta) \\times \\text{homogeneity} \\times \\text{completeness}}{(\\beta \\times \\text{homogeneity} + \\text{completeness})}$ Completeness Score Â¶ All members of a class belongs to the same cluster. $c = 1 - \\frac{H(K|C)}{H(K)}$ $c \\in[0,1]$. higher is better. All members of a class belongs to the same cluster. $c = 1 - \\frac{H(K|C)}{H(K)}$ $c \\in[0,1]$. higher is better. A. J. Gates, Y.-Y. Ahn. CluSim: A Python Package for Calculating Clustering Similarity. Journal of Open Source Software 2019, 4 (35), 1264. DOI . C. D. Manning, P. Raghavan, H. SchÃ¼tze. Introduction to Information Retrieval; Cambridge University Press: New York, 2008. pp. 356â€“359 (Good discussion.) Homogeneity score ($h$) is defined as\n$$\n\\begin{align*}\n&h = 1 - \\frac{H(C|K)}{H(C)}\\;\\text{where}\\\\\n&H(C) = - \\sum_{c=1}^{|C|} \\frac{n_c}{n} \\cdot \\log\\left(\\frac{n_c}{n}\\right)\\\\\n&H(C|K) = - \\sum_{c=1}^{|C|} \\sum_{k=1}^{|K|} \\frac{n_{c,k}}{n}\\cdot \\log\\left(\\frac{n_{c,k}}{n_k}\\right)\n\\end{align*}\n$$ $b$: mean distance between a sample all other points in the next nearest cluster . $s$ takes values in $[-1,1]$ with Calinski Harabasz Index Â¶ Calinskiâ€“Harabasz index (CHI) is the ratio of the sum of between-clusters dispersion for all clusters and sum of inter-cluster dispersion for all clusters. Better-defined clusters have higher CHI value. It's implemented in scikit as $v = \\frac{(1 + \\beta) \\times \\text{homogeneity} \\times \\text{completeness}}{(\\beta \\times \\text{homogeneity} + \\text{completeness})}$ Surprisingly, DB values closer to zero indicate a better partition , unlike Calinski-Harabasz index and Silhouette coefficient. InÂ [3]: FMI = TP / np . sqrt (( TP + FP ) * ( TP + FN )) print ( FMI ) # You Can also Compute FMI Using Scikit-learn. The Results Match, of Course. print ( metrics . fowlkes_mallows_score ([ 0 , 0 , 1 , 1 ], [ 0 , 0 , 1 , 2 ])) # Also Two Same Partitions Will Have no Off-diagonal Elements in the Pair Confusion Matrix and a FMI Score of 1. C = pair_confusion_matrix ([ 0 , 0 , 1 , 1 ], [ 0 , 0 , 1 , 1 ]) print ( C ) TN = C [ 0 , 0 ] FP = C [ 0 , 1 ] FN = C [ 1 , 0 ] TP = C [ 1 , 1 ] FMI = TP / np . sqrt (( TP + FP ) * ( TP + FN )) print ( FMI ) print ( metrics . fowlkes_mallows_score ([ 0 , 0 , 1 , 1 ], [ 0 , 0 , 1 , 1 ])) 0.7071067811865475\n0.7071067811865476\n[[8 0]\n [0 4]]\n1.0\n1.0 References: NOTE: Most of this notebook's content is adaptedâ€”often copied!â€”from scikit-learn's excellent documentation on clustering . This is a compilation of definitions and code-snippets that I found useful while working on a project, along with some of my own thoughts. Evaluating Clustering Performance NOTE: Most of this notebook's content is adaptedâ€”often copied!â€”from scikit-learn's excellent documentation on clustering . This is a compilation of definitions and code-snippets that I found useful while working on a project, along with some of my own thoughts. Evaluation of quality of clusters is often done in two ways: comparing how well the predicted clusters compare with the ground truth and checking if the generated clusters are consistent. The second approach is the only option if no ground truths are available. There are several coefficients that quantify the quality of clustering. Some that do not need a ground truth are Silhouette coefficient, Calinskiâ€“Harabasz index, and Daviesâ€“Bouldin index. Others such as homogeneity Score, completeness score, and V Measure need a ground truth. AMI is adjusted against chance. NMI and MI are not. Davies-Bouldin Index Â¶ Surprisingly, DB values closer to zero indicate a better partition , unlike Calinski-Harabasz index and Silhouette coefficient. For CHI and DBI formulas, check the referenced scikit doc page. For a single sample it is A. J. Gates et al. Element-Centric Clustering Comparison Unifies Overlaps and Hierarchy. Sci Rep 2019, 9 (1), 8574. DOI . Element-centric similarity and issues with FMI and NMI Â¶ $c \\in[0,1]$. higher is better. $\\simeq 1$ (agreement between the ground truth and predicted labels) 0.6666666666666669\n0.420619835714305\n0.5467344787062375 Calinskiâ€“Harabasz index (CHI) is the ratio of the sum of between-clusters dispersion for all clusters and sum of inter-cluster dispersion for all clusters. Better-defined clusters have higher CHI value. FMI is useful because it gives a numberâ€”and not a matrix like the pair confusion matrixâ€”for quickly comparing two clusterings. $<0$: bad (i.e. independent labeling) So, a higher silhouette score means better defined clusters. $$\ns = \\frac{b - a}{\\max(a, b)}.\n$$"
  },
  {
    "title": "Academic advice",
    "url": "https://arghyadutta.github.io/notebooks/academicAdvice.html",
    "content": "Guide to english communication for scientists by Nature How to do research Giving talks If you're just starting to write scientific prose, watch the series On Writing Susan McConnell (Stanford): Designing Effective Scientific Presentations ; 2011.  (accessed 2025-09-04). Advice on scientific writing by Hermut GrubmÃ¼ller See also Schimel, J. (2011). Writing Science: How to Write Papers That Get Cited and Proposals That Get Funded. Oxford University Press. Derek Dreyer; 2020. How To Write Papers So People Can Read Them (accessed 2025-09-04). Showcase of Scholarly Writing Practical, actionable advice. Writing in the Sciences by Kristin Sainani . Jhala, R. 2005. (An Opionionated Talk) On Preparing Good Talks (accessed 2025-09-04). Ian Baldwin (Max Planck Institute): Making Scientific Writing Painless ; 2017.  (accessed 2025-09-04). Academic writing Geroch, R. Suggestions For Giving Talks. arXiv:gr-qc/9703019 1997."
  },
  {
    "title": "Statistical Mechanics: Resources",
    "url": "https://arghyadutta.github.io/notebooks/statMechReadingMaterial.html",
    "content": "Thorough, excellent treatment of non-equilibrium stat mech. Boseâ€“Einstein Condensates Dalvit, D. A. R., Frastai, J., & Lawrie, I. D. (1999). Problems on statistical mechanics . Institute of Physics. Tolman, R. C. (1979). The principles of statistical mechanics . Dover Publications. To read Recommended Books Non-equilibrium Statistical Mechanics by V. S. Balakrishnan . Bayesian Statistics A thorough book andâ€”unusuallyâ€”starts with detailed discussions on entropy. The book uses slightly unwieldy, verbose notations, but I think explicit notations is a feature here, not a problem. Concise, but so well-written! Check, for example, the first chapter on thermodynamics (and especially Legendre transforms). Courses on YouTube Non-equilibrium Statistical Mechanics by V. S. Balakrishnan . Introductory lectures on statistical mechanics by John Preskill Complex Systems (related ideas to stat mech.) Excellent book on problems, more so because it comes with solutions. Check it if you want to know, for example, under which conditions bosons with a $p^s$ dispersion relation in $d$-dimensions will form a condensate. Krapivsky, P. L., Redner, S., & Ben-Naim, E. (2010). A Kinetic View of Statistical Physics (1st ed.). Cambridge University Press. Clean, accessible discussions on numerical approaches in statistical mechanics. For instance, check the discussion on how ensembles are used in molecular dynamics simulations. Chandler, D. (1987). Introduction to Modern Statistical Mechanics. OUP USA. Introductory lectures on statistical mechanics by John Preskill Broad yet concise. The chapter on the general properties of the partition function has a nice discussion on the zeroes of the partition function and connections to phase transition. Allen, M. P., & Tildesley, D. J. (2017). Computer simulation of liquids (Second edition). Oxford University Press. Baxter, R. J. (2008). Exactly Solved Models in Statistical Mechanics . Dover Publications. Nishimori, H., & Ortiz, G. (2011). Elements of phase transitions and critical phenomena . Oxford University Press. Balakrishnan, V. (2021). Elements of Nonequilibrium Statistical Mechanics. Springer International Publishing. I've only had a look at this book, but I'm thoroughly impressed by its depth. My favorite book regarding field-theoretic formulation of critical phenomena. The chapters on the perturbative renormalization group of $\\phi^4$ theories are superb. Ryogo Kubo's books are some of the most didactic books I've ever read. Self-Avoiding Random Walks Swendsen, R. H. (2019). An Introduction to Statistical Mechanics and Thermodynamics (2nd ed.). Oxford University Press. Kardar, M. (2007). Statistical Physics of Fields (1st ed.). Cambridge University Press. Hydrophobicity The chapters on aggregation, fragmentation, and adsorption are particularly good. The best resource for exactly solvable models. If you're trying to calculate higher-order Feynman diagrams for scalar field theories, Kleinert and Schulte-Frohlinde's book is the best reference . Good book with a spectacular beginning where he puts the partition function in the place it deserves. Ambegaokar, V. (1996). Reasoning about luck: Probability and its uses in physics . Cambridge University Press. (Archive) Kleinert, H., & Schulte-frohlinde, V. (2001). Critical Properties Of $\\phi^4$-Theories . World Scientific. Huang, K. (1987). Statistical mechanics (2nd ed). Wiley. Feynman, R. (1998). Statistical Mechanics: A Set Of Lectures . Taylor & Francis. It has a neat discussion on mean field theories (particularly $\\phi^6$ theory related to tricritical points). This is a list of advanced and specialized textbooks. For introductory books, see this list . Also see Toda, M., Kubo, R., & SaitÃ´, N. (1992). Statistical Physics I: Equilibrium Statistical Mechanics (Vol. 30). Springer Berlin Heidelberg. Kubo, R., Toda, M., & Hashitsume, N. (1991). Statistical Physics II (Vol. 31). Springer Berlin Heidelberg."
  },
  {
    "title": "Subgroup Discovery",
    "url": "https://arghyadutta.github.io/notebooks/subgroupDiscovery.html",
    "content": "Lopez-Martinez-Carrasco, A.; Juarez, J. M.; Campos, M.; Mora-Caselles, F. Subgroups: A Python Library for Subgroup Discovery. SoftwareX 2024 , 28 , 101895. https://doi.org/10.1016/j.softx.2024.101895 . Recommended Ghiringhelli, L. M.; Vybiral, J.; Levchenko, S. V.; Draxl, C.; Scheffler, M. Big Data of Materials Science: Critical Role of the Descriptor. Phys. Rev. Lett. 2015 , 114 (10), 105503. https://doi.org/10.1103/PhysRevLett.114.105503 . Atzmueller, M. Subgroup Discovery. WIREs Data Mining and Knowledge Discovery 2015 , 5 (1), 35â€“49. https://doi.org/10.1002/widm.1144 . Optimize: $f(Q)=\\textrm{cov}(Q)^\\gamma \\textrm{eff}(Q)_+$ where $Q=\\{i \\in S: \\sigma(i)= \\textrm{True}\\}$ (extension), $\\textrm{cov}(Q)=|Q|/|S|$ (coverage), $\\textrm{eff}(Q)= \\frac{H_y(S)-H_y(Q)}{H_y(S)}$ (effect), and $H_y(Q) = -\\sum_v p_Q(y=v) \\log p_Q(y=v)$ (entropy). Boley, M.; Goldsmith, B. R.; Ghiringhelli, L. M.; Vreeken, J. Identifying Consistent Statements about Numerical Data with Dispersion-Corrected Subgroup Discovery. Data Min Knowl Disc 2017 , 31 (5), 1391â€“1418. https://doi.org/10.1007/s10618-017-0520-3 . What's new in it then from clustering? My current understanding (need to check and update): Compared to global methods like decision tree, regression, or compressed sensing, SGD is a local method meaning it does not attempt to classify all the elements of the parent set into subsets, rather it tries to find subclasses which are of high quality with respect to the desired property. Goldsmith, B. R.; Boley, M.; Vreeken, J.; Scheffler, M.; Ghiringhelli, L. M. Uncovering Structure-Property Relationships of Materials by Subgroup Discovery. New J. Phys. 2017 , 19 (1), 013031. https://doi.org/10.1088/1367-2630/aa57c2 . Algorithm (tentative, need to check) Subgroup discovery (SGD) is a local knowledge discovery method. It identifies subsets in a set of elements that 'stands out' with respect to some property of the elements. Define: Propositions $Pi_x = {\\pi_1,\\cdots, \\pi_k}$, Selection language $\\mathcal{L}_x = {\\sigma(i)=\\pi_{j_1}(i)\\wedge\\cdots\\wedge \\pi_{j_t}(i)}$ Given: Sample $S \\subseteq P$, Target variable $y:P\\rightarrow {a, b, c, \\cdots}$, and Features $x_j: P\\rightarrow X_j$ It has been used to find out subgroup properties that contribute to one of the two crystal structures of 82 octet binaries (Ghiringhelli et al. 2015). SGD predicts two subgroups which contain elements that have either a zinc blend structure or a rock salt structure (Goldsmith et al 2017, Boley et al. 2017). For a review of the SGD, see Atzmueller (2015). Package New (~2024â€“25): GitHub Repo with Python implementation Also see Lopez-Martinez-Carrasco et al. (2024).  (I tried a Java implementation few years ago; it was useful but a bit clunky.)"
  },
  {
    "title": "Symbolic Regression",
    "url": "https://arghyadutta.github.io/notebooks/symbolicRegression.html",
    "content": "SISSO (Luca's group) and subsequent applications in material science. Genetic algorithm paper by Koza (1994) Schmidt and Lipson (2009). Note this paper has been criticized for not citing the literature (Crutchfield et al. 1987 and 1998). Recommended Timeline of papers Yes, that's me. :P Purcell, T. A. R., Scheffler, M., Carbogno, C., & Ghiringhelli, L. M. (2022). SISSO++: A C++ Implementation of the Sure-Independence Screening and Sparsifying Operator Approach . Journal of Open Source Software, 7(71), 3960 . Dutta, A., Vreeken, J., Ghiringhelli, L. M., & Bereau, T. (2021). Data-driven equation for drugâ€“membrane permeability across drugs and membranes. The Journal of Chemical Physics, 154(24), 244114. https://doi.org/10.1063/5.0053931 Crutchfield, J. P., & Young, K. (1989). Inferring statistical complexity . Physical Review Letters, 63(2), 105â€“108 . Schmidt, M., & Lipson, H. (2009). Distilling Free-Form Natural Laws from Experimental Data . Science, 324(5923), 81â€“85 . Ouyang, R., Ahmetcik, E., Carbogno, C., Scheffler, M., & Ghiringhelli, L. M. (2019). Simultaneous learning of several materials properties from incomplete databases with multi-task SISSO . Journal of Physics: Materials, 2(2), 024002 . AI Feynmann from Tegmark. Udrescu, S.-M., & Tegmark, M. (2020). AI Feynman: A Physics-Inspired Method for Symbolic Regression arXiv:1905.11481 . Ouyang, R., Curtarolo, S., Ahmetcik, E., Scheffler, M., & Ghiringhelli, L. M. (2018). SISSO: A compressed-sensing method for identifying the best low-dimensional descriptor in an immensity of offered candidates . Physical Review Materials, 2(8), 083802 . A method of systematically generating algebraic equations from data, with potential applications in discovering equations and even laws. The result, so far, has been quite promising, but no \"laws\" have been discovered, as yet. Crutchfield, J. P. (1987). Equations of Motion from a Data Series . Complex Systems, 1, 417â€“452. Koza, J. R. (1994). Genetic programming as a means for programming computers by natural selection . Statistics and Computing, 4(2), 87â€“112 ."
  },
  {
    "title": "Biophysics",
    "url": "https://arghyadutta.github.io/notebooks/biophysics.html",
    "content": "Courses on YouTube Books Protein physics by Finkelstein and Ptitsyn Ionic Solution Theory by H. Friedman and I. Prigogine Physical Biology of the Cell by Rob Phillips. Molecular Biophysics by Erik Lindahl. Introduction to Neuroscience by Bing Wen Brunton. Introduction to Proteomics . Biophysics by Walter Hoppe, Wolfgang Lohmann, Hubert Markl, Hubert Ziegler Molecular biophysics by M. V. Volkenshtein An introductory biophysics course by Ali Hassanali. Modelling dynamic phenomena in molecular and cellular biology by Lee Segel Theory of the stability of lyophobic colloids by Verwey and Overbeek Tutorials on PyMOL from Molecular Memory."
  },
  {
    "title": "Physics: Visualizations and Experiments",
    "url": "https://arghyadutta.github.io/notebooks/physicsExperiments.html",
    "content": "Flight manifest: from take-off to landing, a birdâ€™s eye introduction to flying Critical Point of carbon dioxide Quantum mechanics: Wave Particle Duality Cameras and Lenses A Nobel laureate and a flea circus join forces for an unforgettable demonstration of inertia Sublimation of iodine Quantum mechanics: Stern and Gerlach experiment Triple Point of Water Marangoni Bursting: Evaporation-Induced Emulsification of a Two-Component Droplet Brownian motion Melting of a cube of gold metal using the embedded atom method (eam) force field"
  },
  {
    "title": "Deep Learning",
    "url": "https://arghyadutta.github.io/notebooks/deepLearning.html",
    "content": "Leisurely paced, meticulous. Online Resources Deep Learning by Frank Noe ."
  },
  {
    "title": "Internet and Society",
    "url": "https://arghyadutta.github.io/notebooks/internetNUs.html",
    "content": "\"On the internet, a highly functional person is one who can promise everything to an indefinitely increasing audience at all times.\" Tolentino, J. (2019). The I in the Internet . Trick mirror: Reflections on self-delusion (First edition). Random House.\n        ( Free online version )"
  },
  {
    "title": "Statistical Mechanics: Few introductory books and papers",
    "url": "https://arghyadutta.github.io/notebooks/statMechCourse.html",
    "content": "Books Bhattacharjee, J. K. Entropy Ã  La Boltzmann . Resonance 2001, 6 (9), 19â€“34 . Baez, J. C. What Is Entropy? arXiv September 13, 2024 . BÃ¶ttcher, L.; Herrmann, H. J. (2021) Computational Statistical Physics , Cambridge University Press BÃ¶ttcher's and Kennett's are two beginner friendly, undergrad-level recent books covering computational and theoretical aspects of statistical mechanics, respectively. Some books and papers that I recommend in my statistical mechanics courses at undergraduate level. For more advanced references, see Statistical Mechanics: Resources . Kennett, M. P. (2020) Essential Statistical Physics , 1st ed.; Cambridge University Press Other reading materials Much more detailed than the ones listed so far. Bhattacharjee, S. M. Entropy and Perpetual Computers . arXiv October 29, 2003 . Entropy Reif, F. Statistical Physics: Berkeley Physics Course, Vol. 5 ; Mcgraw-Hill College, 1967. Styer, D. Entropy as Disorder: History of a Misconception . The Physics Teacher 2019, 57 (7), 454â€“458 ."
  },
  {
    "title": "Self-avoiding Random Walks",
    "url": "https://arghyadutta.github.io/notebooks/saw.html",
    "content": "$c_n$ is conjectured to behave as: $c_n \\approx A \\mu^n n^{\\gamma -1}$ where $\\mu$ is the connective constant (Sometimes it's called growth constant and $k=\\log \\mu$ is called connective constant). Nienhuis, B. (1982). Exact Critical Point and Critical Exponents of $\\mathrm{O}(n)$ Models in Two Dimensions . Physical Review Letters, 49(15), 1062â€“1065 . Lawler, G. F. (2011). Scaling limits and the Schramm-Loewner evolution . Probability Surveys, 8 . Clisby, N., & DÃ¼nweg, B. (2016). High-precision estimate of the hydrodynamic radius for self-avoiding walks . Physical Review E, 94(5), 052102 . The non-universal amplitudes $B$ and $C$ depend on lattice symmetry and dimension. Numerical Results There are no known method to calculate $c_n$ other than numerical enumeration and asymptotic analysis, as far as I know. The following table shows the length of largest SAWs that have been enumerated, with $c_n$ often becoming as large as $10^{30}$: $d$ $\\max(n)$ Symmetry Reference 2 71 Square Jensen (2004) 3 36 Simple cubic Lawler (2011) 3 28 BCC Schram (2017) 3 24 FCC Schram (2017) Asymptotic results (large $n$) Asymptotically, $\\langle R_\\mathrm{ g}^2\\rangle$ and $\\langle R_\\mathrm{ e}^2\\rangle$ are conjectured to behave as $\\langle R_\\mathrm{ e}^2\\rangle \\approx BN^{2\\nu}$ and $\\langle R_\\mathrm{ g}^2\\rangle \\approx CN^{2\\nu}$. The non-universal amplitudes $B$ and $C$ depend on lattice symmetry and dimension. $c_n$ is conjectured to behave as: $c_n \\approx A \\mu^n n^{\\gamma -1}$ where $\\mu$ is the connective constant (Sometimes it's called growth constant and $k=\\log \\mu$ is called connective constant). The non-universal amplitude $A$ depends on both the dimension and lattice symmetry. However, the $\\nu$ and $\\gamma$ only depend on the dimensionâ€”they are universal. $d$ $\\nu$ $\\gamma$ $c_n\\approx$ Comment Reference 2 $3/4$ $43/32 = 1.343 75$ $A \\mu^n n^{11/32}$ Exact Nienhuis (1982), Nienhuis (1984) 3 0.58759700(40)$\\approx 3/5$ $1.15695300(95) \\approx 7/6$ $A \\mu^n n^{1/6}$ Estimate Schram (2017), Slade (2019) (for summary) 4 $1/2$ with log correction $\\langle R_\\mathrm{ e}^2\\rangle \\approx B n(\\log n)^{1/4}$ $\\langle R_\\mathrm{ g}^2\\rangle \\approx C n(\\log n)^{1/4}$ 1 with log correction $A \\mu^n (\\log n)^{1/4}$ Upper critical dimension Slade (2019) $\\geq 5$ 1/2 1 $A \\mu^n$ Probably exact Slade (2019), p.4 Recommended Clisby, N., & DÃ¼nweg, B. (2016). High-precision estimate of the hydrodynamic radius for self-avoiding walks . Physical Review E, 94(5), 052102 . Jensen, I. (2004). Self-avoiding walks and polygons on the triangular lattice . Journal of Statistical Mechanics: Theory and Experiment, 2004(10), P10008 . Lawler, G. F. (2011). Scaling limits and the Schramm-Loewner evolution . Probability Surveys, 8 . Nienhuis, B. (1982). Exact Critical Point and Critical Exponents of $\\mathrm{O}(n)$ Models in Two Dimensions . Physical Review Letters, 49(15), 1062â€“1065 . Nienhuis, B. (1984). Critical behavior of two-dimensional spin models and charge asymmetry in the Coulomb gas . Journal of Statistical Physics, 34(5), 731â€“761 . Schram, R. D., Barkema, G. T., Bisseling, R. H., & Clisby, N. (2017). Exact enumeration of self-avoiding walks on BCC and FCC lattices . Journal of Statistical Mechanics: Theory and Experiment, 2017(8), 083208 . Slade, G. (2019). Self-avoiding walk, spin systems and renormalization . Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences, 475(2221), 20180549 . However, the $\\nu$ and $\\gamma$ only depend on the dimensionâ€”they are universal. There are no known method to calculate $c_n$ other than numerical enumeration and asymptotic analysis, as far as I know. The following table shows the length of largest SAWs that have been enumerated, with $c_n$ often becoming as large as $10^{30}$: Nienhuis, B. (1984). Critical behavior of two-dimensional spin models and charge asymmetry in the Coulomb gas . Journal of Statistical Physics, 34(5), 731â€“761 . The non-universal amplitude $A$ depends on both the dimension and lattice symmetry. Schram, R. D., Barkema, G. T., Bisseling, R. H., & Clisby, N. (2017). Exact enumeration of self-avoiding walks on BCC and FCC lattices . Journal of Statistical Mechanics: Theory and Experiment, 2017(8), 083208 . Asymptotically, $\\langle R_\\mathrm{ g}^2\\rangle$ and $\\langle R_\\mathrm{ e}^2\\rangle$ are conjectured to behave as $\\langle R_\\mathrm{ e}^2\\rangle \\approx BN^{2\\nu}$ and $\\langle R_\\mathrm{ g}^2\\rangle \\approx CN^{2\\nu}$. To get an idea about the size of $n$-step SAWs, one computes their values averaged over all $n$-step SAWs: Recommended Jensen, I. (2004). Self-avoiding walks and polygons on the triangular lattice . Journal of Statistical Mechanics: Theory and Experiment, 2004(10), P10008 . Let $c_n$ be the number and $R_e$ and $R_g$ be the end-to-end distance and radius of gyration of a $n$-step self-avoiding random walk with vertices at $\\omega_0,\\omega_2,\\cdots,\\omega_n$. Then the radius of gyration and end-to-end distance, two relevant measures quantifying the \"size\" the SAW, are defined as follows: Asymptotic results (large $n$) Asymptotically, $\\langle R_\\mathrm{ g}^2\\rangle$ and $\\langle R_\\mathrm{ e}^2\\rangle$ are conjectured to behave as $\\langle R_\\mathrm{ e}^2\\rangle \\approx BN^{2\\nu}$ and $\\langle R_\\mathrm{ g}^2\\rangle \\approx CN^{2\\nu}$. The non-universal amplitudes $B$ and $C$ depend on lattice symmetry and dimension. $c_n$ is conjectured to behave as: $c_n \\approx A \\mu^n n^{\\gamma -1}$ where $\\mu$ is the connective constant (Sometimes it's called growth constant and $k=\\log \\mu$ is called connective constant). The non-universal amplitude $A$ depends on both the dimension and lattice symmetry. However, the $\\nu$ and $\\gamma$ only depend on the dimensionâ€”they are universal. $d$ $\\nu$ $\\gamma$ $c_n\\approx$ Comment Reference 2 $3/4$ $43/32 = 1.343 75$ $A \\mu^n n^{11/32}$ Exact Nienhuis (1982), Nienhuis (1984) 3 0.58759700(40)$\\approx 3/5$ $1.15695300(95) \\approx 7/6$ $A \\mu^n n^{1/6}$ Estimate Schram (2017), Slade (2019) (for summary) 4 $1/2$ with log correction $\\langle R_\\mathrm{ e}^2\\rangle \\approx B n(\\log n)^{1/4}$ $\\langle R_\\mathrm{ g}^2\\rangle \\approx C n(\\log n)^{1/4}$ 1 with log correction $A \\mu^n (\\log n)^{1/4}$ Upper critical dimension Slade (2019) $\\geq 5$ 1/2 1 $A \\mu^n$ Probably exact Slade (2019), p.4 Recommended Clisby, N., & DÃ¼nweg, B. (2016). High-precision estimate of the hydrodynamic radius for self-avoiding walks . Physical Review E, 94(5), 052102 . Jensen, I. (2004). Self-avoiding walks and polygons on the triangular lattice . Journal of Statistical Mechanics: Theory and Experiment, 2004(10), P10008 . Lawler, G. F. (2011). Scaling limits and the Schramm-Loewner evolution . Probability Surveys, 8 . Nienhuis, B. (1982). Exact Critical Point and Critical Exponents of $\\mathrm{O}(n)$ Models in Two Dimensions . Physical Review Letters, 49(15), 1062â€“1065 . Nienhuis, B. (1984). Critical behavior of two-dimensional spin models and charge asymmetry in the Coulomb gas . Journal of Statistical Physics, 34(5), 731â€“761 . Schram, R. D., Barkema, G. T., Bisseling, R. H., & Clisby, N. (2017). Exact enumeration of self-avoiding walks on BCC and FCC lattices . Journal of Statistical Mechanics: Theory and Experiment, 2017(8), 083208 . Slade, G. (2019). Self-avoiding walk, spin systems and renormalization . Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences, 475(2221), 20180549 . Asymptotic results (large $n$) Slade, G. (2019). Self-avoiding walk, spin systems and renormalization . Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences, 475(2221), 20180549 ."
  },
  {
    "title": "Simpson's Paradox",
    "url": "https://arghyadutta.github.io/notebooks/simpsonsParadox.html",
    "content": "Are subgroup discovery and Simpson's paradox related? I believe they are, though I haven't fully developed the idea yet. My thinking is that computing a global regression line can be misleading when the data contain strong local patterns that are not aligned with each other. I don't think this is something new, but it would be interesting to explore the idea a bit further. Recommended Bookbinder, H. (2025, April 3). Simpsonâ€™s Paradox Explains the World Scriptorium Philosophia Pearl, J. (2016). Simpsonâ€™s Paradox: The riddle that would not die. Blog Post"
  },
  {
    "title": "Electromagnetism",
    "url": "https://arghyadutta.github.io/notebooks/electromagnetism.html",
    "content": "Recommended Purcell, E. M., & Morin, D. J. (2013). Electricity and Magnetism (Third edition). Cambridge University Press. Griffiths, D. J. (2023). Introduction to Electrodynamics (5th ed.). Cambridge University Press. Why Electrostatics Rules the Life of a Cell with Robijn Bruinsma; 2024. (accessed 2025-10-04). This book is on a league of it's own. The CUP edition is expensive, especially for Indian students. But note that it is far more carefully edited than the recent cheaper Berkeley series edition which claims to use the SI units. For a fun aside, see: Fahy, S.; Oâ€™Sullivan, C. All Magnetic Phenomena Are NOT Due to Electric Charges in Motion . Am. J. Phys. 2022, 90 (1), 7â€“8 . And: Griffiths, D. Reply to: All Magnetic Phenomena Are NOT Due to Electric Charges in Motion American Journal of Physics 2022, 90 (1), 9â€“9 ."
  },
  {
    "title": "Food and Drink",
    "url": "https://arghyadutta.github.io/notebooks/foodNDrink.html",
    "content": "14 Types Of Rice And The Best Dishes To Use Them from Food Republic A nice blog on Konkani recipes. The Complete Guide to Sugar Around the World by Craig Cavallo Lewis, J. Tell Me Why the Watermelon Grows. Switchyard Apples on a scale from most tart to most sweet Orwell, G. (1946) A Nice Cup of Tea . Evening Standard Few interesting videos and articles. How to make Chocolate \"I was once told by a grandmother, who lives in my seaside village but grew up in the northern dry regions where the Senegal River winds across a crispy and prickly savanna not far from the great desert, about a watermelon varietal called beref, which is cultivated mostly for its seeds but also serves as a kind of water reserve. â€œThereâ€™s water that you can drink from it like a coconut,â€ she said.\" \"Tenthly, one should pour tea into the cup first.\" And of course, the man had one more rule \"Lastly, teaâ€¦\". Advanced Coffee Making , Lecture at Assembly Coffee London"
  },
  {
    "title": "Title",
    "url": "https://arghyadutta.github.io/notebooks/template.html",
    "content": ""
  },
  {
    "title": "Quantum Mechanics via Moment Dynamics",
    "url": "https://arghyadutta.github.io/notebooks/dynamics.html",
    "content": "Recommended Neat. Also see other notes by Wheeler. (This is not John, but Nicholas Wheeler. A colleague of David Griffiths at the Reed college) Sarkar, P., Chattopadhyay, R., & Bhattacharjee, J. K. (2024). Quantum dynamics of wave packets in a Morse potential: A dynamical system approach . Physical Review E, 110(3), 034207 . Ballentine, L. E., & McRae, S. M. (1998). Moment equations for probability distributions in classical and quantum mechanics . Physical Review A, 58(3), 1799â€“1809 . Sarkar, P., & Bhattacharjee, J. K. (2020). Nonlinear parametric oscillator: A tool for probing quantum fluctuations . Physics Review E, 102(5), 052204 . Can we study a quintessential quantum phenomenonâ€”such as tunnelingâ€”by analyzing dynamical equations of the moments of the quantum distribution? Wheeler, N. (1998). Remarks Concerning the Status & Some Ramifications of Ehrenfestâ€™s Theorem . PDF Biswas, S., Chattopadhyay, R., & Bhattacharjee, J. K. (2018). Propagation of arbitrary initial wave packets in a quantum parametric oscillator: Instability zones for higher order moments . Physics Letters A, 382(18), 1202â€“1206 Ray, S., Bhattacharyya, S., & Bhattacharjee, J. K. (2024). Dynamical System Description of Quantum Tunneling in a Double Well Potential . Physics Letters A, 130174 . Chawla, R., & Bhattacharjee, J. K. (2019). Quantum dynamics from fixed points and their stability . European Physical Journal B, 92(9), 196 ."
  },
  {
    "title": "Indian history",
    "url": "https://arghyadutta.github.io/notebooks/india.html",
    "content": "Recommended More Candid, witty, and sometimes sarcastic, the book starts with a take on colonial India. It then moves into the authorâ€™s experiences working as a historianâ€”in the National Archives, at Delhi University, and later at Oxford. A great pick if you're looking for something that's both fun to read and full of insight into India and how an Indian sees Europe. There's also an abridged English translation called \"The World in Our Time: A Memoir\". Albinia, A. Empires of the Indus: The Story of a River ; John Murray: London, 2008. It focuses on the political history of Bengal since the time of Mahabharata and to the 1970s. Yes a bit too broad, but the author has done a good job in documenting the major political upheavals. I liked a nuanced discussion on the gradual deterioration of Hinduâ€“Muslim relations, starting 1900. Though, at times, the writing is dry and descriptive, it mostly reads well. One issue: a history book should have some decent maps and charts delineating the successions of kings; it lacks those. Sengupta, N. K. Land of Two Rivers: A History of Bengal from the Mahabharata to Mujib ; Penguin Books: New Delhi, 2011. More A well-researched and engaging read. The authorâ€™s sincerity comes throughâ€”she writes with empathy for the people living by the Indus, offering many thoughtful insights. I didnâ€™t always agree with her, especially in her somewhat cautious stance on religion, but overall, Iâ€™d recommend the book. Roychowdhury, T. Bangalnama | à¦¬à¦¾à¦™à¦¾à¦²à¦¨à¦¾à¦®à¦¾; Ananda Publishers. If I Get Time More It focuses on the political history of Bengal since the time of Mahabharata and to the 1970s. Yes a bit too broad, but the author has done a good job in documenting the major political upheavals. I liked a nuanced discussion on the gradual deterioration of Hinduâ€“Muslim relations, starting 1900. Though, at times, the writing is dry and descriptive, it mostly reads well. One issue: a history book should have some decent maps and charts delineating the successions of kings; it lacks those. Sastri, K. A. N.; Gurukkal, P. M. R.; Champakalakshmi, R. The Illustrated History of South India ; Oxford: New Delhi, 2009."
  },
  {
    "title": "Linear algebra",
    "url": "https://arghyadutta.github.io/notebooks/linearAlgebra.html",
    "content": "Singular Value Decomposition by Steven Brunton. Also see: Basic Mathematics for Quantum Mechanics Online resources The Matrix Cookbook by Kaare Brandt Petersen and Michael Syskind Pedersen"
  },
  {
    "title": "Excel keyboard shortcuts for Mac",
    "url": "https://arghyadutta.github.io/notebooks/excelShortcuts.html",
    "content": "Navigation Editing Aids Other Useful Shortcuts Editing & Entering I am not a fan of Excel, but also can't avoid it at work. So, I made this cheat-sheet of sorts using ChatGPT. I haven't tested most of them. I plan to use and edit it as needed. (Last updated: 2025-08-25, Monday) Selection Why you may not want to use this: It's ChatGPT-generated content! Formatting Function Key Shortcuts macOS vs Excel Conflicts"
  },
  {
    "title": "Software Tools",
    "url": "https://arghyadutta.github.io/notebooks/softwareTools.html",
    "content": "You open a webpage like Cosma Shalizi's (check it!) and find hundreds of links. Which one is active and which one is dead? Link analyzer does that job for you with one click. OmniDiskSweeper finds and helps delete large files. Nifty and Free Software For MacBook Tools The Zotero connector allows you to add papers to your Zotero Library from the browser. It can also automatically redirect journal webpages through an institutional proxy, allowing you to download papers with your institutional credentials. uBlock Origin is the ad blocker you should be using. Tt even lets you disable all javascript on a website, if you're adventurous. Addons for Mozilla Firefox HTML cleanup tool & simplifier AppCleaner helps to cleanly uninstall an app. Feedbro helps to get articles from any website that provides an RSS feed (blogs, journals, magazinesâ€¦). A good alternative to Feedly. Learn touch typing Find Journals' short names Amphetamine keeps your MacBook awake, which can be useful during presentations, for example. Rectangle moves and resizes windows. Hide Youtube-Shorts removes YouTube shorts from its homepage, subscriptions page, and search results. It can also hide the \"Shorts\" tab. Good riddance! webplotdigitizer - extract data from plots, images, and maps Tranquility Reader can strip down a webpage to main texts and images, letting you read it without 1000 flashy annoying things. Return YouTube dislikes brings back a useful feature. Gesturefy is such a neat add-on! It lets you do several things, like going back to the previous webpage, only with gestures of your mouse. Get unicode of symbols Tabliss can modify new tab page in the browser. Duplicate File Finder Remover finds (and removes) duplicate files. Single-file is a neat add-on that can download a webpage with everything, including figures, in a single HTML file. Very useful if you want to locally archive, for example, a blog post or a Twitter thread."
  },
  {
    "title": "Few well-written papers",
    "url": "https://arghyadutta.github.io/notebooks/wellWrittenPapers.html",
    "content": "Braberg, H.; Echeverria, I.; Kaake, R. M.; Sali, A.; Krogan, N. J. From Systems to Structure â€” Using Genetic Data to Model Protein Structures. Nat Rev Genet 2022, 23 (6), 342â€“354. link . Greener, Joe G., Shaun M. Kandathil, Lewis Moffat, and David T. Jones. â€œA Guide to Machine Learning for Biologists.â€ Nature Reviews Molecular Cell Biology 23, no. 1 (January 2022): 40â€“55. link . Well-written papers Neidhardt, F. C. Bacterial Growth: Constant Obsession with dN/Dt. J Bacteriol 1999, 181 (24), 7405â€“7408. link . Foley, S.; Deserno, M. Stabilizing Leaflet Asymmetry under Differential Stress in a Highly Coarse-Grained Lipid Membrane Model. J. Chem. Theory Comput. 2020, 16 (11), 7195â€“7206. link . Powers, A. S.; Pham, V.; Burger, W. A. C.; Thompson, G.; Laloudakis, Y.; Sexton, P. M.; Paul, S. M.; Christopoulos, A.; Thal, D. M.; Felder, C. C.; Valant, C.; Dror, R. O. Structural Basis of Efficacy-Driven Ligand Selectivity at GPCRs. Nat Chem Biol 2023, 1â€“10. link . Ghosh, A.; Radhakrishnan, J.; Chaikin, P. M.; Levine, D.; Ghosh, S. Coupled Dynamical Phase Transitions in Driven Disk Packings. Phys. Rev. Lett. 2022, 129 (18), 188002. link . I liked how concise and informative the abstract is! Marder, E. Grandmother Elephants. eLife 2013, 2, e01140. link . Bialek, W. Physical Limits to Sensation and Perception. Annu. Rev. Biophys. Biophys. Chem. 1987, 16 (1), 455â€“478. link . Cubuk, J.; Soranno, A. Macromolecular Crowding and Intrinsically Disordered Proteins: A Polymer Physics Perspective. ChemSystemsChem 2022. link . Guven, J.; Manrique, G. Arresting the Collapse of a Catenary Arch. arXiv:1710.03433 [cond-mat, physics:physics] 2017. Efron, B.; Halloran, E.; Holmes, S. Bootstrap Confidence Levels for Phylogeneticâ€‰Trees. Proceedings of the National Academy of Sciences 1996, 93 (23), 13429â€“13429. link . Overbeek, J. T. G.; Voorn, M. J. Phase Separation in Polyelectrolyte Solutions. Theory of Complex Coacervation. Journal of Cellular and Comparative Physiology 1957, 49 (S1), 7â€“26. link . Jia, X.; Lynch, A.; Huang, Y.; Danielson, M.; Langâ€™at, I.; Milder, A.; Ruby, A. E.; Wang, H.; Friedler, S. A.; Norquist, A. J.; Schrier, J. Anthropogenic Biases in Chemical Reaction Data Hinder Exploratory Inorganic Synthesis. Nature 2019, 573 (7773), 251â€“255. link . Deshpande, N. S.; Furbish, D. J.; Arratia, P. E.; Jerolmack, D. J. The Perpetual Fragility of Creeping Hillslopes. Nat Commun 2021, 12 (1), 3909. link . Galstyan, V.; Phillips, R. Allostery and Kinetic Proofreading. J. Phys. Chem. B 2019, 123 (51), 10990â€“11002. link . Cohen, A. E.; Shi, Z. Do Cell Membranes Flow Like Honey or Jiggle Like Jello? BioEssays 2020, 42 (1), 1900142. link . van de Schoot, R.; Depaoli, S.; King, R.; Kramer, B.; MÃ¤rtens, K.; Tadesse, M. G.; Vannucci, M.; Gelman, A.; Veen, D.; Willemsen, J.; Yau, C. Bayesian Statistics and Modelling. Nat Rev Methods Primers 2021, 1 (1), 1â€“26. link . FranÃ§a, T. F. A.; Monserrat, J. M. Writing Papers to Be Memorable, Even When They Are Not Really Read. BioEssays 2019, 41 (5), 1900035. link . Deserno, M. Fluid Lipid Membranes â€“ a Primer link Fjelland, R. Why General Artificial Intelligence Will Not Be Realized. Humanit Soc Sci Commun 2020, 7 (1), 10. link . Some papers that I liked for their prose; of course, the content is great, too! Luttinger, J. M. An Exactly Soluble Model of a Manyâ€Fermion System. Journal of Mathematical Physics 1963, 4 (9), 1154â€“1162. link . Mandelbrot, B. How Long Is the Coast of Britain? Statistical Self-Similarity and Fractional Dimension. Science 1967, 156 (3775), 636â€“638. link . Binder, K. Theory of First-Order Phase Transitions. Rep. Prog. Phys. 1987, 50 (7), 783. link . Everaers, R.; Sukumaran, S. K.; Grest, G. S.; Svaneborg, C.; Sivasubramanian, A.; Kremer, K. Rheology and Microscopic Topology of Entangled Polymeric Liquids. Science 2004, 303 (5659), 823â€“826. link . Bauer, M.; Bialek, W. Information Bottleneck in Molecular Sensing. PRX Life 2023, 1 (2), 023005. link . Thurston, W. P. On Proof and Progress in Mathematics. arXiv March 31, 1994. link . Sing, C. E.; Perry, S. L. Recent Progress in the Science of Complex Coacervation. Soft Matter 2020, 16 (12), 2885â€“2914. link . Tanaka, H.; Tong, H.; Shi, R.; Russo, J. Revealing Key Structural Features Hidden in Liquids and Glasses. Nat Rev Phys 2019, 1 (5), 333â€“348. link . Jo, M. H.; Meneses, P.; Yang, O.; Carcamo, C. C.; Pangeni, S.; Ha, T. Determination of Single-Molecule Loading Rate during Mechanotransduction in Cell Adhesion. Science 2024, 383 (6689), 1374â€“1379. link Park, J. J.; Lu, Y.-K.; Jamison, A. O.; Tscherbul, T. V.; Ketterle, W. A Feshbach Resonance in Collisions between Triplet Ground-State Molecules. Nature 2023, 614 (7946), 54â€“58. link . Another precise and well-written abstract!"
  },
  {
    "title": "Python programming",
    "url": "https://arghyadutta.github.io/notebooks/python.html",
    "content": "Recommended Ramalho, L. (2022). Fluent Python: Clear, Concise, and Effective Programming , Second Edition (Second edition). Oâ€™Reilly. A Visual Intro to NumPy and Data Representation by Jay Alammar McKinney, W. Python for Data Analysis NumPy Illustrated: The Visual Guide to NumPy by Lev Maximov McKinney, W. (2022). Python for Data Analysis: Data Wrangling with pandas, NumPy, and Jupyter (3rd edition). Oâ€™Reilly Media. Why Python is Slow: Looking Under the Hood by Jake Vanderplus 5 Good Python Habits; 2024. YouTube (accessed 2025-08-22)."
  },
  {
    "title": "Personal Knowledge Management",
    "url": "https://arghyadutta.github.io/notebooks/pkm.html",
    "content": "Ycombinator post on Managing my personal knowledge base Try to write in plain text format. Example: If you're writing a note on phase diagrams of proteins, connect it to existing notes, if any, on thermodynamic phases. Copy well-made images for your personal archive since they are useful, but donâ€™t ever forget to keep the attributions in your notes. Otherwise, you will forget the attribution and will risk plagiarism. Resources from others Think carefully about how the newfound piece of information connects to what you already know. Referencing: Use some marker, like `name-year-firstWord`, in the text and include a full bibliography at the end. You can also use pandoc to generate sorted bibliographies automatically; search for how to do it. A few thoughts on taking notes for your work or research: Related point : If you donâ€™t have a note yet, it can be tempting to start one, which can be useful or a waste of time depending on your needs. Since these are your personal notes , they need not be encyclopedic. Knowledge gaps in personal notes are fine given our limited time, but if the topic you skipped repeatedly comes up in your research, consider reading about it and writing a note. Think before you write a noteâ€”a noteâ€™s usefulness generally derives from the effort you put in. See how to take effective research notes . Donâ€™t copy text written by others; instead archive it in Zotero if you really want to keep a copy. Copying text for your personal notes defeats the purpose of taking notes: understanding and summarizing ideas for yourself . Weekly Review: Using Obsidian to Close Open Loops at the End of the Week . Managing project folders is tricky. Dan Larremore suggests making these folders in your project folder: raw-inputs, cleaned-inputs, code, output, figures, and writing. I think that's very useful. Also, keeping a README file in plain text format in your project folder and actively updating it is crucial."
  },
  {
    "title": "Probability and Statistics",
    "url": "https://arghyadutta.github.io/notebooks/statistics.html",
    "content": "A YouTube playlist with beginner friendly lectures by John Tsitsiklis and Patrick Jaillet from MIT. They also wrote a book on probability and made a summary of it freely available . Introduction to Probability, Statistics, and Random Processes by Hossein Pishro-Nik Recommended The Little Handbook of Statistical Practice by Gerard E. Dallal Beyond Multiple Linear Regression: Applied Generalized Linear Models and Multilevel Models in R by Paul Roback and Julie Legler A Userâ€™s Guide to Statistical Inference and Regression by Matthew Blackwell Undergraduate Probability and High-Dimensional Probability by Roman Vershynin, on YouTube. MIT RES.6-012 Introduction to Probability, Spring 2018 A placeholder note for both topics. Neat. Particularly nice discussion on the confidence interval. Get the sixth edition, especially if you're buying in India. It's excellent and relatively cheap (~INR. 800, August 2025). Also, this edition provides code examples in R, a welcome change. Holmes, S., & Huber, W. (2019). Modern statistics for modern biology . Cambridge university press. ( Free online copy ) Ross, S. (2021) Introduction to Probability and Statistics for Engineers and Scientists , Sixth edition.; Elsevier, 2021. The Book of Statistical Proofs by Joram Soch and collaborators (it's an open book on GitHub) Regression and Other Stories by Andrew Gelman, Jennifer Hill, Aki Vehtari"
  },
  {
    "title": "Germany",
    "url": "https://arghyadutta.github.io/notebooks/germany.html",
    "content": "Learning the language MacGregor, N. (2014). Germany: Memories of a Nation (1st edition). Penguin. German.net Second, the series provides a rare glimpse into the life of the common members of the Nazi party, Nazi party bosses and, also, ordinary German folks. How did the Germans react to the overbearing regime of the Nazi party? Were they happy under its rule? Did they share the murderous attitude of Nazi party towards \"subhuman(!)\" Polish and Slovakian people and, worst of them all, according to Nazi ideology, Jewish people? As expected, these questions do not have a binary answer and the author explored them with the help of many diaries (most notably, a jew'sâ€”Victor Klemperer'sâ€”and a Nazi party member's, Melita Maschmann's). Learning German: An Annotated Parallel Texts Reading List Highly recommended to all interested in the perilous journey undertaken by mankind during WW2. The Most Frequent German Words â€“ Deutsch 101-326 More on these Recounting the experience of individuals brings home, as nothing else can, the sheer complexity of the choices they had to make, and the difficult and often opaque nature of the situations they confronted. Contemporaries could not see things as clearly as we can, with the gift of hindsight: they could not know in 1930 what was to come in 1933, they could not know in 1933 what was to come in 1939 or 1942 or 1945. If they had known, doubtless the choices they made would have been different. One of the greatest problems in writing history is to imagine oneself back in the world of the past, with all the doubts and uncertainties people faced in dealing with a future that for the historian has also become the past. Developments that seem inevitable in retrospect were by no means so at the time, and in writing this book I have tried to remind the reader repeatedly that things could easily have turned out very differently to the way they did at a number of points in the history of Germany in the second half of the nineteenth century and the first half of the twentieth. People make their own history, as Karl Marx once memorably observed, but not under conditions of their own choosing. These conditions included not only the historical context in which they lived, but also the way in which they thought, the assumptions they acted upon, and the principles and beliefs that informed their behavior. A central aim of this book is to re-create all these things for a modern readership, and to remind readers that, to quote another well-known aphorism about history, 'the past is a foreign country: they do things differently there'. â€”Richard J. Evans A splendid set of books! The first volume deals with the coming in power of the Nazi party; the second volume is about the social, political and many other dimensions of German life under the Nazi rule and, finally, the third volume describes the expansion of the Third Reich and its subsequent downfall in 1945. I liked these books for a number of reasons: First, without obsessing over the brutality and sadism of individual Nazi party members, this series provides a complete and unnerving account of how the Nazis, after coming in power, \"won\" the support of German people. It portrays the contribution of Joseph Goebbels, the Reich Propaganda minister, in mobilizing the people to a war for the \"Living space\". Second, the series provides a rare glimpse into the life of the common members of the Nazi party, Nazi party bosses and, also, ordinary German folks. How did the Germans react to the overbearing regime of the Nazi party? Were they happy under its rule? Did they share the murderous attitude of Nazi party towards \"subhuman(!)\" Polish and Slovakian people and, worst of them all, according to Nazi ideology, Jewish people? As expected, these questions do not have a binary answer and the author explored them with the help of many diaries (most notably, a jew'sâ€”Victor Klemperer'sâ€”and a Nazi party member's, Melita Maschmann's). Finally, it's reassuring to learn about humane actions done by some Germans to help the oppressed and to observe that the Germans did not lose their sense of humor even under such oppressive regime, as evidenced by many jokes mentioned in the book. Though, let me be clear, this does not at all means that Evans has tried to defend the German atrocities committed during the war. Quite contrarily, he devoted almost 40% of the third volume detailing the massacre brought upon by the military and the SS on the civilian of the conquered countries. Highly recommended to all interested in the perilous journey undertaken by mankind during WW2. A splendid set of books! Finally, it's reassuring to learn about humane actions done by some Germans to help the oppressed and to observe that the Germans did not lose their sense of humor even under such oppressive regime, as evidenced by many jokes mentioned in the book. Though, let me be clear, this does not at all means that Evans has tried to defend the German atrocities committed during the war. Quite contrarily, he devoted almost 40% of the third volume detailing the massacre brought upon by the military and the SS on the civilian of the conquered countries. Grammar | DW Learn German A Guide to German Verbs for Beginners Reverso | Free Translation, Dictionary Bilingual Electronic Books â€“ Dual Language German/English â€“ Doppeltext 15 Great German Childrenâ€™s Books for Readers of All Ages | FluentU German Denglisch Dictionary: When Languages Collide The first volume deals with the coming in power of the Nazi party; the second volume is about the social, political and many other dimensions of German life under the Nazi rule and, finally, the third volume describes the expansion of the Third Reich and its subsequent downfall in 1945. I liked these books for a number of reasons: Dartmouth German German History in Documents and Images Recommended Evans, R. J. (2004). Coming Of The Third Reich . Penguin Evans, R. J. (2006). Third Reich in Power , 1933-1939. Penguin Evans, R. J. (2009). Third Reich at War , Penguin Textbooks - German First, without obsessing over the brutality and sadism of individual Nazi party members, this series provides a complete and unnerving account of how the Nazis, after coming in power, \"won\" the support of German people. It portrays the contribution of Joseph Goebbels, the Reich Propaganda minister, in mobilizing the people to a war for the \"Living space\"."
  },
  {
    "title": "Phase diagrams",
    "url": "https://arghyadutta.github.io/notebooks/phaseDiagram.html",
    "content": "Gasic, A. G.; Boob, M. M.; Prigozhin, M. B.; Homouz, D.; Daugherty, C. M.; Gruebele, M.; Cheung, M. S. Critical Phenomena in the Temperature-Pressure-Crowding Phase Diagram of a Protein . Phys. Rev. X 2019, 9 (4), 041035 . I came to know about protein's interesting phase diagrams from a YouTube course by Ali Hassanali (ICTP). For some examples, see Hawley (1971), Asherie (2004), and Gasic et al. (2019). Hawley, S. A. Reversible Pressure-Temperature Denaturation of Chymotrypsinogen . Biochemistry 1971, 10 (13), 2436â€“2442 . Intriguing and informative! Here is a rough drawing of a few. Asherie, N. Protein Crystallization and Phase Diagrams . Methods 2004, 34 (3), 266â€“272 . Protein $Pâ€“T$ phase diagrams"
  },
  {
    "title": "Philosophy of Science",
    "url": "https://arghyadutta.github.io/notebooks/philosophyOfScience.html",
    "content": "Courses on YouTube Recommended Humphreys, P. (Ed.). (2019). The Oxford Handbook of Philosophy of Science (Reprint edition). Oxford University Press. If I get time Good description of of the main school of thoughts, especially the theories of Popper (falsification) and Kuhn (paradigm shift). Alan, C. (2013). What Is This Thing Called Science? McGraw-Hill Education (UK). Philosophy of Science by Paul Hoyningen Karl Popper - Science: Conjectures and Refutations by Victor Gijsbers."
  },
  {
    "title": "Statistics with Python",
    "url": "https://arghyadutta.github.io/notebooks/statPython.html",
    "content": " count: number of observations in each bin probability or proportion: normalize such that bar heights sum to 1 density: normalize such that the total area of the histogram equals 1 Statistics with Python Histogram and KDE Â¶ Histogram and KDE for 1000 random integers from [0,8]. Make sure to mention what the y-axis denotes. In Seaborn, you can choose it via the stat parameter. From Seaborn documentation, stat has the following options [ws:seaborn] count: number of observations in each bin frequency: number of observations divided by the bin width probability or proportion: normalize such that bar heights sum to 1 percent: normalize such that bar heights sum to 100 density: normalize such that the total area of the histogram equals 1 See: Seaborn histplot documentation InÂ [2]: import pandas as pd import numpy as np from scipy import stats import matplotlib.pyplot as plt import seaborn as sns import statsmodels.api as sm # fix seed to get reproducible results # scipy.stats uses np.random to generate its random numbers, so setting seed using numpy works. (ref: https://stackoverflow.com/questions/16016959/scipy-stats-seed) np . random . seed ( seed = 23 ) nobs = 1000 # number of observations standard_normal = np . random . normal ( loc = 0.0 , scale = 1.0 , size = nobs ) shifted_normal = np . random . normal ( loc = 0.5 , scale = 1.0 , size = nobs ) plt . hist ( standard_normal , bins = 30 , histtype = \"step\" , label = \"standard normal\" ) plt . hist ( shifted_normal , bins = 30 , histtype = \"step\" , label = \"shifted normal\" ) plt . legend () # a quick way to fit a linear regression model using statmodels # ref: https://realpython.com/linear-regression-in-python/#advanced-linear-regression-with-statsmodels # simple linear regression # You need to add the column of ones to the inputs if you want statsmodels to calculate the intercept. It doesnâ€™t take the intercept into account by default. x = standard_normal X = sm . add_constant ( standard_normal ) # print(x[:5], X[:5]) # create a y axis to test linear regression with an error term y = 2.3 + 1.55 * x + 2.3 * np . random . exponential ( size = nobs ) # be sure to provide X, not x lin_reg = sm . OLS ( y , X ) . fit () print ( lin_reg . summary ()) # you can get the predicted values using lin_reg.fittedvalues as used in the plot below. you can also use the model to predict y for new x values. x_new = np . linspace ( - 3 , 3 , 5 ) X_new = sm . add_constant ( x_new ) y_new = lin_reg . predict ( X_new ) # print(x_new,y_new) fig , ax = plt . subplots ( 1 , 2 ) ax [ 0 ] . scatter ( x , y , c = \"gray\" ) ax [ 0 ] . plot ( x , lin_reg . fittedvalues , c = \"orange\" ) ax [ 0 ] . scatter ( x_new , y_new , c = \"crimson\" ) # seaborn automatically plots with confidence interval sns . regplot ( x = x , y = y , ax = ax [ 1 ]) # multiple linear regression X = np . array ( list ( zip ( standard_normal , shifted_normal ))) X # You need to add the column of ones to the inputs if you want statsmodels to calculate the intercept. It doesnâ€™t take intercept into account by default. X = sm . add_constant ( X ) X # create a y axis to test linear regression y = ( 2.3 + 1.55 * standard_normal + 2.4 * shifted_normal + 5 * np . sin ( np . random . random ( size = nobs )) ) lin_reg = sm . OLS ( y , X ) . fit () lin_reg . summary () # get statistical properties using scipy stats module stats . describe ( standard_normal ) stats . describe ( shifted_normal ) # checking if the distributions are normal (we know they are) stats . normaltest ( standard_normal ) stats . normaltest ( shifted_normal ) # KS-test stats . kstest ( standard_normal , stats . norm . cdf ) stats . kstest ( shifted_normal , stats . norm . cdf ) # two-tailed test # finds if the given random numbers are from same distribution stats . ttest_ind ( standard_normal , shifted_normal ) pass ; OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.262\nModel:                            OLS   Adj. R-squared:                  0.261\nMethod:                 Least Squares   F-statistic:                     354.1\nDate:                Thu, 04 Sep 2025   Prob (F-statistic):           7.79e-68\nTime:                        14:19:30   Log-Likelihood:                -2342.7\nNo. Observations:                1000   AIC:                             4689.\nDf Residuals:                     998   BIC:                             4699.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          4.5249      0.080     56.637      0.000       4.368       4.682\nx1             1.5599      0.083     18.816      0.000       1.397       1.723\n==============================================================================\nOmnibus:                      822.687   Durbin-Watson:                   1.998\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            27895.609\nSkew:                           3.536   Prob(JB):                         0.00\nKurtosis:                      27.890   Cond. No.                         1.08\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified. import pandas as pd import numpy as np from scipy import stats import matplotlib.pyplot as plt import seaborn as sns import statsmodels.api as sm # fix seed to get reproducible results # scipy.stats uses np.random to generate its random numbers, so setting seed using numpy works. (ref: https://stackoverflow.com/questions/16016959/scipy-stats-seed) np . random . seed ( seed = 23 ) nobs = 1000 # number of observations standard_normal = np . random . normal ( loc = 0.0 , scale = 1.0 , size = nobs ) shifted_normal = np . random . normal ( loc = 0.5 , scale = 1.0 , size = nobs ) plt . hist ( standard_normal , bins = 30 , histtype = \"step\" , label = \"standard normal\" ) plt . hist ( shifted_normal , bins = 30 , histtype = \"step\" , label = \"shifted normal\" ) plt . legend () # a quick way to fit a linear regression model using statmodels # ref: https://realpython.com/linear-regression-in-python/#advanced-linear-regression-with-statsmodels # simple linear regression # You need to add the column of ones to the inputs if you want statsmodels to calculate the intercept. It doesnâ€™t take the intercept into account by default. x = standard_normal X = sm . add_constant ( standard_normal ) # print(x[:5], X[:5]) # create a y axis to test linear regression with an error term y = 2.3 + 1.55 * x + 2.3 * np . random . exponential ( size = nobs ) # be sure to provide X, not x lin_reg = sm . OLS ( y , X ) . fit () print ( lin_reg . summary ()) # you can get the predicted values using lin_reg.fittedvalues as used in the plot below. you can also use the model to predict y for new x values. x_new = np . linspace ( - 3 , 3 , 5 ) X_new = sm . add_constant ( x_new ) y_new = lin_reg . predict ( X_new ) # print(x_new,y_new) fig , ax = plt . subplots ( 1 , 2 ) ax [ 0 ] . scatter ( x , y , c = \"gray\" ) ax [ 0 ] . plot ( x , lin_reg . fittedvalues , c = \"orange\" ) ax [ 0 ] . scatter ( x_new , y_new , c = \"crimson\" ) # seaborn automatically plots with confidence interval sns . regplot ( x = x , y = y , ax = ax [ 1 ]) # multiple linear regression X = np . array ( list ( zip ( standard_normal , shifted_normal ))) X # You need to add the column of ones to the inputs if you want statsmodels to calculate the intercept. It doesnâ€™t take intercept into account by default. X = sm . add_constant ( X ) X # create a y axis to test linear regression y = ( 2.3 + 1.55 * standard_normal + 2.4 * shifted_normal + 5 * np . sin ( np . random . random ( size = nobs )) ) lin_reg = sm . OLS ( y , X ) . fit () lin_reg . summary () # get statistical properties using scipy stats module stats . describe ( standard_normal ) stats . describe ( shifted_normal ) # checking if the distributions are normal (we know they are) stats . normaltest ( standard_normal ) stats . normaltest ( shifted_normal ) # KS-test stats . kstest ( standard_normal , stats . norm . cdf ) stats . kstest ( shifted_normal , stats . norm . cdf ) # two-tailed test # finds if the given random numbers are from same distribution stats . ttest_ind ( standard_normal , shifted_normal ) pass ; Make sure to mention what the y-axis denotes. InÂ [2]: import pandas as pd import numpy as np from scipy import stats import matplotlib.pyplot as plt import seaborn as sns import statsmodels.api as sm # fix seed to get reproducible results # scipy.stats uses np.random to generate its random numbers, so setting seed using numpy works. (ref: https://stackoverflow.com/questions/16016959/scipy-stats-seed) np . random . seed ( seed = 23 ) nobs = 1000 # number of observations standard_normal = np . random . normal ( loc = 0.0 , scale = 1.0 , size = nobs ) shifted_normal = np . random . normal ( loc = 0.5 , scale = 1.0 , size = nobs ) plt . hist ( standard_normal , bins = 30 , histtype = \"step\" , label = \"standard normal\" ) plt . hist ( shifted_normal , bins = 30 , histtype = \"step\" , label = \"shifted normal\" ) plt . legend () # a quick way to fit a linear regression model using statmodels # ref: https://realpython.com/linear-regression-in-python/#advanced-linear-regression-with-statsmodels # simple linear regression # You need to add the column of ones to the inputs if you want statsmodels to calculate the intercept. It doesnâ€™t take the intercept into account by default. x = standard_normal X = sm . add_constant ( standard_normal ) # print(x[:5], X[:5]) # create a y axis to test linear regression with an error term y = 2.3 + 1.55 * x + 2.3 * np . random . exponential ( size = nobs ) # be sure to provide X, not x lin_reg = sm . OLS ( y , X ) . fit () print ( lin_reg . summary ()) # you can get the predicted values using lin_reg.fittedvalues as used in the plot below. you can also use the model to predict y for new x values. x_new = np . linspace ( - 3 , 3 , 5 ) X_new = sm . add_constant ( x_new ) y_new = lin_reg . predict ( X_new ) # print(x_new,y_new) fig , ax = plt . subplots ( 1 , 2 ) ax [ 0 ] . scatter ( x , y , c = \"gray\" ) ax [ 0 ] . plot ( x , lin_reg . fittedvalues , c = \"orange\" ) ax [ 0 ] . scatter ( x_new , y_new , c = \"crimson\" ) # seaborn automatically plots with confidence interval sns . regplot ( x = x , y = y , ax = ax [ 1 ]) # multiple linear regression X = np . array ( list ( zip ( standard_normal , shifted_normal ))) X # You need to add the column of ones to the inputs if you want statsmodels to calculate the intercept. It doesnâ€™t take intercept into account by default. X = sm . add_constant ( X ) X # create a y axis to test linear regression y = ( 2.3 + 1.55 * standard_normal + 2.4 * shifted_normal + 5 * np . sin ( np . random . random ( size = nobs )) ) lin_reg = sm . OLS ( y , X ) . fit () lin_reg . summary () # get statistical properties using scipy stats module stats . describe ( standard_normal ) stats . describe ( shifted_normal ) # checking if the distributions are normal (we know they are) stats . normaltest ( standard_normal ) stats . normaltest ( shifted_normal ) # KS-test stats . kstest ( standard_normal , stats . norm . cdf ) stats . kstest ( shifted_normal , stats . norm . cdf ) # two-tailed test # finds if the given random numbers are from same distribution stats . ttest_ind ( standard_normal , shifted_normal ) pass ; See: Seaborn histplot documentation InÂ [1]: import numpy as np import matplotlib.pyplot as plt import seaborn as sns fig , ax = plt . subplots ( 1 , 5 , figsize = ( 10 , 2 ), constrained_layout = True ) x = np . random . randint ( 0 , 9 , 1000 ) properties = { \"kde\" : True , \"bins\" : 9 } sns . histplot ( x , ax = ax [ 0 ], stat = \"count\" , ** properties ) sns . histplot ( x , ax = ax [ 1 ], stat = \"frequency\" , ** properties ) sns . histplot ( x , ax = ax [ 2 ], stat = \"probability\" , ** properties ) sns . histplot ( x , ax = ax [ 3 ], stat = \"percent\" , ** properties ) sns . histplot ( x , ax = ax [ 4 ], stat = \"density\" , ** properties ) pass ; Linear Regression in Python Â¶ OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.262\nModel:                            OLS   Adj. R-squared:                  0.261\nMethod:                 Least Squares   F-statistic:                     354.1\nDate:                Thu, 04 Sep 2025   Prob (F-statistic):           7.79e-68\nTime:                        14:19:30   Log-Likelihood:                -2342.7\nNo. Observations:                1000   AIC:                             4689.\nDf Residuals:                     998   BIC:                             4699.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          4.5249      0.080     56.637      0.000       4.368       4.682\nx1             1.5599      0.083     18.816      0.000       1.397       1.723\n==============================================================================\nOmnibus:                      822.687   Durbin-Watson:                   1.998\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            27895.609\nSkew:                           3.536   Prob(JB):                         0.00\nKurtosis:                      27.890   Cond. No.                         1.08\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified. Histogram and KDE Â¶ Histogram and KDE for 1000 random integers from [0,8]. InÂ [1]: frequency: number of observations divided by the bin width In Seaborn, you can choose it via the stat parameter. From Seaborn documentation, stat has the following options [ws:seaborn] import numpy as np import matplotlib.pyplot as plt import seaborn as sns fig , ax = plt . subplots ( 1 , 5 , figsize = ( 10 , 2 ), constrained_layout = True ) x = np . random . randint ( 0 , 9 , 1000 ) properties = { \"kde\" : True , \"bins\" : 9 } sns . histplot ( x , ax = ax [ 0 ], stat = \"count\" , ** properties ) sns . histplot ( x , ax = ax [ 1 ], stat = \"frequency\" , ** properties ) sns . histplot ( x , ax = ax [ 2 ], stat = \"probability\" , ** properties ) sns . histplot ( x , ax = ax [ 3 ], stat = \"percent\" , ** properties ) sns . histplot ( x , ax = ax [ 4 ], stat = \"density\" , ** properties ) pass ; InÂ [2]: percent: normalize such that bar heights sum to 100"
  },
  {
    "title": "LaTeX",
    "url": "https://arghyadutta.github.io/notebooks/latex.html",
    "content": "Recommended Detexify Draw a symbol to get its LaTeX command Create LaTeX tables from data Overleaf's short tutorial is useful. Online BibTex tidy A no-frills LaTeX format Cleaning up BibTex files"
  },
  {
    "title": "Genetic Algorithm",
    "url": "https://arghyadutta.github.io/notebooks/geneticAlgorithm.html",
    "content": "Recommended GA is a type of evolutionary computing. It imitates biological evolution to find the `fittest' solution to an optimization problem. So, it is another optimization algorithm like gradient descent or exhaustive search. The fitness is determined by a fitness function . Kim, C., Batra, R., Chen, L., Tran, H., & Ramprasad, R. (2021). Polymer design using genetic algorithm and machine learning . Computational Materials Science, 186, 110067 . One practical example is shown in Kim et al. 2021 who designed new polymers starting from a set of parent polymers with the goal of achieving high glass-transition temperature $T_\\text{g}$ (to ensure mechanical stability at high temperatures) and high band-gap $E_\\text{g}$ (to ensure protection from dielectric breakdown). They proposed 192 new polymers that may have that function based on designing new polymers using GA and then evaluating the $T_\\text{g}$ and $E_\\text{g}$ of those from pre-trained ML models. Carr, J. (2014). An Introduction to Genetic Algorithms . PDF Procedure: It defines a population of chromosomes, which are candidate solutions to the optimization problem). It then does the following operations on the initial set of parents: selection, crossover, mutation, elicitation. (pp. 2â€“4, Carr 2014)"
  },
  {
    "title": "Artificial Intelligence",
    "url": "https://arghyadutta.github.io/notebooks/ai.html",
    "content": "A neat and well-argued metaphor. The Limits of AI by Hubert Dreyfus (1985) Burnett, D. G. (2025, April 26). Will the Humanities Survive Artificial Intelligence? The New Yorker . Crockett, MJ. (2025, February 27). AI is â€˜beatingâ€™ humans at empathy and creativity. But these games are rigged . The Guardian . Chomsky, N., Roberts, I., & Watumull, J. (2023, March 8). The False Promise of ChatGPT . The New York Times . Provocative but thoughtful, as usual from Chomsky. Pei Wang's A Gentle Introduction to AGI . Chayka, K. (2025, April 2). The Limits of A.I.-Generated Miyazaki . The New Yorker . Geoffrey Hinton's profile by the New Yorker. Useful read, despite the almost click-bait title. MÃ¼ller, V. C. (2023). Ethics of Artificial Intelligence and Robotics . In E. N. Zalta & U. Nodelman (Eds.), The Stanford Encyclopedia of Philosophy Broad discussion with well-chosen pointers for further reading. Excellent and up-to-date broad overview of AI. Dreyfus lays down the core objections against AGI. Fjelland, R. (2020). Why general artificial intelligence will not be realized . Humanities and Social Sciences Communications, 7(1), 10 . Rothman, J. (2023, November 13). Why the Godfather of A.I. Fears What Heâ€™s Built . The New Yorker . Artificial General Intelligence (AGI) Chiang, T. (2023, February 9). ChatGPT Is a Blurry JPEG of the Web . The New Yorker . Compelling and persuasive article against AGI, often elaborating on Dreyfus's ideas. Pins down issues with the methods and performance metrics. Rothman, J. (2025, April 29). Why Even Try if You Have A.I.? The New Yorker . Heitzinger, C., & Woltran, S. (2024). A Short Introduction to Artificial Intelligence: Methods, Success Stories, and Current Limitations . Introduction to Digital Humanism: A Textbook (pp. 135â€“149). Springer Nature Switzerland. Partial list, reflecting my annoyance with the ongoing hype about AGI. Issues of AI LLMs affectedâ€”destroyedâ€”the learning process of young students in a way I wouldn't have believed was possibleâ€”until I began teaching. Rothman offers an alternative approach."
  },
  {
    "title": "R Programming",
    "url": "https://arghyadutta.github.io/notebooks/rProgramming.html",
    "content": "If I get time Long, J., & Teetor, P. (2019). R Cookbook: Proven Recipes for Data Analysis, Statistics, and Graphics (2nd ed. edition). Oâ€™Reilly Media."
  },
  {
    "title": "Silence",
    "url": "https://arghyadutta.github.io/notebooks/silence.html",
    "content": "On a Still Morning Nan Shepherd Recommended IntoGreatSilence (Director). (2007, February 21). Official Into Great Silence US Trailer YouTube . An astounding documentary! From the description: \"Nestled deep in the postcard-perfect French Alps, the Grande Chartreuse is considered one of the world's most ascetic monasteries. In 1984, German filmmaker Philip GrÃ¶ning wrote to the Carthusian order for permission to make a documentary about them. They said they would get back to him. Sixteen years later, they were ready. GrÃ¶ning, sans crew or artificial lighting, lived in the monks' quarters for six monthsâ€”filming their daily prayers, tasks, rituals and rare outdoor excursions. This transcendent, closely observed film seeks to embody a monastery, rather than simply depict oneâ€”it has no score, no voiceover and no archival footage. What remains is stunningly elemental: time, space and light. One of the most mesmerizing and poetic chronicles of spirituality ever created, INTO GREAT SILENCE dissolves the border between screen and audience with a total immersion into the hush of monastic life. More meditation than documentary, it's a rare, transformative theatrical experience for all.\" As white light gathers all â€“\nThe rose and the amethyst, The ice-green and the copper-green, The peacock blue and the mist â€“ So if I bend my ear To silence, I grown aware The stir of sounds I have almost heard That are not quite there. If I Get Time I hear the silence now, Alive within its heart Are the sounds that can not be heard That the ear may not dispart? Shepherd, N. (2008). The living mountain: A celebration of the Cairngorm Mountains of Scotland . Edinburgh: Canongate. ( Archive ) Pyrah, S. (2025, February 2). Quiet, please! The remarkable power of silence â€“ for our bodies and our minds . The Guardian . Robert Gershon (Director). (2022, March 29). And They Kept Silence YouTube . Shepherd, N., & Macfarlane, R. (2019). In the Cairngorms (UK ed. edition). Galileo Publishers. Bold Books and Bones (Director). (2022, April 23). What do you hear when you find silence? YouTube ."
  },
  {
    "title": "Art",
    "url": "https://arghyadutta.github.io/notebooks/art.html",
    "content": "Jeannie Lynn Paske KÃ¤the Kollwitz Some of his works Norman, D. (2013). The Design of Everyday Things (2nd edition). Basic Books. JÃ³zef WilkoÅ„ Surreal, slow. Gregory Fromenteau Nan Goldin Design Bentley, T. (2017, July 15). The Obsessive Art and Great Confession of Charlotte Salomon . The New Yorker . How to design thingsâ€”doors, software interfaces, teapots, switchesâ€”so that people find them delightful to use? Charlotte Salomon I haven't seen anything quite like Paske's Surreal, melancholic, otherworldly art. Yuki Kawae Ernst Barlach Mangla, R. (2015, June 8). A Brief History of Ultramarineâ€”The Worldâ€™s Costliest Color . The Paris Review . Geometric patterns on sand. Meditative, ephemeral. The Drawing Database-Northern Kentucky University (Director). (2020, August 18). Art History & Drawing: 15 Minutes with Kathe Kollwitz YouTube Xuan Loc Xuan Wonderful. LearnFree (Director). (2016, October 5). Beginning Graphic Design: Color . YouTube Favorite artists Modern vietnamese artist.I love the way she uses colors and patterns. Jamie Windsor. (2019, October 29). Wabi-Sabi: When Bad Photos Are Better YouTube Colors Victoria and Albert Museum (Director). (2018, June 6). In Search of Forgotten Coloursâ€”Sachio Yoshioka and the Art of Natural Dyeing YouTube"
  },
  {
    "title": "Teaching",
    "url": "https://arghyadutta.github.io/notebooks/teaching.html",
    "content": "Recommended Hoare, E. (2025). Gentleness in Academia . Plough . \"Your accomplishments are NOT what make you a worthy human being. You learn this lesson by receiving GRACE: good things you didn't earn or deserve, but you're getting them anyway.\" Su, F. (2013). The Lesson of Grace in Teaching . His blog (2013)"
  },
  {
    "title": "Blogs",
    "url": "https://arghyadutta.github.io/notebooks/blogs.html",
    "content": "Blogging Almost Sure. A random mathematical blog Separated by a Common Language Scatterings Simon Willison's weblog Condensed concepts nanoscale views Writing Science AI: A Guide for Thinking Humans Blogs I Read The Last Word On Nothing off the convex path And then. Academics and beyond Academic Garden Ahead of AI Machine Learning Research Blog Probably overthinking it Statistical Modeling, Causal Inference, and Social Science \"We humans are such collecting creatures. We love creating sets of things, hoarding what we know or what we have, maybe arranging it all carefully for display.\" Forms of life, forms of mind The 20% Statistician Blogs as Modern Commonplace Books, and the Pleasures Thereof; Ana Ulin; Her blog (2018)"
  },
  {
    "title": "Combinatorics",
    "url": "https://arghyadutta.github.io/notebooks/combinatorics.html",
    "content": "If I Get Time Graham, R. L., Knuth, D. E., & Patashnik, O. (2017). Concrete mathematics: A foundation for computer science (2. ed., 31. print). Addison-Wesley. Bona, M. (2011). Walk Through Combinatorics, A: An Introduction To Enumeration And Graph Theory."
  },
  {
    "title": "Irrationality and Stupidity",
    "url": "https://arghyadutta.github.io/notebooks/stupidity.html",
    "content": "Bonhoeffer's Theory of Stupidity Recommended Frankfurt, H. G. (2005). On Bullshit (1st edition). Princeton University Press. If I Get Time"
  },
  {
    "title": "Philosophy: Online Resources",
    "url": "https://arghyadutta.github.io/notebooks/philosophyResources.html",
    "content": "Open Greek and Latin Perseus Digital Library archive.org Fadedpage Canada's project Gutenberg. Surprisingly rich collection. Digital Library of Darshan Manisha (Texts on Indian Philosophy) Two rather obvious entries, but the more I explore them, the more I get pleasantly surprised. They are true treasure troves. Project Gutenberg Classic books (from the U.S. Library of Congress)"
  },
  {
    "title": "Academic Life: Features",
    "url": "https://arghyadutta.github.io/notebooks/academiaFeatures.html",
    "content": "Recommended Mermin, N. D. (1990). Commencement address at St. John's College, Santa Fe in Boojums all the way through: Communicating science in a prosaic age. Cambridge University Press. Thomas, K. (2010). Working Methods . London Review of Books Gadagkar, R. (2012). The Luxury of Introspection . In Uber das Kolleg hinaus (pp.152â€“157). Wissenschaftskolleg zu Berlin, Germany. PDF . Marder, E. (2013). Grandmother elephants . eLife, 2, e01140 . In stead of discussing how society is systematically underminingâ€”and mockingâ€”any vestige of scholarly life that's still there, let me just point to some articles and books that show why scholarship is still a worthy vocation. On how scientific methods help us to make sense of the world. Delightful essay on a scholar and his notes. Hitz, Z. (2020). Lost in Thought: The Hidden Pleasures of an Intellectual Life (First Edition). Princeton University Press. Hitz, Z. (2023, September 1). What Is Time For? Plough On learning from the older generation of scientists. Marder is a brilliant scientist and a perceptive writerâ€”read her work."
  },
  {
    "title": "Quantum Information",
    "url": "https://arghyadutta.github.io/notebooks/qInfo.html",
    "content": "Quantum Computation Lecture Notes (Caltech CS219) by John Preskill Recommended See also Basic Mathematics for Quantum Mechanics"
  },
  {
    "title": "High-performance Computing",
    "url": "https://arghyadutta.github.io/notebooks/hpc.html",
    "content": "Courses on YouTube C++ by iamcanadian Recommended A short series on asymptotic notation and analysis of algorithms by David Scot Taylor . Check his YouTube channel, Algorithms with Attitude, for more stuff. Improving and optimizing Python by Sebastian MathÃ´t Introduction to SLURM Message-passing programming with MPI Pacheco, P. S. (2011). An introduction to parallel programming . Morgan Kaufmann. Pointers in C and C++ Intro to parallel programming HPC by Matthew Jacob from IISc Introduction to parallel Programming in Open MP NPTEL course by Yogish Sabharwal from IIT Delhi Useful if you're coming from a python background and not C/C++ or Fortran. Zaccone, G. (2019). Python parallel programming cookbook."
  },
  {
    "title": "Ehrenfest Theorem",
    "url": "https://arghyadutta.github.io/notebooks/ehrenfest.html",
    "content": "Wheeler, N. Remarks Concerning the Status & Some Ramifications of Ehrenfestâ€™s Theorem, 1998. PDF (accessed 2024-10-03). Recommended A. Messiah. Quantum Mechanics: Two Volumes Bound as One; Dover Publications, Inc: Garden City, New York, 2020. Another surprisingly thorough note from Nicholas Wheeler from Reed College (Yes, Griffiths is from Reed, too). Chapters 5, 6 discusses commutators and Ehrenfest's equation."
  },
  {
    "title": "Boseâ€“Einstein Condensates",
    "url": "https://arghyadutta.github.io/notebooks/bec.html",
    "content": "Proukakis, N. P. A Century of Boseâ€“Einstein Condensation . Commun Phys 2025, 8 (1), 264 . Recommended Schwartz, M. Lecture 12: Bose-Einstein Condensation , 2019. PDF"
  },
  {
    "title": "Visualization",
    "url": "https://arghyadutta.github.io/notebooks/visualization.html",
    "content": "Fundamentals of Data Visualization I wrote few simple codes to create decent plots using python. ggcoverage BioRender Inkscape explained by Logos by Nick Gosling cmasher SciDraw cpt-city: An archive of color gradients Muth, L. C. Your Friendly Guide to Colors in Data Visualisation . 2018, Her Blog Software Tools LearnFree (Director). (2016, October 5). Beginning Graphic Design: Color [Video recording] . Plotly Gephi HiPlot Veusz 2D HiGlass Got this link from Hacker News when I posted a Nature Communication paper The misuse of colour in science communication there.\n        This webpage contains color gradient files, and importing them in matplotlib is possible using packages like getcpt-master . Servier Medical ART Useful Python package. Contains some new colormaps; I don't like them that much but they are interesting. Information is beautiful Inkscape tutorial for beginners (really good) Recommended If I get time Canva Fundamentals of Data Visualization by Claus O. Wilke Palettable WashU Epigenome Browser Useful Python package. Contains an extensive set of diverging, sequential, and qualitative colormaps. Very useful. Inkscape tutorials by TJ Free Websites for drawing flowcharts etc. from text Data to Viz The Elements of Visual Grammar: A Designer's Guide for Writers, Scholars, and Professionals by Angela Riechers Figma"
  },
  {
    "title": "Poetry",
    "url": "https://arghyadutta.github.io/notebooks/poetry.html",
    "content": "Du Fu The Naming of Cats T S Eliot Few poems. Like this one An abandoned courtyard: an old tree: A temple bell lying on its side: The world I live in. They win and we lose; we lose and they win. Vines wrap around the rotting bones. She knows he wonâ€™t come back from the army, but patches the clothes he left just in case. Or this (I feel this) I am about to scream madly in the office, Especially when they bring more papers to pile higher on my desk. I mean, wow! In my beginning is my end. In succession Houses rise and fall, crumble, are extended, Are removed, destroyed, restored, or in their place Is an open field, or a factory, or a by-pass. Old stone to new building, old timber to new fires, Old fires to ashes, and ashes to the earth Which is already flesh, fur and faeces, Bone of man and beast, cornstalk and leaf. Houses live and die: there is a time for building And a time for living and for generation And a time for the wind to break the loosened pane And to shake the wainscot where the field-mouse trots And to shake the tattered arras woven with a silent motto. East Coker Langston Hughes Ted Kooser I, Too Ella Wheeler Wilcox The Year Bank Fishing for Bluegills Father"
  },
  {
    "title": "C. S. Lewis",
    "url": "https://arghyadutta.github.io/notebooks/cSLewis.html",
    "content": "Recommended Lewis on looking at and looking along. Thoughtful piece. C. S Lewis, Meditation in a Toolshed , The Coventry Evening Telegraph (1945) ."
  },
  {
    "title": "Advaita Vedanta",
    "url": "https://arghyadutta.github.io/notebooks/advaitaVedanta.html",
    "content": "Hindi translation Accurate, concise english translations of Shankaracharya's commentaries on eight of the principal upanishads: Isa, Kena, Katha, Taittariya (vol. 1) and Aitareya, Mundaka, Mandukya (with Karika), Prasna (vol. 2). Bramhasutra Shankaracharya (1956). Bramhasutra Bhasya (Sw. Gambhirananda, Trans.). Advaita Ashrama. Vishwarupananda, S. (1997). Vedanta Darshan (4 volumes, in Bengali). Udbodhan Karyalaya, Kolkata. Gita Shrimad Bhagwat Gita (ShankarBhashya Hindi Anuvad Sahit) (2022). Gita Press, Gorakhpur. Swami, Gambhirananda (1984). Bhagavad Gita: With the commentary of Shankaracharya . Advaita Ashrama. English translation Shankaracharya's commentaries, along with his hymns and other writings, are now made available online in original (Sanskrit) by Shringeri math. Hindi translation of the commentaries of Shankaracharya of eleven principal upanishads. Excellent, nuanced translations. Accurate english translation of Shankaracharya's commentary. 1) Isshadi Nau Upnishad Shankar Bhashya Sahit 2) Chandogya Upanishad 3) Brihadaranyak Upanishad Gita Press, Gorakhpur Upanishads A complete, heavily annotated bengali translation of the Shankaracharya's commentary of the Bramha Sutra. The footnotes are often useful, particularly to clarify doubts regarding Mimansa and Nyaya. 1) Shankaracharya (1957). Eight Upanisads Vol 1 (S. Gambhirananda, Trans.). Advaita Ashrama. 2) Shankaracharya (1957). Eight Upanisads Vol 2 (S. Gambhirananda, Trans.). Advaita Ashrama."
  },
  {
    "title": "Engineering Physics",
    "url": "https://arghyadutta.github.io/notebooks/enggPhysics.html",
    "content": "Two wonderful books. And all the accompanying lectures are available for free (the books are transcripts of what Shankar taught in the class.) I was aware of these two books, but I thank my colleague Supravat Dey for strongly recommending them! Additional readings Recommended resources Shankar, R. Fundamentals of Physics: Mechanics, Relativity, and Thermodynamics; Open Yale courses series; Yale University Press: New Haven, 2014. Shankar, R. Fundamentals of Physics. II: Electromagnetism, Optics, and Quantum Mechanics; The Open Yale courses series; Yale University Press: New Haven, 2016. Serway, R. A.; Jewett, J. W.; Peroomian, VahÃ©. Physics for Scientists and Engineers , Tenth edition.; Cengage: Boston, MA, USA, 2019. Balakrishnan, V. How Is a Vector Rotated? Resonance 1999, 4 (10), 61â€“68 . Excellent visualizations of many physical phenomena. A few references for the Engineering Physics course (FIC-102) that I teach at SRM University-AP."
  },
  {
    "title": "Melancholia and Grief",
    "url": "https://arghyadutta.github.io/notebooks/melancholia.html",
    "content": "Polk, E. (2024), Peregrinations of grief , Aeon . Recommended \"Whether due to a downsizing, divorce, or death, an estate sale is a sort of liminal space â€“ a passing of the tools and accumulated flotsam of a life onto descendants who cannot always bear the added weight and so jettison it.\" \"Everybody should know what it is to have friends like these. Everybody should know what it is to be loved like this.\" Astonishingly beautiful and tender recollection of a friend. Father by Ted Kooser Shenoda, S. (2025). It's Just Stuff What Estate Sales Reveal About Us . Plough Crosley, S. (2024), The Tail End, What we lose when we lose a pet , The New Yorker . Read it Sometimes you have to take your own hand as though you were a lost child and bring yourself stumbling home over twisted ice. Whiteness drifts over your house. A page of warm light falls steady from the open door. Here is your bed, folded open. Lie down, lie down, let the blue snow cover you. (from \"Original Fire: Selected and New Poems\", Harper Perennial, 2004) \"People often ask me: â€œWhy rabbits?â€ Usually my answer is just â€œTheyâ€™re floofy.â€ Heartfelt reminiscence. Grief , Louise Erdrich Smith, N. (2024), Why rabbits? Towards a better, floofier world , Noahpinion"
  },
  {
    "title": "Learnability in Machine Learning",
    "url": "https://arghyadutta.github.io/notebooks/learnability.html",
    "content": "Target function : $f:\\mathcal{X}\\rightarrow\\mathcal{Y}$ (ideal credit approval formula) Output : $y$ (good/bad customer) Learning algorithm: $\\mathcal{A}$ (e.g. back-propagation for neural network.) It does the searching and produces $g$. Online Resources We don't know $f$, we can only guess what it is from the data. The learning algorithm picks $g\\simeq f$ from the hypothesis set $\\mathcal{H}$. Especially useful for its introductory discussion on learnability and VC dimension. Machine learning course by Yaser Abu-Mostafa. Data : $(\\mathbf{x}_1, y_1),(\\mathbf{x}_2, y_2),\\cdots,(\\mathbf{x}_N, y_N)$ (historical records of credit customers) Hypothesis : $g:\\mathcal{X}\\rightarrow\\mathcal{Y}$, $g \\in \\mathcal{H}$. We hope that $g$ approximates $f$ well, that is the goal of learning. The hypothesis set and the learning algorithm $(\\mathcal{H}, \\mathcal{A})$ together are known as the learning model . Hypothesis set : $\\mathcal{H}={h}$. It plays a pivotal role. It can be a linear regression, a neural network, a support vector machineâ€¦ Input : $\\mathbf{x}$ (customer application)"
  },
  {
    "title": "Quantum mechanics: Books and Online Resources",
    "url": "https://arghyadutta.github.io/notebooks/qMech.html",
    "content": "It's very much worth watching despite the poor recording quality. It's at Griffiths's level and comes with several detailed examples. Though not as popular, it does provide a solid introduction. FlÃ¼gge, S. (1999). Practical quantum mechanics . Springer. A completely different sort of book which teaches QM through a series of problems. Bohm, D. (1989). Quantum Theory . Dover Publications. Townsend, J. S. (2012). A modern approach to quantum mechanics (2nd ed). University Science Books. An easier version of Sakurai's book. (And there exists a cheaper Indian edition.) For the path integral formalism of QM, the first few chapters of Ashok Das's book provide a pedagogic introduction. From YouTube Gasiorowicz, S. (1974). Quantum physics . New York, Wiley. Das, A. (2006). Field Theory: A Path Integral Approach (2nd ed., Vol. 75). World Scientific. A good undergrad-level textbook. The chapter on solutions to time-independent SchrÃ¶dinger equation is neat. I love this book! The chapters on rotational invariance, addition of angular momenta, scattering theory, and path integral are particularly good. Shankar, R. (1994). Principles of quantum mechanics (2nd ed). Plenum Press. One of the best graduate-level QM book. Precise and concise. Lectures on Quantum Computing . Sakurai, J. J., & Napolitano, J. (2021). Modern quantum mechanics (3rd ed). Cambridge University Press. Introduction to Quantum Information Science by Artur Ekert. Ekert is a patient teacher. The accompanying free book is useful, too. Recommended I have discovered this gem surprisingly late. Bohm lucidly discussed several topics which are hard to find in other books (one example would be statistical properties of correlations for classical and quantum systems, see chapter 10!) Quantum entanglement by Mark Wilde Griffiths, D. J., & Schroeter, D. F. (2018). Introduction to quantum mechanics (Third edition). Cambridge University Press."
  },
  {
    "title": "On Writing (and Some Examples)",
    "url": "https://arghyadutta.github.io/notebooks/writing.html",
    "content": "Common Errors in English Usage Gaitskill M. (2022) The deracination of literature . Unheard . Writing Styleâ€”An Online Guide YouTube playlists Mukherjee, S. (2023). All the Carcinogens We Cannot See . The New Yorker . Purdue Online Writing Lab Kolln, M. J., & Gray, L. S. (2015). Rhetorical Grammar: Grammatical Choices, Rhetorical Effects . Pearson Education. Ed Yong shows that one can write a good essay only when one deeply cares about the topic. Wolchover, N. (2020). What Is a Particle? Quanta . Orwell, G. (1946). Politics and the English Language; George Orwell . Horizon . Why read fiction? And does good writing matter? McPhee J. (2015) The Art of Omission . The New Yorker . Excellent! See, for example, the one on tense and aspect . Yong, E. (2021). America Is Not Ready for Omicron . The Atlantic . The Writer's workshop Smith, Z. (2023). On Killing Charles Dickens . The New Yorker . Neelakantan, A. (2021). Write because it makes you think or feel. the Record . Zadie Smith triesâ€”and failsâ€”to ignore Dickens while writing a nineteenth-century historical fiction. Some well-written essays Grammar and Stylistics: Writing Clearly by Randall Eggert from University of Utah. Good introduction to English grammar and writing beyond simple definitions. Newport, C. (2024). One Reason Hybrid Work Makes Employees Miserable. And how to fix it. The Atlantic . Internet resources Wolchover has an uncanny ability to phrase difficult ideas in popular language; this essay demonstrates that once again. Recommendations and resources on writing Possibly the most famous essay on NYCâ€”for good reasons. Recommended If I get time Orwell gives five rules for good writing and adds a sixth: \"Break any of these rules sooner than say anything outright barbarous.\" Writing is emotional: it's hard to remove unnecessary sentences that you've written. McPhees shares few personal stories. Williams, J. M. (2000). Style: Ten lessons in clarity and grace (6. ed). Longman. Punctuation guide White, E. B. (1949). Here is New York . Reprinted in Essays of E. B. White (2016) ."
  },
  {
    "title": "Logic",
    "url": "https://arghyadutta.github.io/notebooks/logic.html",
    "content": "Recommended Herbert, E. A Mathematical Introduction to Logic , 2nd edition.; Elsevier India, 2014 If I Get Time Almossawi, A. An Illustrated Book of Bad Arguments , Illustrated edition. Experiment, 2014. ( Available online ) Halbach, V. The Logic Manual ; Oxford University Press: Oxford; New York, 2010. Weston, A. A Rulebook for Arguments , Fifth edition.; Hackett Publishing Company, Inc: Indianapolis; Cambridge, 2017."
  },
  {
    "title": "Mathematical Methods in Physics: Resources",
    "url": "https://arghyadutta.github.io/notebooks/mathematicalMethods.html",
    "content": "Affine transformations Recommended Visualizing prime factors Short, insightful notes by Markus Deserno Index to Catalogue of Lattices The World of Mathematical Equations Garg, A. (2023) Mathematics with a Scientific Sensibility ; Northwestern University. Special functions Excellent book and the author has made it freely available ! Scholarpedia Visual group theory Balakrishnan, V. (2020). Mathematical Physics: Applications and Problems . Springer International Publishing."
  },
  {
    "title": "Dictionary",
    "url": "https://arghyadutta.github.io/notebooks/dictionary.html",
    "content": "An ode to Websterâ€™s 1913 dictionary . Excellent! Recommended Auburn, D. et al. (2012). Oxford American Writerâ€™s Thesaurus (3rd edition). Oxford University Press Inc. Surprisingly good. Mautner, T. (Ed.). (2005). Dictionary Of Philosophy (2nd edition). Penguin UK. If I Get Time Cuddon, J. A., & Habib, M. A. R. (2015). Dictionary of Literary Terms & Literary (5th edition). Penguin UK. Editors of the A. H. (2018). The American Heritage Dictionary Of The English Language , Fifth Edition: Fiftieth Anniversary Printing (Indexed edition). Collins Reference. I love dictionaries! Usage notes, fights between descriptivists and prescriptivists, illustrations, portraitsâ€”what more could you ask for in a dictionary? Medawar, P. B., & Medawar, J. S. (1985). Aristotle to Zoos: A Philosophical Dictionary of Biology . Oxford University Press. Dictionary of Untranslatables: A Philosophical Lexicon ; Cassin, B., Rendall, S., Apter, E. S., Eds.; Translation/Transnation Ser; Princeton University Press: Princeton, 2014. Langenscheidt Basic German Vocabulary. Hauptbd. A Learnerâ€™s Dictionary Divided into Subject Categories with Example Sentences ; Langenscheidt: Berlin, 2010. Tschirner, E.; MÃ¶hring, J. A Frequency Dictionary of German . Somers, J. (2014). Youâ€™re probably using the wrong dictionary His blog"
  },
  {
    "title": "Academic Biographies",
    "url": "https://arghyadutta.github.io/notebooks/biography.html",
    "content": "Hoad, P. (2024). â€˜He was in mystic deliriumâ€™: was this hermit mathematician a forgotten genius whose ideas could transform AI â€“ or a lonely madman? . Phil Hoad; Guardian Story of an unusual mathematician, Grigory Perelman. One of the best profiles that I've read in The New Yorker. Frisch, O. R. (1979). What little I remember . Cambridge University Press. Archive A short and delightful book by Pierre-Gilles de Gennes, a French Nobel laureate in Physics. He had a distinctive research style. He could, somehow , distill essential insights from complex physical phenomena, and then present them with minimal equations and clear, concise prose. DoxiadÄ“s, A. K. (2001). Uncle Petros and Goldbachâ€™s Conjecture: A Novel of Mathematical Obsession . Bloomsbury USA. Gaillard, M. K. (2015). A singularly unfeminine profession: One womanâ€™s journey in physics . World Scientific Publishing Company. Ray, S., Spangenburg, R., & Moser, D. K. (1995). Niels Bohr: Gentle genius of Denmark . Facts on File. As a physicist, it is easyâ€”very easyâ€”to suffer from mild to severe imposter syndrome. Physics is full of brilliant people, often leading one to feel like the only person who doesnâ€™t understand an otherwise â€œsimpleâ€ idea that everyone else seems to have seamlessly mastered. I read these biographies as a cure for these bouts of doubt. For me, they often show how exhilarating is the joy of understanding a natural phenomenon; the reason we started doing physics in the first place. This simple joy often gets overlooked when you have to apply for your next postdoc position, next faculty position, or apply for grants. Also, these books show how developing any substantial idea can be very time-consuming, in no sense straightforward, and taxing. A playlist with an extended interview of an intriguing academic: Freeman Dyson. This particular book, however, is not about physics. It is written as fables about imaginary physicists navigating an all-too-real world. The stories are thought-provoking, controversial, and candid. Having spent a fair amount of time in academia, I know how true most of them are. I recommend it to anyone interested in the social dynamics of scientists. A nice introduction to the life and times of the physicist Niels Bohr. A series where Rob Phillips is interviewed by David Zierler for the Caltech Heritage project. Inspiring and bold! Gennes, P.-G. de. (2004). Petit point: A candid portrait on the aberrations of science . World Scientific Publications Balaram P. (2017) Memories of a Bangalore Quartet . IISc Connet More on it As a physicist, it is easyâ€”very easyâ€”to suffer from mild to severe imposter syndrome. Physics is full of brilliant people, often leading one to feel like the only person who doesnâ€™t understand an otherwise â€œsimpleâ€ idea that everyone else seems to have seamlessly mastered. I read these biographies as a cure for these bouts of doubt. For me, they often show how exhilarating is the joy of understanding a natural phenomenon; the reason we started doing physics in the first place. This simple joy often gets overlooked when you have to apply for your next postdoc position, next faculty position, or apply for grants. Also, these books show how developing any substantial idea can be very time-consuming, in no sense straightforward, and taxing. This book is exceptionally well-written from both of these perspectives. Mary K. Gaillard is Berkeley's first tenured female faculty. Getting that position as a male physicist was not easy, as a female physicist it was close to impossible. She made it possible. And yet the book's tone is not accusatory; she is not into the blame game. She never said anything about why she and her first husband were divorced; she simply said it was very personal and did not think it was relevant for the aspiring female physicists, her target audience. She also was brutally honest about many incidents that were demeaning to her in many ways; she frequently quoted big names saying those horrible things to her. But when she turned out to be wrong, she admitted. The frankness is admirable. If you want to read a book about a physicist who did brilliant work, made her way through the jungle of academia to the top, and fought against daily, casual sexism from her colleagues, friends, and family members, and yet somehow managed not to hate them, read this book. You won't be disappointed. Mlodinow, L. (2003). Feynmanâ€™s Rainbow: A Search for Beauty in Physics and in Life . Warner Books. Polchinski, J. (2017). Memories of a Theoretical Physicist . arXiv:1708.09093 More on it A short and delightful book by Pierre-Gilles de Gennes, a French Nobel laureate in Physics. He had a distinctive research style. He could, somehow , distill essential insights from complex physical phenomena, and then present them with minimal equations and clear, concise prose. This particular book, however, is not about physics. It is written as fables about imaginary physicists navigating an all-too-real world. The stories are thought-provoking, controversial, and candid. Having spent a fair amount of time in academia, I know how true most of them are. I recommend it to anyone interested in the social dynamics of scientists. The end life of Grothendieck. This book is exceptionally well-written from both of these perspectives. Mary K. Gaillard is Berkeley's first tenured female faculty. Getting that position as a male physicist was not easy, as a female physicist it was close to impossible. She made it possible. Halmos, P. R. (1985). I Want to be a Mathematician . Springer New York. Recommended If I get time And yet the book's tone is not accusatory; she is not into the blame game. She never said anything about why she and her first husband were divorced; she simply said it was very personal and did not think it was relevant for the aspiring female physicists, her target audience. She also was brutally honest about many incidents that were demeaning to her in many ways; she frequently quoted big names saying those horrible things to her. But when she turned out to be wrong, she admitted. The frankness is admirable. Ulam, S. M. (1991). Adventures of a Mathematician . University of California Press. Archive If you want to read a book about a physicist who did brilliant work, made her way through the jungle of academia to the top, and fought against daily, casual sexism from her colleagues, friends, and family members, and yet somehow managed not to hate them, read this book. You won't be disappointed. Nasar, S., & Gruber, D. (2006, August 28). Manifold Destiny . The New Yorker ."
  },
  {
    "title": "Complex Systems",
    "url": "https://arghyadutta.github.io/notebooks/complexity.html",
    "content": "Recommended If I get time Bianconi, G. et al. (2023). Complex systems in the spotlight: Next steps after the 2021 Nobel Prize in Physics . Journal of Physics: Complexity, 4(1), 010201 . For Ising Model: Mitchell, M. (2009). Complexity: A guided tour . Oxford University Press. Parisi, G. (2002). Complex Systems: A Physicistâ€™s Viewpoint arXiv:cond-mat/0205297 BÃ¶ttcher, L., & Herrmann, H. J. (2021). Computational Statistical Physics (1st ed.). Cambridge University Press. Crutchfield, J. P., & Young, K. (1989). Inferring statistical complexity . Physical Review Letters, 63(2), 105â€“108 . Anderson, P. W. (1972). More Is Different: Broken symmetry and the nature of the hierarchical structure of science . Science, 177(4047), 393â€“396 For Computational Physics Chandler, D. (1987). Introduction to Modern Statistical Mechanics . Oxford University Press. (Excellent book.) Taroni, A. (2015). 90 years of the Ising model . Nature Physics, 11(12), 997â€“997. (for the story)"
  },
  {
    "title": "Plain Text Format",
    "url": "https://arghyadutta.github.io/notebooks/plainText.html",
    "content": "If you're putting effort into writing your own notes, consider not locking them in a proprietary format like Word or Google Doc. Instead, use plain text format such as markdown or LaTeX . See Why Plain Text Matters and The Unreasonable Effectiveness Of Plain Text (business perspective) Software : I recommend Obsidian : it's free and has a large community of users. Also see: personal knowledge management and LaTeX ."
  },
  {
    "title": "Dimensional Analysis",
    "url": "https://arghyadutta.github.io/notebooks/dimensionalAnalysis.html",
    "content": "Lemons, D. S. (2017). A Studentâ€™s Guide to Dimensional Analysis (1st ed.). Cambridge University Press. Recommended McKinley, G. H. (2024). Getting the (dimensionless) numbers right . Nature Chemical Engineering, 1(1), Article 1 . Anything Lemons writes is well-written; this is not an exception. Morin, D. J. (2008). Introduction to classical mechanics: With problems and solutions . Cambridge university press. Chapter 1 contains a short but useful discussion."
  },
  {
    "title": "How to do research",
    "url": "https://arghyadutta.github.io/notebooks/researchMethodology.html",
    "content": "Finding the Search Terms for Your Literature Review , 2022. (accessed 2025-09-04). Recommended Dyson argues why we need both specialists and generalists in research. I wish I had a foolproof protocol. Since I don't, here are some useful pointers. And my thoughts on personal knowledge management may be useful. Dyson, F. (2009). Birds and Frogs . Notices of the AMS, 56(2), 212â€“223 . Nielsen, M. A. (2004). Principles of Effective Research . Blogpost . Imposter syndrome is the worst; Schwartz argues why feeling stupid is the norm in scientific research. Chatterjee, A. (2015). Some musings on academic research . Personal webpage . Guttal, V. (n.d.). Assorted articles on professional skills . Curated blog series . Mckenji, R. (2011). Towards research independence . Blogpost . A wonderful guide to working scientists. Brilliant! Schwartz, M. A. (2008). The importance of stupidity in scientific research . Journal of Cell Science, 121(11), 1771 . Research ideas are difficult to conceive; Rockmore tells what works for him. Rockmore, D. (2019). The Myth and Magic of Generating New Ideas . The New Yorker . Hamming, R. (1986). You and Your Research . Lecture transcript ."
  },
  {
    "title": "Chemistry: Online Resources",
    "url": "https://arghyadutta.github.io/notebooks/chemistry.html",
    "content": "Beautiful Chemistry MolView Illustrated Glossary of Organic Chemistry"
  },
  {
    "title": "Ursula K. Le Guin",
    "url": "https://arghyadutta.github.io/notebooks/leguin.html",
    "content": "Guin, U. K. L. (2016). A Wizard of Earthsea . Puffin. Recommended Le Guin's prose flows, interlaced with intricate, compassionate thoughts. Guin, U. K. L. (2018). The Books of Earthsea . Gollancz."
  }
]