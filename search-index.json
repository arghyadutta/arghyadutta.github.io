[
  {
    "title": "Physics Free Books",
    "url": "https://arghyadutta.github.io/notebooks/physicsFreeBooks.html",
    "content": "Group Theory in Physics: An Introduction with Mathematica by Balasubramanian Ananthanarayan, Souradeep Das, Amitabha Lahiri, Suhas Sheikh, Sarthak Talukdar Feynman lectures Entropy by John Baez Online course on topology in condensed matter Lecture notes on Soft Matter and Interfaces by Lydéric Bocquet Lecture notes on Statistical Physics by Lydéric Bocquet Understanding the Properties of Matter by Michael de Podesta Electrodynamics by Phil Nelson (excellent book!)"
  },
  {
    "title": "Combinatorics",
    "url": "https://arghyadutta.github.io/notebooks/combinatorics.html",
    "content": "Bona, M. (2011). Walk Through Combinatorics, A: An Introduction To Enumeration And Graph Theory. Graham, R. L., Knuth, D. E., & Patashnik, O. (2017). Concrete mathematics: A foundation for computer science (2. ed., 31. print). Addison-Wesley. If I Get Time"
  },
  {
    "title": "Poetry",
    "url": "https://arghyadutta.github.io/notebooks/poetry.html",
    "content": "Bank Fishing for Bluegills Ella Wheeler Wilcox Du Fu Ted Kooser East Coker I, Too Like this one An abandoned courtyard: an old tree: A temple bell lying on its side: The world I live in. They win and we lose; we lose and they win. Vines wrap around the rotting bones. She knows he won’t come back from the army, but patches the clothes he left just in case. Or this (I feel this) I am about to scream madly in the office, Especially when they bring more papers to pile higher on my desk. T S Eliot Father I mean, wow! In my beginning is my end. In succession Houses rise and fall, crumble, are extended, Are removed, destroyed, restored, or in their place Is an open field, or a factory, or a by-pass. Old stone to new building, old timber to new fires, Old fires to ashes, and ashes to the earth Which is already flesh, fur and faeces, Bone of man and beast, cornstalk and leaf. Houses live and die: there is a time for building And a time for living and for generation And a time for the wind to break the loosened pane And to shake the wainscot where the field-mouse trots And to shake the tattered arras woven with a silent motto. The Naming of Cats Few poems. The Year Langston Hughes"
  },
  {
    "title": "Software Tools",
    "url": "https://arghyadutta.github.io/notebooks/softwareTools.html",
    "content": "You open a webpage like Cosma Shalizi's (check it!) and find hundreds of links. Which one is active and which one is dead? Link analyzer does that job for you with one click. Gesturefy is such a neat add-on! It lets you do several things, like going back to the previous webpage, only with gestures of your mouse. Return YouTube dislikes brings back a useful feature. Hide Youtube-Shorts removes YouTube shorts from its homepage, subscriptions page, and search results. It can also hide the \"Shorts\" tab. Good riddance! webplotdigitizer - extract data from plots, images, and maps Single-file is a neat add-on that can download a webpage with everything, including figures, in a single HTML file. Very useful if you want to locally archive, for example, a blog post or a Twitter thread. Get unicode of symbols AppCleaner helps to cleanly uninstall an app. uBlock Origin is the ad blocker you should be using. Tt even lets you disable all javascript on a website, if you're adventurous. Amphetamine keeps your MacBook awake, which can be useful during presentations, for example. Addons for Mozilla Firefox OmniDiskSweeper finds and helps delete large files. Tabliss can modify new tab page in the browser. Tranquility Reader can strip down a webpage to main texts and images, letting you read it without 1000 flashy annoying things. Duplicate File Finder Remover finds (and removes) duplicate files. Learn touch typing Tools The Zotero connector allows you to add papers to your Zotero Library from the browser. It can also automatically redirect journal webpages through an institutional proxy, allowing you to download papers with your institutional credentials. Feedbro helps to get articles from any website that provides an RSS feed (blogs, journals, magazines…). A good alternative to Feedly. Find Journals' short names Nifty and Free Software For MacBook Rectangle moves and resizes windows. HTML cleanup tool & simplifier"
  },
  {
    "title": "Simpson's Paradox",
    "url": "https://arghyadutta.github.io/notebooks/simpsonsParadox.html",
    "content": "Are subgroup discovery and Simpson's paradox related? I believe they are, though I haven't fully developed the idea yet. My thinking is that computing a global regression line can be misleading when the data contain strong local patterns that are not aligned with each other. I don't think this is something new, but it would be interesting to explore the idea a bit further. Bookbinder, H. (2025, April 3). Simpson’s Paradox Explains the World Scriptorium Philosophia Pearl, J. (2016). Simpson’s Paradox: The riddle that would not die. Blog Post Recommended"
  },
  {
    "title": "Learnability in Machine Learning",
    "url": "https://arghyadutta.github.io/notebooks/learnability.html",
    "content": "Online Resources Output : $y$ (good/bad customer) Especially useful for its introductory discussion on learnability and VC dimension. Hypothesis : $g:\\mathcal{X}\\rightarrow\\mathcal{Y}$, $g \\in \\mathcal{H}$. We hope that $g$ approximates $f$ well, that is the goal of learning. We don't know $f$, we can only guess what it is from the data. The learning algorithm picks $g\\simeq f$ from the hypothesis set $\\mathcal{H}$. Hypothesis set : $\\mathcal{H}={h}$. It plays a pivotal role. It can be a linear regression, a neural network, a support vector machine… Learning algorithm: $\\mathcal{A}$ (e.g. back-propagation for neural network.) It does the searching and produces $g$. Machine learning course by Yaser Abu-Mostafa. Input : $\\mathbf{x}$ (customer application) The hypothesis set and the learning algorithm $(\\mathcal{H}, \\mathcal{A})$ together are known as the learning model . Target function : $f:\\mathcal{X}\\rightarrow\\mathcal{Y}$ (ideal credit approval formula) Data : $(\\mathbf{x}_1, y_1),(\\mathbf{x}_2, y_2),\\cdots,(\\mathbf{x}_N, y_N)$ (historical records of credit customers)"
  },
  {
    "title": "Subgroup Discovery",
    "url": "https://arghyadutta.github.io/notebooks/subgroupDiscovery.html",
    "content": "What's new in it then from clustering? My current understanding (need to check and update): Compared to global methods like decision tree, regression, or compressed sensing, SGD is a local method meaning it does not attempt to classify all the elements of the parent set into subsets, rather it tries to find subclasses which are of high quality with respect to the desired property. Given: Sample $S \\subseteq P$, Target variable $y:P\\rightarrow {a, b, c, \\cdots}$, and Features $x_j: P\\rightarrow X_j$ Recommended Lopez-Martinez-Carrasco, A.; Juarez, J. M.; Campos, M.; Mora-Caselles, F. Subgroups: A Python Library for Subgroup Discovery. SoftwareX 2024 , 28 , 101895. https://doi.org/10.1016/j.softx.2024.101895 . Algorithm (tentative, need to check) Atzmueller, M. Subgroup Discovery. WIREs Data Mining and Knowledge Discovery 2015 , 5 (1), 35–49. https://doi.org/10.1002/widm.1144 . Define: Propositions $Pi_x = {\\pi_1,\\cdots, \\pi_k}$, Selection language $\\mathcal{L}_x = {\\sigma(i)=\\pi_{j_1}(i)\\wedge\\cdots\\wedge \\pi_{j_t}(i)}$ Subgroup discovery (SGD) is a local knowledge discovery method. It identifies subsets in a set of elements that 'stands out' with respect to some property of the elements. Ghiringhelli, L. M.; Vybiral, J.; Levchenko, S. V.; Draxl, C.; Scheffler, M. Big Data of Materials Science: Critical Role of the Descriptor. Phys. Rev. Lett. 2015 , 114 (10), 105503. https://doi.org/10.1103/PhysRevLett.114.105503 . Goldsmith, B. R.; Boley, M.; Vreeken, J.; Scheffler, M.; Ghiringhelli, L. M. Uncovering Structure-Property Relationships of Materials by Subgroup Discovery. New J. Phys. 2017 , 19 (1), 013031. https://doi.org/10.1088/1367-2630/aa57c2 . New (~2024–25): GitHub Repo with Python implementation Also see Lopez-Martinez-Carrasco et al. (2024).  (I tried a Java implementation few years ago; it was useful but a bit clunky.) Boley, M.; Goldsmith, B. R.; Ghiringhelli, L. M.; Vreeken, J. Identifying Consistent Statements about Numerical Data with Dispersion-Corrected Subgroup Discovery. Data Min Knowl Disc 2017 , 31 (5), 1391–1418. https://doi.org/10.1007/s10618-017-0520-3 . Optimize: $f(Q)=\\textrm{cov}(Q)^\\gamma \\textrm{eff}(Q)_+$ where $Q=\\{i \\in S: \\sigma(i)= \\textrm{True}\\}$ (extension), $\\textrm{cov}(Q)=|Q|/|S|$ (coverage), $\\textrm{eff}(Q)= \\frac{H_y(S)-H_y(Q)}{H_y(S)}$ (effect), and $H_y(Q) = -\\sum_v p_Q(y=v) \\log p_Q(y=v)$ (entropy). It has been used to find out subgroup properties that contribute to one of the two crystal structures of 82 octet binaries (Ghiringhelli et al. 2015). SGD predicts two subgroups which contain elements that have either a zinc blend structure or a rock salt structure (Goldsmith et al 2017, Boley et al. 2017). For a review of the SGD, see Atzmueller (2015). Package"
  },
  {
    "title": "Quantum mechanics: Books and Online Resources",
    "url": "https://arghyadutta.github.io/notebooks/qMech.html",
    "content": "Recommended Gasiorowicz, S. (1974). Quantum physics . New York, Wiley. Das, A. (2006). Field Theory: A Path Integral Approach (2nd ed., Vol. 75). World Scientific. Bohm, D. (1989). Quantum Theory . Dover Publications. Quantum entanglement by Mark Wilde It's very much worth watching despite the poor recording quality. One of the best graduate-level QM book. Precise and concise. Sakurai, J. J., & Napolitano, J. (2021). Modern quantum mechanics (3rd ed). Cambridge University Press. Introduction to Quantum Information Science by Artur Ekert. Lectures on Quantum Computing . From YouTube I have discovered this gem surprisingly late. Bohm lucidly discussed several topics which are hard to find in other books (one example would be statistical properties of correlations for classical and quantum systems, see chapter 10!) Flügge, S. (1999). Practical quantum mechanics . Springer. It's at Griffiths's level and comes with several detailed examples. Though not as popular, it does provide a solid introduction. I love this book! The chapters on rotational invariance, addition of angular momenta, scattering theory, and path integral are particularly good. A good undergrad-level textbook. The chapter on solutions to time-independent Schrödinger equation is neat. A completely different sort of book which teaches QM through a series of problems. Shankar, R. (1994). Principles of quantum mechanics (2nd ed). Plenum Press. Townsend, J. S. (2012). A modern approach to quantum mechanics (2nd ed). University Science Books. An easier version of Sakurai's book. (And there exists a cheaper Indian edition.) Ekert is a patient teacher. The accompanying free book is useful, too. For the path integral formalism of QM, the first few chapters of Ashok Das's book provide a pedagogic introduction. Griffiths, D. J., & Schroeter, D. F. (2018). Introduction to quantum mechanics (Third edition). Cambridge University Press."
  },
  {
    "title": "Deep Learning",
    "url": "https://arghyadutta.github.io/notebooks/deepLearning.html",
    "content": "Online Resources Deep Learning by Frank Noe . Leisurely paced, meticulous."
  },
  {
    "title": "Blogs",
    "url": "https://arghyadutta.github.io/notebooks/blogs.html",
    "content": "Statistical Modeling, Causal Inference, and Social Science Ahead of AI Machine Learning Research Blog Forms of life, forms of mind Scatterings Probably overthinking it \"We humans are such collecting creatures. We love creating sets of things, hoarding what we know or what we have, maybe arranging it all carefully for display.\" nanoscale views Condensed concepts And then. Academics and beyond off the convex path Blogs as Modern Commonplace Books, and the Pleasures Thereof; Ana Ulin; Her blog (2018) Simon Willison's weblog Almost Sure. A random mathematical blog Writing Science The Last Word On Nothing Separated by a Common Language Blogs I Read The 20% Statistician AI: A Guide for Thinking Humans Blogging Academic Garden"
  },
  {
    "title": "Engineering Physics",
    "url": "https://arghyadutta.github.io/notebooks/enggPhysics.html",
    "content": "Balakrishnan, V. How Is a Vector Rotated? Resonance 1999, 4 (10), 61–68 . A few references for the Engineering Physics course (FIC-102) that I teach at SRM University-AP. Excellent visualizations of many physical phenomena. Two wonderful books. And all the accompanying lectures are available for free (the books are transcripts of what Shankar taught in the class.) I was aware of these two books, but I thank my colleague Supravat Dey for strongly recommending them! Serway, R. A.; Jewett, J. W.; Peroomian, Vahé. Physics for Scientists and Engineers , Tenth edition.; Cengage: Boston, MA, USA, 2019. Additional readings Recommended resources Shankar, R. Fundamentals of Physics: Mechanics, Relativity, and Thermodynamics; Open Yale courses series; Yale University Press: New Haven, 2014. Shankar, R. Fundamentals of Physics. II: Electromagnetism, Optics, and Quantum Mechanics; The Open Yale courses series; Yale University Press: New Haven, 2016."
  },
  {
    "title": "Time's Arrow",
    "url": "https://arghyadutta.github.io/notebooks/timesArrow.html",
    "content": "If I Get Time Arrow of Time and Irreversibility"
  },
  {
    "title": "Dictionary",
    "url": "https://arghyadutta.github.io/notebooks/dictionary.html",
    "content": "Usage notes, fights between descriptivists and prescriptivists, illustrations, portraits—what more could you ask for in a dictionary? Auburn, D. et al. (2012). Oxford American Writer’s Thesaurus (3rd edition). Oxford University Press Inc. Editors of the A. H. (2018). The American Heritage Dictionary Of The English Language , Fifth Edition: Fiftieth Anniversary Printing (Indexed edition). Collins Reference. Tschirner, E.; Möhring, J. A Frequency Dictionary of German . Medawar, P. B., & Medawar, J. S. (1985). Aristotle to Zoos: A Philosophical Dictionary of Biology . Oxford University Press. Recommended Somers, J. (2014). You’re probably using the wrong dictionary His blog Surprisingly good. If I Get Time Mautner, T. (Ed.). (2005). Dictionary Of Philosophy (2nd edition). Penguin UK. Dictionary of Untranslatables: A Philosophical Lexicon ; Cassin, B., Rendall, S., Apter, E. S., Eds.; Translation/Transnation Ser; Princeton University Press: Princeton, 2014. Excellent! I love dictionaries! Langenscheidt Basic German Vocabulary. Hauptbd. A Learner’s Dictionary Divided into Subject Categories with Example Sentences ; Langenscheidt: Berlin, 2010. Cuddon, J. A., & Habib, M. A. R. (2015). Dictionary of Literary Terms & Literary (5th edition). Penguin UK. An ode to Webster’s 1913 dictionary ."
  },
  {
    "title": "How to do research",
    "url": "https://arghyadutta.github.io/notebooks/researchMethodology.html",
    "content": "Research ideas are difficult to conceive; Rockmore tells what works for him. Dyson argues why we need both specialists and generalists in research. Recommended Schwartz, M. A. (2008). The importance of stupidity in scientific research . Journal of Cell Science, 121(11), 1771 . Dyson, F. (2009). Birds and Frogs . Notices of the AMS, 56(2), 212–223 . Mckenji, R. (2011). Towards research independence . Blogpost . Hamming, R. (1986). You and Your Research . Lecture transcript . Chatterjee, A. (2015). Some musings on academic research . Personal webpage . Guttal, V. (n.d.). Assorted articles on professional skills . Curated blog series . Finding the Search Terms for Your Literature Review , 2022. (accessed 2025-09-04). I wish I had a foolproof protocol. Since I don't, here are some useful pointers. And my thoughts on personal knowledge management may be useful. Nielsen, M. A. (2004). Principles of Effective Research . Blogpost . Imposter syndrome is the worst; Schwartz argues why feeling stupid is the norm in scientific research. A wonderful guide to working scientists. Brilliant! Rockmore, D. (2019). The Myth and Magic of Generating New Ideas . The New Yorker ."
  },
  {
    "title": "Germany",
    "url": "https://arghyadutta.github.io/notebooks/germany.html",
    "content": "First, without obsessing over the brutality and sadism of individual Nazi party members, this series provides a complete and unnerving account of how the Nazis, after coming in power, \"won\" the support of German people. It portrays the contribution of Joseph Goebbels, the Reich Propaganda minister, in mobilizing the people to a war for the \"Living space\". Recommended A splendid set of books! The Most Frequent German Words – Deutsch 101-326 MacGregor, N. (2014). Germany: Memories of a Nation (1st edition). Penguin. Highly recommended to all interested in the perilous journey undertaken by mankind during WW2. Bilingual Electronic Books – Dual Language German/English – Doppeltext Evans, R. J. (2004). Coming Of The Third Reich . Penguin Evans, R. J. (2006). Third Reich in Power , 1933-1939. Penguin Evans, R. J. (2009). Third Reich at War , Penguin 15 Great German Children’s Books for Readers of All Ages | FluentU German More on these Recounting the experience of individuals brings home, as nothing else can, the sheer complexity of the choices they had to make, and the difficult and often opaque nature of the situations they confronted. Contemporaries could not see things as clearly as we can, with the gift of hindsight: they could not know in 1930 what was to come in 1933, they could not know in 1933 what was to come in 1939 or 1942 or 1945. If they had known, doubtless the choices they made would have been different. One of the greatest problems in writing history is to imagine oneself back in the world of the past, with all the doubts and uncertainties people faced in dealing with a future that for the historian has also become the past. Developments that seem inevitable in retrospect were by no means so at the time, and in writing this book I have tried to remind the reader repeatedly that things could easily have turned out very differently to the way they did at a number of points in the history of Germany in the second half of the nineteenth century and the first half of the twentieth. People make their own history, as Karl Marx once memorably observed, but not under conditions of their own choosing. These conditions included not only the historical context in which they lived, but also the way in which they thought, the assumptions they acted upon, and the principles and beliefs that informed their behavior. A central aim of this book is to re-create all these things for a modern readership, and to remind readers that, to quote another well-known aphorism about history, 'the past is a foreign country: they do things differently there'. —Richard J. Evans A splendid set of books! The first volume deals with the coming in power of the Nazi party; the second volume is about the social, political and many other dimensions of German life under the Nazi rule and, finally, the third volume describes the expansion of the Third Reich and its subsequent downfall in 1945. I liked these books for a number of reasons: First, without obsessing over the brutality and sadism of individual Nazi party members, this series provides a complete and unnerving account of how the Nazis, after coming in power, \"won\" the support of German people. It portrays the contribution of Joseph Goebbels, the Reich Propaganda minister, in mobilizing the people to a war for the \"Living space\". Second, the series provides a rare glimpse into the life of the common members of the Nazi party, Nazi party bosses and, also, ordinary German folks. How did the Germans react to the overbearing regime of the Nazi party? Were they happy under its rule? Did they share the murderous attitude of Nazi party towards \"subhuman(!)\" Polish and Slovakian people and, worst of them all, according to Nazi ideology, Jewish people? As expected, these questions do not have a binary answer and the author explored them with the help of many diaries (most notably, a jew's—Victor Klemperer's—and a Nazi party member's, Melita Maschmann's). Finally, it's reassuring to learn about humane actions done by some Germans to help the oppressed and to observe that the Germans did not lose their sense of humor even under such oppressive regime, as evidenced by many jokes mentioned in the book. Though, let me be clear, this does not at all means that Evans has tried to defend the German atrocities committed during the war. Quite contrarily, he devoted almost 40% of the third volume detailing the massacre brought upon by the military and the SS on the civilian of the conquered countries. Highly recommended to all interested in the perilous journey undertaken by mankind during WW2. Learning German: An Annotated Parallel Texts Reading List Learning the language A Guide to German Verbs for Beginners German.net Textbooks - German German History in Documents and Images The first volume deals with the coming in power of the Nazi party; the second volume is about the social, political and many other dimensions of German life under the Nazi rule and, finally, the third volume describes the expansion of the Third Reich and its subsequent downfall in 1945. I liked these books for a number of reasons: Finally, it's reassuring to learn about humane actions done by some Germans to help the oppressed and to observe that the Germans did not lose their sense of humor even under such oppressive regime, as evidenced by many jokes mentioned in the book. Though, let me be clear, this does not at all means that Evans has tried to defend the German atrocities committed during the war. Quite contrarily, he devoted almost 40% of the third volume detailing the massacre brought upon by the military and the SS on the civilian of the conquered countries. Dartmouth German Second, the series provides a rare glimpse into the life of the common members of the Nazi party, Nazi party bosses and, also, ordinary German folks. How did the Germans react to the overbearing regime of the Nazi party? Were they happy under its rule? Did they share the murderous attitude of Nazi party towards \"subhuman(!)\" Polish and Slovakian people and, worst of them all, according to Nazi ideology, Jewish people? As expected, these questions do not have a binary answer and the author explored them with the help of many diaries (most notably, a jew's—Victor Klemperer's—and a Nazi party member's, Melita Maschmann's). Grammar | DW Learn German Reverso | Free Translation, Dictionary Denglisch Dictionary: When Languages Collide"
  },
  {
    "title": "Academic advice",
    "url": "https://arghyadutta.github.io/notebooks/academicAdvice.html",
    "content": "Schimel, J. (2011). Writing Science: How to Write Papers That Get Cited and Proposals That Get Funded. Oxford University Press. Practical, actionable advice. See also Advice on scientific writing by Hermut Grubmüller Showcase of Scholarly Writing Geroch, R. Suggestions For Giving Talks. arXiv:gr-qc/9703019 1997. How to do research Susan McConnell (Stanford): Designing Effective Scientific Presentations ; 2011.  (accessed 2025-09-04). Guide to english communication for scientists by Nature Giving talks On Writing Academic writing Jhala, R. 2005. (An Opionionated Talk) On Preparing Good Talks (accessed 2025-09-04). Derek Dreyer; 2020. How To Write Papers So People Can Read Them (accessed 2025-09-04). Ian Baldwin (Max Planck Institute): Making Scientific Writing Painless ; 2017.  (accessed 2025-09-04). Writing in the Sciences by Kristin Sainani . If you're just starting to write scientific prose, watch the series"
  },
  {
    "title": "Academia: Issues",
    "url": "https://arghyadutta.github.io/notebooks/academiaIssues.html",
    "content": "Nothing we make, alas, is perfect. Lenz, M. (2020). The adversarial culture in philosophy does not serve the truth Aeon Ideas. Aeon. See also To be fair, I've published in Nature family of journals—but never as a corresponding author. I think Sierra's is argument is sound. Recommended Skinner, B. (2019) What It Means, and Doesn’t Mean, to Get a Job in Physics . Gravity and Levity Sierra, C. A. (n.d.). Reject Nature: Elite journals and the defeat of science . Retrieved December 6, 2021, from his blog \". Curry, S. Sick of Impact Factors . Reciprocal Space . Retrieved December 6, 2021. Lin, J. (2010). Unraveling tenure at MIT . The Tech The unsavory side of academia. Academia, the Good Side"
  },
  {
    "title": "High-performance Computing",
    "url": "https://arghyadutta.github.io/notebooks/hpc.html",
    "content": "A short series on asymptotic notation and analysis of algorithms by David Scot Taylor . Check his YouTube channel, Algorithms with Attitude, for more stuff. Useful if you're coming from a python background and not C/C++ or Fortran. HPC by Matthew Jacob from IISc Improving and optimizing Python by Sebastian Mathôt Recommended Introduction to parallel Programming in Open MP NPTEL course by Yogish Sabharwal from IIT Delhi Pointers in C and C++ C++ by iamcanadian Pacheco, P. S. (2011). An introduction to parallel programming . Morgan Kaufmann. Introduction to SLURM Courses on YouTube Intro to parallel programming Message-passing programming with MPI Zaccone, G. (2019). Python parallel programming cookbook."
  },
  {
    "title": "Fountain Pen, Pencil, Paper",
    "url": "https://arghyadutta.github.io/notebooks/stationery.html",
    "content": "Fountain Pen Network I have read (and spent) a bit too much on fountain pens, pencils, and paper—in general, writing paraphernalia. Maybe someday I'll write about them, but not today. Today, I am busy filling out electronic forms, Word files, and Excel sheets that my workplace is asking for. Lamy 2000 And The Origins Of Lamy Design . (2012, August 11). The Fountain Pen Network In case you're serious about Lamy 2000."
  },
  {
    "title": "Self-avoiding Random Walks",
    "url": "https://arghyadutta.github.io/notebooks/saw.html",
    "content": "Lawler, G. F. (2011). Scaling limits and the Schramm-Loewner evolution . Probability Surveys, 8 . Recommended Numerical Results There are no known method to calculate $c_n$ other than numerical enumeration and asymptotic analysis, as far as I know. The following table shows the length of largest SAWs that have been enumerated, with $c_n$ often becoming as large as $10^{30}$: $d$ $\\max(n)$ Symmetry Reference 2 71 Square Jensen (2004) 3 36 Simple cubic Lawler (2011) 3 28 BCC Schram (2017) 3 24 FCC Schram (2017) Asymptotic results (large $n$) Asymptotically, $\\langle R_\\mathrm{ g}^2\\rangle$ and $\\langle R_\\mathrm{ e}^2\\rangle$ are conjectured to behave as $\\langle R_\\mathrm{ e}^2\\rangle \\approx BN^{2\\nu}$ and $\\langle R_\\mathrm{ g}^2\\rangle \\approx CN^{2\\nu}$. The non-universal amplitudes $B$ and $C$ depend on lattice symmetry and dimension. $c_n$ is conjectured to behave as: $c_n \\approx A \\mu^n n^{\\gamma -1}$ where $\\mu$ is the connective constant (Sometimes it's called growth constant and $k=\\log \\mu$ is called connective constant). The non-universal amplitude $A$ depends on both the dimension and lattice symmetry. However, the $\\nu$ and $\\gamma$ only depend on the dimension—they are universal. $d$ $\\nu$ $\\gamma$ $c_n\\approx$ Comment Reference 2 $3/4$ $43/32 = 1.343 75$ $A \\mu^n n^{11/32}$ Exact Nienhuis (1982), Nienhuis (1984) 3 0.58759700(40)$\\approx 3/5$ $1.15695300(95) \\approx 7/6$ $A \\mu^n n^{1/6}$ Estimate Schram (2017), Slade (2019) (for summary) 4 $1/2$ with log correction $\\langle R_\\mathrm{ e}^2\\rangle \\approx B n(\\log n)^{1/4}$ $\\langle R_\\mathrm{ g}^2\\rangle \\approx C n(\\log n)^{1/4}$ 1 with log correction $A \\mu^n (\\log n)^{1/4}$ Upper critical dimension Slade (2019) $\\geq 5$ 1/2 1 $A \\mu^n$ Probably exact Slade (2019), p.4 Recommended Clisby, N., & Dünweg, B. (2016). High-precision estimate of the hydrodynamic radius for self-avoiding walks . Physical Review E, 94(5), 052102 . Jensen, I. (2004). Self-avoiding walks and polygons on the triangular lattice . Journal of Statistical Mechanics: Theory and Experiment, 2004(10), P10008 . Lawler, G. F. (2011). Scaling limits and the Schramm-Loewner evolution . Probability Surveys, 8 . Nienhuis, B. (1982). Exact Critical Point and Critical Exponents of $\\mathrm{O}(n)$ Models in Two Dimensions . Physical Review Letters, 49(15), 1062–1065 . Nienhuis, B. (1984). Critical behavior of two-dimensional spin models and charge asymmetry in the Coulomb gas . Journal of Statistical Physics, 34(5), 731–761 . Schram, R. D., Barkema, G. T., Bisseling, R. H., & Clisby, N. (2017). Exact enumeration of self-avoiding walks on BCC and FCC lattices . Journal of Statistical Mechanics: Theory and Experiment, 2017(8), 083208 . Slade, G. (2019). Self-avoiding walk, spin systems and renormalization . Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences, 475(2221), 20180549 . However, the $\\nu$ and $\\gamma$ only depend on the dimension—they are universal. Schram, R. D., Barkema, G. T., Bisseling, R. H., & Clisby, N. (2017). Exact enumeration of self-avoiding walks on BCC and FCC lattices . Journal of Statistical Mechanics: Theory and Experiment, 2017(8), 083208 . Asymptotic results (large $n$) Asymptotically, $\\langle R_\\mathrm{ g}^2\\rangle$ and $\\langle R_\\mathrm{ e}^2\\rangle$ are conjectured to behave as $\\langle R_\\mathrm{ e}^2\\rangle \\approx BN^{2\\nu}$ and $\\langle R_\\mathrm{ g}^2\\rangle \\approx CN^{2\\nu}$. The non-universal amplitudes $B$ and $C$ depend on lattice symmetry and dimension. $c_n$ is conjectured to behave as: $c_n \\approx A \\mu^n n^{\\gamma -1}$ where $\\mu$ is the connective constant (Sometimes it's called growth constant and $k=\\log \\mu$ is called connective constant). The non-universal amplitude $A$ depends on both the dimension and lattice symmetry. However, the $\\nu$ and $\\gamma$ only depend on the dimension—they are universal. $d$ $\\nu$ $\\gamma$ $c_n\\approx$ Comment Reference 2 $3/4$ $43/32 = 1.343 75$ $A \\mu^n n^{11/32}$ Exact Nienhuis (1982), Nienhuis (1984) 3 0.58759700(40)$\\approx 3/5$ $1.15695300(95) \\approx 7/6$ $A \\mu^n n^{1/6}$ Estimate Schram (2017), Slade (2019) (for summary) 4 $1/2$ with log correction $\\langle R_\\mathrm{ e}^2\\rangle \\approx B n(\\log n)^{1/4}$ $\\langle R_\\mathrm{ g}^2\\rangle \\approx C n(\\log n)^{1/4}$ 1 with log correction $A \\mu^n (\\log n)^{1/4}$ Upper critical dimension Slade (2019) $\\geq 5$ 1/2 1 $A \\mu^n$ Probably exact Slade (2019), p.4 Recommended Clisby, N., & Dünweg, B. (2016). High-precision estimate of the hydrodynamic radius for self-avoiding walks . Physical Review E, 94(5), 052102 . Jensen, I. (2004). Self-avoiding walks and polygons on the triangular lattice . Journal of Statistical Mechanics: Theory and Experiment, 2004(10), P10008 . Lawler, G. F. (2011). Scaling limits and the Schramm-Loewner evolution . Probability Surveys, 8 . Nienhuis, B. (1982). Exact Critical Point and Critical Exponents of $\\mathrm{O}(n)$ Models in Two Dimensions . Physical Review Letters, 49(15), 1062–1065 . Nienhuis, B. (1984). Critical behavior of two-dimensional spin models and charge asymmetry in the Coulomb gas . Journal of Statistical Physics, 34(5), 731–761 . Schram, R. D., Barkema, G. T., Bisseling, R. H., & Clisby, N. (2017). Exact enumeration of self-avoiding walks on BCC and FCC lattices . Journal of Statistical Mechanics: Theory and Experiment, 2017(8), 083208 . Slade, G. (2019). Self-avoiding walk, spin systems and renormalization . Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences, 475(2221), 20180549 . Slade, G. (2019). Self-avoiding walk, spin systems and renormalization . Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences, 475(2221), 20180549 . $c_n$ is conjectured to behave as: $c_n \\approx A \\mu^n n^{\\gamma -1}$ where $\\mu$ is the connective constant (Sometimes it's called growth constant and $k=\\log \\mu$ is called connective constant). To get an idea about the size of $n$-step SAWs, one computes their values averaged over all $n$-step SAWs: The non-universal amplitude $A$ depends on both the dimension and lattice symmetry. Nienhuis, B. (1984). Critical behavior of two-dimensional spin models and charge asymmetry in the Coulomb gas . Journal of Statistical Physics, 34(5), 731–761 . There are no known method to calculate $c_n$ other than numerical enumeration and asymptotic analysis, as far as I know. The following table shows the length of largest SAWs that have been enumerated, with $c_n$ often becoming as large as $10^{30}$: The non-universal amplitudes $B$ and $C$ depend on lattice symmetry and dimension. Clisby, N., & Dünweg, B. (2016). High-precision estimate of the hydrodynamic radius for self-avoiding walks . Physical Review E, 94(5), 052102 . Let $c_n$ be the number and $R_e$ and $R_g$ be the end-to-end distance and radius of gyration of a $n$-step self-avoiding random walk with vertices at $\\omega_0,\\omega_2,\\cdots,\\omega_n$. Then the radius of gyration and end-to-end distance, two relevant measures quantifying the \"size\" the SAW, are defined as follows: Asymptotically, $\\langle R_\\mathrm{ g}^2\\rangle$ and $\\langle R_\\mathrm{ e}^2\\rangle$ are conjectured to behave as $\\langle R_\\mathrm{ e}^2\\rangle \\approx BN^{2\\nu}$ and $\\langle R_\\mathrm{ g}^2\\rangle \\approx CN^{2\\nu}$. Asymptotic results (large $n$) Jensen, I. (2004). Self-avoiding walks and polygons on the triangular lattice . Journal of Statistical Mechanics: Theory and Experiment, 2004(10), P10008 . Nienhuis, B. (1982). Exact Critical Point and Critical Exponents of $\\mathrm{O}(n)$ Models in Two Dimensions . Physical Review Letters, 49(15), 1062–1065 ."
  },
  {
    "title": "Plain Text Format",
    "url": "https://arghyadutta.github.io/notebooks/plainText.html",
    "content": "If you're putting effort into writing your own notes, consider not locking them in a proprietary format like Word or Google Doc. Instead, use plain text format such as markdown or LaTeX . Also see: personal knowledge management and LaTeX . Software : I recommend Obsidian : it's free and has a large community of users. See Why Plain Text Matters and The Unreasonable Effectiveness Of Plain Text (business perspective)"
  },
  {
    "title": "Advaita Vedanta",
    "url": "https://arghyadutta.github.io/notebooks/advaitaVedanta.html",
    "content": "Accurate english translation of Shankaracharya's commentary. Shankaracharya's commentaries, along with his hymns and other writings, are now made available online in original (Sanskrit) by Shringeri math. Swami, Gambhirananda (1984). Bhagavad Gita: With the commentary of Shankaracharya . Advaita Ashrama. Hindi translation 1) Isshadi Nau Upnishad Shankar Bhashya Sahit 2) Chandogya Upanishad 3) Brihadaranyak Upanishad Gita Press, Gorakhpur Hindi translation of the commentaries of Shankaracharya of eleven principal upanishads. Excellent, nuanced translations. 1) Shankaracharya (1957). Eight Upanisads Vol 1 (S. Gambhirananda, Trans.). Advaita Ashrama. 2) Shankaracharya (1957). Eight Upanisads Vol 2 (S. Gambhirananda, Trans.). Advaita Ashrama. Accurate, concise english translations of Shankaracharya's commentaries on eight of the principal upanishads: Isa, Kena, Katha, Taittariya (vol. 1) and Aitareya, Mundaka, Mandukya (with Karika), Prasna (vol. 2). Upanishads Bramhasutra Vishwarupananda, S. (1997). Vedanta Darshan (4 volumes, in Bengali). Udbodhan Karyalaya, Kolkata. A complete, heavily annotated bengali translation of the Shankaracharya's commentary of the Bramha Sutra. The footnotes are often useful, particularly to clarify doubts regarding Mimansa and Nyaya. Shrimad Bhagwat Gita (ShankarBhashya Hindi Anuvad Sahit) (2022). Gita Press, Gorakhpur. Shankaracharya (1956). Bramhasutra Bhasya (Sw. Gambhirananda, Trans.). Advaita Ashrama. English translation Gita"
  },
  {
    "title": "Philosophy: Online Resources",
    "url": "https://arghyadutta.github.io/notebooks/philosophyResources.html",
    "content": "Fadedpage Canada's project Gutenberg. Surprisingly rich collection. Project Gutenberg archive.org Classic books (from the U.S. Library of Congress) Two rather obvious entries, but the more I explore them, the more I get pleasantly surprised. They are true treasure troves. Open Greek and Latin Perseus Digital Library Digital Library of Darshan Manisha (Texts on Indian Philosophy)"
  },
  {
    "title": "Biology",
    "url": "https://arghyadutta.github.io/notebooks/biology.html",
    "content": "Outstanding content and Harold is a master of prose. Much better than standard popular science books. I haven't read another 'dictionary' quite like it. Medawar, P. B., & Medawar, J. S. (1985). Aristotle to Zoos: A Philosophical Dictionary of Biology . Oxford University Press. Recommended A unique book, like Phillips's other ones. Harold, F. M. (2001). The Way of the Cell: Molecules, Organisms, and the Order of Life . Oxford University Press. Milo, R., & Phillips, R. (2016). Cell biology by the numbers. Garland Science, Taylor & Francis Group. ( Online free version ) An Owner's Guide to the Human Genome: an introduction to human population genetics, variation and disease by Jonathan Pritchard"
  },
  {
    "title": "Personal Knowledge Management",
    "url": "https://arghyadutta.github.io/notebooks/pkm.html",
    "content": "Weekly Review: Using Obsidian to Close Open Loops at the End of the Week . Resources from others Example: If you're writing a note on phase diagrams of proteins, connect it to existing notes, if any, on thermodynamic phases. Try to write in plain text format. Related point : If you don’t have a note yet, it can be tempting to start one, which can be useful or a waste of time depending on your needs. Since these are your personal notes , they need not be encyclopedic. Knowledge gaps in personal notes are fine given our limited time, but if the topic you skipped repeatedly comes up in your research, consider reading about it and writing a note. Referencing: Use some marker, like `name-year-firstWord`, in the text and include a full bibliography at the end. You can also use pandoc to generate sorted bibliographies automatically; search for how to do it. Don’t copy text written by others; instead archive it in Zotero if you really want to keep a copy. Copying text for your personal notes defeats the purpose of taking notes: understanding and summarizing ideas for yourself . A few thoughts on taking notes for your work or research: Managing project folders is tricky. Dan Larremore suggests making these folders in your project folder: raw-inputs, cleaned-inputs, code, output, figures, and writing. I think that's very useful. Also, keeping a README file in plain text format in your project folder and actively updating it is crucial. Ycombinator post on Managing my personal knowledge base Think carefully about how the newfound piece of information connects to what you already know. Copy well-made images for your personal archive since they are useful, but don’t ever forget to keep the attributions in your notes. Otherwise, you will forget the attribution and will risk plagiarism. Think before you write a note—a note’s usefulness generally derives from the effort you put in. See how to take effective research notes ."
  },
  {
    "title": "Hydrophobicity",
    "url": "https://arghyadutta.github.io/notebooks/hydrophobicity.html",
    "content": "A result of interplay between entropy and enthalpy. Berne, B. J.; Weeks, J. D.; Zhou, R. Dewetting and Hydrophobic Interaction in Physical and Biological Systems. Annual Review of Physical Chemistry 2009 , 60 (1), 85–103. https://doi.org/10.1146/annurev.physchem.58.032806.104445 If I Get Time Xi, E.; Patel, A. J. The Hydrophobic Effect, and Fluctuations: The Long and the Short of It. Proc. Natl. Acad. Sci. U.S.A. 2016 , 113 (17), 4549–4551. https://doi.org/10.1073/pnas.1603014113 . Tanford, C. The Hydrophobic Effect and the Organization of Living Matter. Science 1978 , 200 (4345), 1012–1018. https://doi.org/10.1126/science.653353 . Ben-Naim, A. Hydrophobic Interaction and Structural Changes in the Solvent. Biopolymers 1975 , 14 (7), 1337–1355. https://doi.org/10.1002/bip.1975.360140704 . Kauzmann, W. Some Factors in the Interpretation of Protein Denaturation. In Advances in Protein Chemistry ; Elsevier, 1959; Vol. 14, pp 1–63. https://doi.org/10.1016/S0065-3233(08)60608-7 . Lazaridis, T. Hydrophobic Effect. In eLS ; John Wiley & Sons, Ltd, 2013. https://doi.org/10.1002/9780470015902.a0002974.pub2 ."
  },
  {
    "title": "Electromagnetism",
    "url": "https://arghyadutta.github.io/notebooks/electromagnetism.html",
    "content": "Recommended Purcell, E. M., & Morin, D. J. (2013). Electricity and Magnetism (Third edition). Cambridge University Press. Why Electrostatics Rules the Life of a Cell with Robijn Bruinsma; 2024. (accessed 2025-10-04). For a fun aside, see: Fahy, S.; O’Sullivan, C. All Magnetic Phenomena Are NOT Due to Electric Charges in Motion . Am. J. Phys. 2022, 90 (1), 7–8 . And: Griffiths, D. Reply to: All Magnetic Phenomena Are NOT Due to Electric Charges in Motion American Journal of Physics 2022, 90 (1), 9–9 . Griffiths, D. J. (2023). Introduction to Electrodynamics (5th ed.). Cambridge University Press. This book is on a league of it's own. The CUP edition is expensive, especially for Indian students. But note that it is far more carefully edited than the recent cheaper Berkeley series edition which claims to use the SI units."
  },
  {
    "title": "Probability and Statistics",
    "url": "https://arghyadutta.github.io/notebooks/statistics.html",
    "content": "Holmes, S., & Huber, W. (2019). Modern statistics for modern biology . Cambridge university press. ( Free online copy ) Regression and Other Stories by Andrew Gelman, Jennifer Hill, Aki Vehtari Introduction to Probability, Statistics, and Random Processes by Hossein Pishro-Nik Recommended MIT RES.6-012 Introduction to Probability, Spring 2018 A YouTube playlist with beginner friendly lectures by John Tsitsiklis and Patrick Jaillet from MIT. They also wrote a book on probability and made a summary of it freely available . The Little Handbook of Statistical Practice by Gerard E. Dallal The Book of Statistical Proofs by Joram Soch and collaborators (it's an open book on GitHub) Beyond Multiple Linear Regression: Applied Generalized Linear Models and Multilevel Models in R by Paul Roback and Julie Legler Ross, S. (2021) Introduction to Probability and Statistics for Engineers and Scientists , Sixth edition.; Elsevier, 2021. Neat. Particularly nice discussion on the confidence interval. Get the sixth edition, especially if you're buying in India. It's excellent and relatively cheap (~INR. 800, August 2025). Also, this edition provides code examples in R, a welcome change. A User’s Guide to Statistical Inference and Regression by Matthew Blackwell A placeholder note for both topics. Undergraduate Probability and High-Dimensional Probability by Roman Vershynin, on YouTube."
  },
  {
    "title": "Phase diagrams",
    "url": "https://arghyadutta.github.io/notebooks/phaseDiagram.html",
    "content": "I came to know about protein's interesting phase diagrams from a YouTube course by Ali Hassanali (ICTP). For some examples, see Hawley (1971), Asherie (2004), and Gasic et al. (2019). Gasic, A. G.; Boob, M. M.; Prigozhin, M. B.; Homouz, D.; Daugherty, C. M.; Gruebele, M.; Cheung, M. S. Critical Phenomena in the Temperature-Pressure-Crowding Phase Diagram of a Protein . Phys. Rev. X 2019, 9 (4), 041035 . Hawley, S. A. Reversible Pressure-Temperature Denaturation of Chymotrypsinogen . Biochemistry 1971, 10 (13), 2436–2442 . Protein $P–T$ phase diagrams Intriguing and informative! Here is a rough drawing of a few. Asherie, N. Protein Crystallization and Phase Diagrams . Methods 2004, 34 (3), 266–272 ."
  },
  {
    "title": "Teaching",
    "url": "https://arghyadutta.github.io/notebooks/teaching.html",
    "content": "Recommended Su, F. (2013). The Lesson of Grace in Teaching . His blog (2013) \"Your accomplishments are NOT what make you a worthy human being. You learn this lesson by receiving GRACE: good things you didn't earn or deserve, but you're getting them anyway.\" Hoare, E. (2025). Gentleness in Academia . Plough ."
  },
  {
    "title": "Stoicism",
    "url": "https://arghyadutta.github.io/notebooks/stoicism.html",
    "content": "If I get time Another run-of-the-mill, cliché-filled self-help book written to optimize every second of your life. Don't buy it. Recommended Long, A. A. (2004). Epictetus: A Stoic and Socratic Guide to Life (1st edition). Clarendon Press. Seneca, A. L. (2000). Letters From A Stoic (Reprint edition). Penguin UK. Farnsworth is a careful writer, and he lays down the main tenets of Stoicism with ample quotes and nuanced analysis. Not recommended Holiday, R. (2016). The Daily Stoic: 366 Meditations on Wisdom, Perseverance, and the Art of Living. Profile Books. Farnsworth, W. (2018). The Practicing Stoic: A Philosophical User’s Manual . David R. Godine, Publisher. Epictetus. (2018). How to Be Free: An Ancient Guide to the Stoic Life (A. Long, Trans.; Early Printing edition). Princeton University Press."
  },
  {
    "title": "Melancholia and Grief",
    "url": "https://arghyadutta.github.io/notebooks/melancholia.html",
    "content": "Shenoda, S. (2025). It's Just Stuff What Estate Sales Reveal About Us . Plough \"Everybody should know what it is to have friends like these. Everybody should know what it is to be loved like this.\" Astonishingly beautiful and tender recollection of a friend. Recommended Smith, N. (2024), Why rabbits? Towards a better, floofier world , Noahpinion Father by Ted Kooser Read it Sometimes you have to take your own hand as though you were a lost child and bring yourself stumbling home over twisted ice. Whiteness drifts over your house. A page of warm light falls steady from the open door. Here is your bed, folded open. Lie down, lie down, let the blue snow cover you. (from \"Original Fire: Selected and New Poems\", Harper Perennial, 2004) Grief , Louise Erdrich Crosley, S. (2024), The Tail End, What we lose when we lose a pet , The New Yorker . \"Whether due to a downsizing, divorce, or death, an estate sale is a sort of liminal space – a passing of the tools and accumulated flotsam of a life onto descendants who cannot always bear the added weight and so jettison it.\" \"People often ask me: “Why rabbits?” Usually my answer is just “They’re floofy.” Heartfelt reminiscence. Polk, E. (2024), Peregrinations of grief , Aeon ."
  },
  {
    "title": "Mathematical Methods in Physics: Resources",
    "url": "https://arghyadutta.github.io/notebooks/mathematicalMethods.html",
    "content": "Index to Catalogue of Lattices Recommended Visual group theory Excellent book and the author has made it freely available ! Garg, A. (2023) Mathematics with a Scientific Sensibility ; Northwestern University. Visualizing prime factors Scholarpedia Affine transformations Short, insightful notes by Markus Deserno The World of Mathematical Equations Balakrishnan, V. (2020). Mathematical Physics: Applications and Problems . Springer International Publishing. Special functions"
  },
  {
    "title": "R Programming",
    "url": "https://arghyadutta.github.io/notebooks/rProgramming.html",
    "content": "If I get time Long, J., & Teetor, P. (2019). R Cookbook: Proven Recipes for Data Analysis, Statistics, and Graphics (2nd ed. edition). O’Reilly Media."
  },
  {
    "title": "Academic Life: Features",
    "url": "https://arghyadutta.github.io/notebooks/academiaFeatures.html",
    "content": "In stead of discussing how society is systematically undermining—and mocking—any vestige of scholarly life that's still there, let me just point to some articles and books that show why scholarship is still a worthy vocation. Gadagkar, R. (2012). The Luxury of Introspection . In Uber das Kolleg hinaus (pp.152–157). Wissenschaftskolleg zu Berlin, Germany. PDF . On how scientific methods help us to make sense of the world. Recommended Marder, E. (2013). Grandmother elephants . eLife, 2, e01140 . Delightful essay on a scholar and his notes. Thomas, K. (2010). Working Methods . London Review of Books Hitz, Z. (2023, September 1). What Is Time For? Plough On learning from the older generation of scientists. Marder is a brilliant scientist and a perceptive writer—read her work. Hitz, Z. (2020). Lost in Thought: The Hidden Pleasures of an Intellectual Life (First Edition). Princeton University Press. Mermin, N. D. (1990). Commencement address at St. John's College, Santa Fe in Boojums all the way through: Communicating science in a prosaic age. Cambridge University Press."
  },
  {
    "title": "Visualization",
    "url": "https://arghyadutta.github.io/notebooks/visualization.html",
    "content": "Servier Medical ART Recommended Muth, L. C. Your Friendly Guide to Colors in Data Visualisation . 2018, Her Blog Gephi Veusz 2D The Elements of Visual Grammar: A Designer's Guide for Writers, Scholars, and Professionals by Angela Riechers WashU Epigenome Browser HiGlass Inkscape tutorials by TJ Free Useful Python package. Contains an extensive set of diverging, sequential, and qualitative colormaps. Very useful. I wrote few simple codes to create decent plots using python. Figma Software Tools Inkscape tutorial for beginners (really good) Canva Fundamentals of Data Visualization by Claus O. Wilke Got this link from Hacker News when I posted a Nature Communication paper The misuse of colour in science communication there.\n        This webpage contains color gradient files, and importing them in matplotlib is possible using packages like getcpt-master . Information is beautiful Data to Viz BioRender HiPlot Gosling ggcoverage LearnFree (Director). (2016, October 5). Beginning Graphic Design: Color [Video recording] . Inkscape explained by Logos by Nick If I get time cpt-city: An archive of color gradients Fundamentals of Data Visualization Useful Python package. Contains some new colormaps; I don't like them that much but they are interesting. SciDraw cmasher Plotly Websites for drawing flowcharts etc. from text Palettable"
  },
  {
    "title": "Open Source: Ideas and Tools",
    "url": "https://arghyadutta.github.io/notebooks/openSource.html",
    "content": "Writings and rehearsals by Nathan Schneider If I Get Time Find out publishers' copyright policies. Open policy finder Simard, M.-A., Butler, L.-A., Alperin, J. P., & Haustein, S. (2024). We need to rethink the way we identify diamond open access journals in quantitative science studies . Quantitative Science Studies, 5(4), 1042–1046 . Interesting work regarding free and open source software"
  },
  {
    "title": "Dimensional Analysis",
    "url": "https://arghyadutta.github.io/notebooks/dimensionalAnalysis.html",
    "content": "Lemons, D. S. (2017). A Student’s Guide to Dimensional Analysis (1st ed.). Cambridge University Press. Chapter 1 contains a short but useful discussion. Recommended Morin, D. J. (2008). Introduction to classical mechanics: With problems and solutions . Cambridge university press. McKinley, G. H. (2024). Getting the (dimensionless) numbers right . Nature Chemical Engineering, 1(1), Article 1 . Anything Lemons writes is well-written; this is not an exception."
  },
  {
    "title": "Academic Biographies",
    "url": "https://arghyadutta.github.io/notebooks/biography.html",
    "content": "Nasar, S., & Gruber, D. (2006, August 28). Manifold Destiny . The New Yorker . Gaillard, M. K. (2015). A singularly unfeminine profession: One woman’s journey in physics . World Scientific Publishing Company. Recommended Story of an unusual mathematician, Grigory Perelman. One of the best profiles that I've read in The New Yorker. A playlist with an extended interview of an intriguing academic: Freeman Dyson. More on it As a physicist, it is easy—very easy—to suffer from mild to severe imposter syndrome. Physics is full of brilliant people, often leading one to feel like the only person who doesn’t understand an otherwise “simple” idea that everyone else seems to have seamlessly mastered. I read these biographies as a cure for these bouts of doubt. For me, they often show how exhilarating is the joy of understanding a natural phenomenon; the reason we started doing physics in the first place. This simple joy often gets overlooked when you have to apply for your next postdoc position, next faculty position, or apply for grants. Also, these books show how developing any substantial idea can be very time-consuming, in no sense straightforward, and taxing. This book is exceptionally well-written from both of these perspectives. Mary K. Gaillard is Berkeley's first tenured female faculty. Getting that position as a male physicist was not easy, as a female physicist it was close to impossible. She made it possible. And yet the book's tone is not accusatory; she is not into the blame game. She never said anything about why she and her first husband were divorced; she simply said it was very personal and did not think it was relevant for the aspiring female physicists, her target audience. She also was brutally honest about many incidents that were demeaning to her in many ways; she frequently quoted big names saying those horrible things to her. But when she turned out to be wrong, she admitted. The frankness is admirable. If you want to read a book about a physicist who did brilliant work, made her way through the jungle of academia to the top, and fought against daily, casual sexism from her colleagues, friends, and family members, and yet somehow managed not to hate them, read this book. You won't be disappointed. Ray, S., Spangenburg, R., & Moser, D. K. (1995). Niels Bohr: Gentle genius of Denmark . Facts on File. As a physicist, it is easy—very easy—to suffer from mild to severe imposter syndrome. Physics is full of brilliant people, often leading one to feel like the only person who doesn’t understand an otherwise “simple” idea that everyone else seems to have seamlessly mastered. I read these biographies as a cure for these bouts of doubt. For me, they often show how exhilarating is the joy of understanding a natural phenomenon; the reason we started doing physics in the first place. This simple joy often gets overlooked when you have to apply for your next postdoc position, next faculty position, or apply for grants. Also, these books show how developing any substantial idea can be very time-consuming, in no sense straightforward, and taxing. Ulam, S. M. (1991). Adventures of a Mathematician . University of California Press. Archive A short and delightful book by Pierre-Gilles de Gennes, a French Nobel laureate in Physics. He had a distinctive research style. He could, somehow , distill essential insights from complex physical phenomena, and then present them with minimal equations and clear, concise prose. Mlodinow, L. (2003). Feynman’s Rainbow: A Search for Beauty in Physics and in Life . Warner Books. Gennes, P.-G. de. (2004). Petit point: A candid portrait on the aberrations of science . World Scientific Publications Hoad, P. (2024). ‘He was in mystic delirium’: was this hermit mathematician a forgotten genius whose ideas could transform AI – or a lonely madman? . Phil Hoad; Guardian A nice introduction to the life and times of the physicist Niels Bohr. This particular book, however, is not about physics. It is written as fables about imaginary physicists navigating an all-too-real world. The stories are thought-provoking, controversial, and candid. Having spent a fair amount of time in academia, I know how true most of them are. I recommend it to anyone interested in the social dynamics of scientists. A series where Rob Phillips is interviewed by David Zierler for the Caltech Heritage project. Inspiring and bold! Polchinski, J. (2017). Memories of a Theoretical Physicist . arXiv:1708.09093 Halmos, P. R. (1985). I Want to be a Mathematician . Springer New York. And yet the book's tone is not accusatory; she is not into the blame game. She never said anything about why she and her first husband were divorced; she simply said it was very personal and did not think it was relevant for the aspiring female physicists, her target audience. She also was brutally honest about many incidents that were demeaning to her in many ways; she frequently quoted big names saying those horrible things to her. But when she turned out to be wrong, she admitted. The frankness is admirable. Doxiadēs, A. K. (2001). Uncle Petros and Goldbach’s Conjecture: A Novel of Mathematical Obsession . Bloomsbury USA. This book is exceptionally well-written from both of these perspectives. Mary K. Gaillard is Berkeley's first tenured female faculty. Getting that position as a male physicist was not easy, as a female physicist it was close to impossible. She made it possible. The end life of Grothendieck. If I get time If you want to read a book about a physicist who did brilliant work, made her way through the jungle of academia to the top, and fought against daily, casual sexism from her colleagues, friends, and family members, and yet somehow managed not to hate them, read this book. You won't be disappointed. Frisch, O. R. (1979). What little I remember . Cambridge University Press. Archive More on it A short and delightful book by Pierre-Gilles de Gennes, a French Nobel laureate in Physics. He had a distinctive research style. He could, somehow , distill essential insights from complex physical phenomena, and then present them with minimal equations and clear, concise prose. This particular book, however, is not about physics. It is written as fables about imaginary physicists navigating an all-too-real world. The stories are thought-provoking, controversial, and candid. Having spent a fair amount of time in academia, I know how true most of them are. I recommend it to anyone interested in the social dynamics of scientists. Balaram P. (2017) Memories of a Bangalore Quartet . IISc Connet"
  },
  {
    "title": "Quantum Information",
    "url": "https://arghyadutta.github.io/notebooks/qInfo.html",
    "content": "Recommended See also Basic Mathematics for Quantum Mechanics Quantum Computation Lecture Notes (Caltech CS219) by John Preskill"
  },
  {
    "title": "Indian history",
    "url": "https://arghyadutta.github.io/notebooks/india.html",
    "content": "More It focuses on the political history of Bengal since the time of Mahabharata and to the 1970s. Yes a bit too broad, but the author has done a good job in documenting the major political upheavals. I liked a nuanced discussion on the gradual deterioration of Hindu–Muslim relations, starting 1900. Though, at times, the writing is dry and descriptive, it mostly reads well. One issue: a history book should have some decent maps and charts delineating the successions of kings; it lacks those. More Candid, witty, and sometimes sarcastic, the book starts with a take on colonial India. It then moves into the author’s experiences working as a historian—in the National Archives, at Delhi University, and later at Oxford. A great pick if you're looking for something that's both fun to read and full of insight into India and how an Indian sees Europe. There's also an abridged English translation called \"The World in Our Time: A Memoir\". Recommended If I Get Time Sengupta, N. K. Land of Two Rivers: A History of Bengal from the Mahabharata to Mujib ; Penguin Books: New Delhi, 2011. Sastri, K. A. N.; Gurukkal, P. M. R.; Champakalakshmi, R. The Illustrated History of South India ; Oxford: New Delhi, 2009. More A well-researched and engaging read. The author’s sincerity comes through—she writes with empathy for the people living by the Indus, offering many thoughtful insights. I didn’t always agree with her, especially in her somewhat cautious stance on religion, but overall, I’d recommend the book. Albinia, A. Empires of the Indus: The Story of a River ; John Murray: London, 2008. It focuses on the political history of Bengal since the time of Mahabharata and to the 1970s. Yes a bit too broad, but the author has done a good job in documenting the major political upheavals. I liked a nuanced discussion on the gradual deterioration of Hindu–Muslim relations, starting 1900. Though, at times, the writing is dry and descriptive, it mostly reads well. One issue: a history book should have some decent maps and charts delineating the successions of kings; it lacks those. Roychowdhury, T. Bangalnama | বাঙালনামা; Ananda Publishers."
  },
  {
    "title": "Biophysics",
    "url": "https://arghyadutta.github.io/notebooks/biophysics.html",
    "content": "Molecular Biophysics by Erik Lindahl. Molecular biophysics by M. V. Volkenshtein Physical Biology of the Cell by Rob Phillips. Protein physics by Finkelstein and Ptitsyn Modelling dynamic phenomena in molecular and cellular biology by Lee Segel Books Introduction to Proteomics . Courses on YouTube Introduction to Neuroscience by Bing Wen Brunton. Theory of the stability of lyophobic colloids by Verwey and Overbeek An introductory biophysics course by Ali Hassanali. Biophysics by Walter Hoppe, Wolfgang Lohmann, Hubert Markl, Hubert Ziegler Tutorials on PyMOL from Molecular Memory. Ionic Solution Theory by H. Friedman and I. Prigogine"
  },
  {
    "title": "Statistical Mechanics: Few introductory books and papers",
    "url": "https://arghyadutta.github.io/notebooks/statMechCourse.html",
    "content": "Styer, D. Entropy as Disorder: History of a Misconception . The Physics Teacher 2019, 57 (7), 454–458 . Baez, J. C. What Is Entropy? arXiv September 13, 2024 . Books Other reading materials Much more detailed than the ones listed so far. Bhattacharjee, J. K. Entropy à La Boltzmann . Resonance 2001, 6 (9), 19–34 . Böttcher's and Kennett's are two beginner friendly, undergrad-level recent books covering computational and theoretical aspects of statistical mechanics, respectively. Bhattacharjee, S. M. Entropy and Perpetual Computers . arXiv October 29, 2003 . Kennett, M. P. (2020) Essential Statistical Physics , 1st ed.; Cambridge University Press Entropy Some books and papers that I recommend in my statistical mechanics courses at undergraduate level. For more advanced references, see Statistical Mechanics: Resources . Böttcher, L.; Herrmann, H. J. (2021) Computational Statistical Physics , Cambridge University Press Reif, F. Statistical Physics: Berkeley Physics Course, Vol. 5 ; Mcgraw-Hill College, 1967."
  },
  {
    "title": "Art",
    "url": "https://arghyadutta.github.io/notebooks/art.html",
    "content": "Favorite artists Geometric patterns on sand. Meditative, ephemeral. Gregory Fromenteau Bentley, T. (2017, July 15). The Obsessive Art and Great Confession of Charlotte Salomon . The New Yorker . Jamie Windsor. (2019, October 29). Wabi-Sabi: When Bad Photos Are Better YouTube I haven't seen anything quite like Paske's Surreal, melancholic, otherworldly art. Charlotte Salomon Ernst Barlach Surreal, slow. Xuan Loc Xuan Modern vietnamese artist.I love the way she uses colors and patterns. The Drawing Database-Northern Kentucky University (Director). (2020, August 18). Art History & Drawing: 15 Minutes with Kathe Kollwitz YouTube Victoria and Albert Museum (Director). (2018, June 6). In Search of Forgotten Colours—Sachio Yoshioka and the Art of Natural Dyeing YouTube Design LearnFree (Director). (2016, October 5). Beginning Graphic Design: Color . YouTube Jeannie Lynn Paske Some of his works Yuki Kawae Colors Józef Wilkoń Wonderful. Norman, D. (2013). The Design of Everyday Things (2nd edition). Basic Books. Käthe Kollwitz Nan Goldin Mangla, R. (2015, June 8). A Brief History of Ultramarine—The World’s Costliest Color . The Paris Review . How to design things—doors, software interfaces, teapots, switches—so that people find them delightful to use?"
  },
  {
    "title": "Loschmidt Paradox",
    "url": "https://arghyadutta.github.io/notebooks/loschmidtParadox.html",
    "content": "Moreover, all the microstates with same total energy can be found with equal probability. As a concrete example, the number of microstate of a system with $N$ particles is proportional to $V^N$. So if you halve the volume, the available microstates decrease by $1/2^N$. So, in the thermodynamic limit, the probability of a gas spontaneously collapsing to half of its volume is $1/2^N \\rightarrow 0$. Swendsen, R. H. (2008). Explaining irreversibility . American Journal of Physics, 76(7), 643–648 . Recommended An overwhelmingly large number of microstates are compatible with a macrostate, specified by a few state quantities like pressure, volume, and temperature. That means the initial state typically evolves to one of numerous possible microstates which cannot be distinguished macroscopically. Consider the time evolution of a system $N$ particles. In the $6N$-dimensional phase space, a point $(q_i, p_i)\\equiv(q_1,p_1,q_2,p_2,\\cdots,q_{3N},p_{3N})$ denote one microstate. Loschmidt argued that for a system starting from $(q_i(t_0),p_i(t_0))$ and evolving to $(q_i(t_f),p_f(t_f))$, if we reverse the momenta of all particles at $t_f$—i.e., if we can prepare a system described by $(q_i(t_0),-p_i(t_0))$—then the system should evolve back to the initial state at a later time because the laws of classical mechanics are time reversible. If true, this implies the amazing phenomenon of a gas spontaneously returning to a smaller volume from a larger volume. Boltzmann solved this paradox by noting that: That means the exactly one final state that allows the system to go back to the initial state is the actual final state can occur with a miniscule probability. Greiner, W., Neise, L., & Stöcker, H. (1995). Thermodynamics and Statistical Mechanics. Springer. (pp. 43–46)"
  },
  {
    "title": "LaTeX",
    "url": "https://arghyadutta.github.io/notebooks/latex.html",
    "content": "Detexify Recommended Online BibTex tidy A no-frills LaTeX format Cleaning up BibTex files Overleaf's short tutorial is useful. Draw a symbol to get its LaTeX command Create LaTeX tables from data"
  },
  {
    "title": "Fiction",
    "url": "https://arghyadutta.github.io/notebooks/fiction.html",
    "content": "নিঃসন্দেহে আমার পড়া অন্যতম শ্রেষ্ঠ কিশোরসাহিত্য। Wood, J. (2013), Becoming them , The New Yorker আমি গ্রামের ছেলে, কিন্তু সত্যিকারের ভালো একটা কিশোরসাহিত্য যে গ্রাম-শহরের সঙ্কীর্ণ ভূগোলের সীমা হেলায় অতিক্রম করে যেতে পারে তার প্রমাণ দেয় এই বই: খাস কোলকাতা শহরের চার কিশোরের adventure (এবং, আর বেশি misadventure) গুলো থেকে মজা পেতে আমার কোন অসুবিধা হয় নি। পাঁচখানা ধুতি, সাতখানা শাড়ি এ-সব হিসেবে হইবে কিবা? এ-জগতে জীব কত ব্যাথা পায়, তাই ভাবি আমি রাত্রি-দিবা। রামধনের ওই বৃদ্ধ গাধা মনটি তাহার বড়ই সাদা- সে-বেচারা তার পিঠেতে চাপায়ে কত শাড়ি-ধুতি-প্যান্ট লইয়া যায়- মনোদুখে খালি বোঝা টেনে ফেরে গাধা একখানা ধুতি-প্যান্ট পরিতে না পায়! হেমেন্দ্রকুমার রচনাবলীর এই খণ্ডটা পড়ে আমার বেশ ভালো লাগল। এর মধ্যে প্রথম তিনটি উপন্যাস রয়েছে ডাকাত দীনবন্ধুকে নিয়ে। দীনবন্ধু এমন একজন ডাকাত যে, সৌভাগ্যবশত, নিজের নামের মানে বোঝে; তাই তার করা ডাকাতিকে মেনে নিতে লেখক বা পাঠক কারোরই অসুবিধা হয় না। দীনবন্ধুকে ধরার ব্যর্থ চেষ্টা করে বেরায় গোয়েন্দা প্রশান্তবাবু, যাকে বুদ্ধিমান দীনবন্ধু ডাকে অশান্তবাবু বলে। এই দুজনকে নিয়ে লেখা গল্পগুলো বেশ সুখপাঠ্য। বইয়ের শেষ দুটি উপন্যাস জয়ন্ত, মানিক ও সুন্দরবাবুকে নিয়ে লেখা। এই দুটিতে রক্তপাতের কিছু বাড়াবাড়ি থাকলেও তা মাত্রা ছাড়িয়ে যায় না — তাই সব মিলিয়ে বইটা পড়তে বেশ ভালই লাগে। আজকালকার ছেলেমেয়েরা পড়লে তাদের খারাপ লাগবে বলে মনে হয় না। Recommended Gangopadhyay, N. (2014). Samagra Kishor Sahitya (1st edition). Ananda Publishers. (Bengali) …কিন্তু পিসিমা ধোপার হিসেবের খাতায় এইসব দেখে ভীষণ রেগে গেল! রেগে গিয়ে হাতের কাছে আর কিছু না পেয়ে একটা চালকুমড়ো নিয়ে ফুচুদাকে তাড়া করলে। ঠিক যেন গদা হাতে নিয়ে শাড়িপরা ভীম দৌড়চ্ছে। হেমেন্দ্র রায়ের লেখা বই গুলো একটানা পড়লে ভালো লাগে না, অন্তত আমার। তাই ওনার বইগুলো একটু রয়েসয়ে পড়ি। এই বইতে পাওয়া নারায়ণ গঙ্গোপাধ্যায়ের অধিকাংশ গল্পগুলিই নির্ভেজাল হাসির, অনাবিল আনন্দের উৎস: ক্রিকেট খেলতে গিয়ে দেরি করে বাড়ি ফিরে মায়ের বকুনি খাওয়া, কিংবা জামরুল পারতে গিয়ে কাঠপিঁপড়ের কামড় খাওয়ার মত ছোটবেলার বিভিন্ন স্মৃতির সাথে এই বইটা পড়ার সুখস্মৃতি এমনভাবে জড়িয়ে আছে যে আজ আর আমার ছোটবেলাকে এই বইটার থেকে আলাদা করে ভাবতে পারি না। টেনিদার সাথে আমার প্রথম পরিচয় কোন একটা ক্লাসে প্রথম হবার দৌলতে দিদার উপহার দেওয়া টেনিদা সমগ্রর মাধ্যমে। More হেমেন্দ্র রায়ের লেখা বই গুলো একটানা পড়লে ভালো লাগে না, অন্তত আমার। তাই ওনার বইগুলো একটু রয়েসয়ে পড়ি। হেমেন্দ্রকুমার রচনাবলীর এই খণ্ডটা পড়ে আমার বেশ ভালো লাগল। এর মধ্যে প্রথম তিনটি উপন্যাস রয়েছে ডাকাত দীনবন্ধুকে নিয়ে। দীনবন্ধু এমন একজন ডাকাত যে, সৌভাগ্যবশত, নিজের নামের মানে বোঝে; তাই তার করা ডাকাতিকে মেনে নিতে লেখক বা পাঠক কারোরই অসুবিধা হয় না। দীনবন্ধুকে ধরার ব্যর্থ চেষ্টা করে বেরায় গোয়েন্দা প্রশান্তবাবু, যাকে বুদ্ধিমান দীনবন্ধু ডাকে অশান্তবাবু বলে। এই দুজনকে নিয়ে লেখা গল্পগুলো বেশ সুখপাঠ্য। বইয়ের শেষ দুটি উপন্যাস জয়ন্ত, মানিক ও সুন্দরবাবুকে নিয়ে লেখা। এই দুটিতে রক্তপাতের কিছু বাড়াবাড়ি থাকলেও তা মাত্রা ছাড়িয়ে যায় না — তাই সব মিলিয়ে বইটা পড়তে বেশ ভালই লাগে। আজকালকার ছেলেমেয়েরা পড়লে তাদের খারাপ লাগবে বলে মনে হয় না। Krystal, A. (2022), What’s the Deal, Hummingbird? , The New Yorker তাই এই বই ক্লাসে প্রথম হবার দৌড়ে ক্লান্ত আজকের কিশোর-কিশোরীদের অবশ্যপাঠ্য। আর মজা ছাড়াও এই বই থেকে বাস্তব-জীবনে কাজে লাগানোর মত অনেক কিছু শেখা যায়। একটি নমুনা: More on it নিঃসন্দেহে আমার পড়া অন্যতম শ্রেষ্ঠ কিশোরসাহিত্য। ক্রিকেট খেলতে গিয়ে দেরি করে বাড়ি ফিরে মায়ের বকুনি খাওয়া, কিংবা জামরুল পারতে গিয়ে কাঠপিঁপড়ের কামড় খাওয়ার মত ছোটবেলার বিভিন্ন স্মৃতির সাথে এই বইটা পড়ার সুখস্মৃতি এমনভাবে জড়িয়ে আছে যে আজ আর আমার ছোটবেলাকে এই বইটার থেকে আলাদা করে ভাবতে পারি না। টেনিদার সাথে আমার প্রথম পরিচয় কোন একটা ক্লাসে প্রথম হবার দৌলতে দিদার উপহার দেওয়া টেনিদা সমগ্রর মাধ্যমে। আমি গ্রামের ছেলে, কিন্তু সত্যিকারের ভালো একটা কিশোরসাহিত্য যে গ্রাম-শহরের সঙ্কীর্ণ ভূগোলের সীমা হেলায় অতিক্রম করে যেতে পারে তার প্রমাণ দেয় এই বই: খাস কোলকাতা শহরের চার কিশোরের adventure (এবং, আর বেশি misadventure) গুলো থেকে মজা পেতে আমার কোন অসুবিধা হয় নি। এই বইতে পাওয়া নারায়ণ গঙ্গোপাধ্যায়ের অধিকাংশ গল্পগুলিই নির্ভেজাল হাসির, অনাবিল আনন্দের উৎস: ...টেনিদা তবু হাঁড়িটাকে ছাড়ে না। শেষকালে মুখের ওপর তুলে চোঁ করে রসটা পর্যন্ত নিকেশ করে দিলে। তারপর নাক-টাক কুঁচকে বললে, দুত্তোর, গোটাকয়েক ডেয়ো পিঁপড়েও খেয়ে ফেললুম রে! জ্যান্তও ছিল দু’-তিনটে! পেটের ভেতরে গিয়ে কামড়াবে না তো? হাবুল বললে, কামড়াইতেও পারে। কামড়াক গে, বয়ে গেল! একবার ভীমরুল-সুদ্ধ একটা জামরুল খেয়ে ফেলেছিলুম, তা সে-ই যখন কিছু করতে পারলে না, তখন ক’টা পিঁপড়েতে আর কী করবে! তাই এই বই ক্লাসে প্রথম হবার দৌড়ে ক্লান্ত আজকের কিশোর-কিশোরীদের অবশ্যপাঠ্য। আর মজা ছাড়াও এই বই থেকে বাস্তব-জীবনে কাজে লাগানোর মত অনেক কিছু শেখা যায়। একটি নমুনা: হুঁ, কবি হওয়া খুব খারাপ। আমার পিসতুতো ভাই ফুচুদা একবার কবি হয়েছিলো। দিনরাত কবিতা লিখত। একদিন রামধন ধোপার খাতায় কবিতা করে লিখল : পাঁচখানা ধুতি, সাতখানা শাড়ি এ-সব হিসেবে হইবে কিবা? এ-জগতে জীব কত ব্যাথা পায়, তাই ভাবি আমি রাত্রি-দিবা। রামধনের ওই বৃদ্ধ গাধা মনটি তাহার বড়ই সাদা- সে-বেচারা তার পিঠেতে চাপায়ে কত শাড়ি-ধুতি-প্যান্ট লইয়া যায়- মনোদুখে খালি বোঝা টেনে ফেরে গাধা একখানা ধুতি-প্যান্ট পরিতে না পায়! …কিন্তু পিসিমা ধোপার হিসেবের খাতায় এইসব দেখে ভীষণ রেগে গেল! রেগে গিয়ে হাতের কাছে আর কিছু না পেয়ে একটা চালকুমড়ো নিয়ে ফুচুদাকে তাড়া করলে। ঠিক যেন গদা হাতে নিয়ে শাড়িপরা ভীম দৌড়চ্ছে। তাই আর দেরি না করে বইটা পড়ে ফেলুন আর যদি পড়া হয়ে গিয়ে থাকে তো আরও একবার পড়ে নিন; প্রাণখোলা হাসি আপনাকে সুস্থ রাখবে। হেমেন্দ্রকুমার রায় রচনাবলী (১৭)"
  },
  {
    "title": "Title",
    "url": "https://arghyadutta.github.io/notebooks/template.html",
    "content": ""
  },
  {
    "title": "Artificial Intelligence",
    "url": "https://arghyadutta.github.io/notebooks/ai.html",
    "content": "Pei Wang's A Gentle Introduction to AGI . Rothman, J. (2025, April 29). Why Even Try if You Have A.I.? The New Yorker . Rothman, J. (2023, November 13). Why the Godfather of A.I. Fears What He’s Built . The New Yorker . The Limits of AI by Hubert Dreyfus (1985) Excellent and up-to-date broad overview of AI. Geoffrey Hinton's profile by the New Yorker. Useful read, despite the almost click-bait title. Provocative but thoughtful, as usual from Chomsky. Dreyfus lays down the core objections against AGI. Chayka, K. (2025, April 2). The Limits of A.I.-Generated Miyazaki . The New Yorker . Burnett, D. G. (2025, April 26). Will the Humanities Survive Artificial Intelligence? The New Yorker . Pins down issues with the methods and performance metrics. Chomsky, N., Roberts, I., & Watumull, J. (2023, March 8). The False Promise of ChatGPT . The New York Times . Gijsbers, V. (2025) Rise of the Deception Machines Issues of AI Partial list, reflecting my annoyance with the ongoing hype about AGI. A neat and well-argued metaphor. Compelling and persuasive article against AGI, often elaborating on Dreyfus's ideas. LLMs affected—destroyed—the learning process of young students in a way I wouldn't have believed was possible—until I began teaching. Rothman offers an alternative approach. Broad discussion with well-chosen pointers for further reading. Fjelland, R. (2020). Why general artificial intelligence will not be realized . Humanities and Social Sciences Communications, 7(1), 10 . Chiang, T. (2023, February 9). ChatGPT Is a Blurry JPEG of the Web . The New Yorker . Müller, V. C. (2023). Ethics of Artificial Intelligence and Robotics . In E. N. Zalta & U. Nodelman (Eds.), The Stanford Encyclopedia of Philosophy Crockett, MJ. (2025, February 27). AI is ‘beating’ humans at empathy and creativity. But these games are rigged . The Guardian . Heitzinger, C., & Woltran, S. (2024). A Short Introduction to Artificial Intelligence: Methods, Success Stories, and Current Limitations . Introduction to Digital Humanism: A Textbook (pp. 135–149). Springer Nature Switzerland. Artificial General Intelligence (AGI)"
  },
  {
    "title": "Excel keyboard shortcuts for Mac",
    "url": "https://arghyadutta.github.io/notebooks/excelShortcuts.html",
    "content": "macOS vs Excel Conflicts Formatting Selection Function Key Shortcuts Navigation Editing & Entering Why you may not want to use this: It's ChatGPT-generated content! Other Useful Shortcuts I am not a fan of Excel, but also can't avoid it at work. So, I made this cheat-sheet of sorts using ChatGPT. I haven't tested most of them. I plan to use and edit it as needed. (Last updated: 2025-08-25, Monday) Editing Aids"
  },
  {
    "title": "Logic",
    "url": "https://arghyadutta.github.io/notebooks/logic.html",
    "content": "Recommended Weston, A. A Rulebook for Arguments , Fifth edition.; Hackett Publishing Company, Inc: Indianapolis; Cambridge, 2017. If I Get Time Almossawi, A. An Illustrated Book of Bad Arguments , Illustrated edition. Experiment, 2014. ( Available online ) Herbert, E. A Mathematical Introduction to Logic , 2nd edition.; Elsevier India, 2014 Halbach, V. The Logic Manual ; Oxford University Press: Oxford; New York, 2010."
  },
  {
    "title": "Food and Drink",
    "url": "https://arghyadutta.github.io/notebooks/foodNDrink.html",
    "content": "The Complete Guide to Sugar Around the World by Craig Cavallo A nice blog on Konkani recipes. Few interesting videos and articles. \"I was once told by a grandmother, who lives in my seaside village but grew up in the northern dry regions where the Senegal River winds across a crispy and prickly savanna not far from the great desert, about a watermelon varietal called beref, which is cultivated mostly for its seeds but also serves as a kind of water reserve. “There’s water that you can drink from it like a coconut,” she said.\" 14 Types Of Rice And The Best Dishes To Use Them from Food Republic Apples on a scale from most tart to most sweet Lewis, J. Tell Me Why the Watermelon Grows. Switchyard \"Tenthly, one should pour tea into the cup first.\" And of course, the man had one more rule \"Lastly, tea…\". Orwell, G. (1946) A Nice Cup of Tea . Evening Standard Advanced Coffee Making , Lecture at Assembly Coffee London How to make Chocolate"
  },
  {
    "title": "Bayesian Methods",
    "url": "https://arghyadutta.github.io/notebooks/bayesian.html",
    "content": "Eddy, S. R. What Is Bayesian Statistics? Biotechnol 2004, 22 (9), 1177–1178 . Online Resources Statistics for application by Phillippe Rigollet Recommended The most beginner friendly introduction to Bayesian statistics that I've came across. Ma, W. J.; Kording, K.; Goldreich, D. Bayesian Models of Perception and Action: An Introduction ; The MIT Press: London, England, 2023 Titelbaum, M. G. How to Think like a Bayesian . Psyche. 2024 . (accessed 2024-01-29). van de Schoot, R.; Depaoli, S.; King, R.; Kramer, B.; Märtens, K.; Tadesse, M. G.; Vannucci, M.; Gelman, A.; Veen, D.; Willemsen, J.; Yau, C. Bayesian Statistics and Modelling . Nat Rev Methods Primers 2021, 1 (1), 1–26 . Neat description of Bayesian ideas (it's a neuroscience book). Bayesian methods course by Richard McElreath"
  },
  {
    "title": "Philosophy of Science",
    "url": "https://arghyadutta.github.io/notebooks/philosophyOfScience.html",
    "content": "If I get time Alan, C. (2013). What Is This Thing Called Science? McGraw-Hill Education (UK). Recommended Humphreys, P. (Ed.). (2019). The Oxford Handbook of Philosophy of Science (Reprint edition). Oxford University Press. Good description of of the main school of thoughts, especially the theories of Popper (falsification) and Kuhn (paradigm shift). Karl Popper - Science: Conjectures and Refutations by Victor Gijsbers. Courses on YouTube Philosophy of Science by Paul Hoyningen"
  },
  {
    "title": "Internet and Society",
    "url": "https://arghyadutta.github.io/notebooks/internetNUs.html",
    "content": "Tolentino, J. (2019). The I in the Internet . Trick mirror: Reflections on self-delusion (First edition). Random House.\n        ( Free online version ) \"On the internet, a highly functional person is one who can promise everything to an indefinitely increasing audience at all times.\""
  },
  {
    "title": "Sanskrit",
    "url": "https://arghyadutta.github.io/notebooks/sanskrit.html",
    "content": "Recommended Ruppel, A. M. The Cambridge Introduction to Sanskrit , 1st ed.; Cambridge University Press, 2017. https://doi.org/10.1017/9781107088283 ."
  },
  {
    "title": "Relativity: Special and General Theory",
    "url": "https://arghyadutta.github.io/notebooks/relativity.html",
    "content": "Guidry, M. (2019) Modern General Relativity: Black Holes, Gravitational Waves, and Cosmology ; Cambridge university press Really good introductory book. Especially the first few chapters introducing the idea of tensors are excellent. If I get time Coleman, S. (2021) Sidney Coleman’s Lectures on Relativity , ; Griffiths, D. J., Derbes, D., Sohn, R. B., Eds.; Cambridge University Press It should not be the first book, but it's well-written and full of insights. Recommended Schutz, B. F. (2009) A First Course in General Relativity , 2nd ed.; Cambridge University Press."
  },
  {
    "title": "Python programming",
    "url": "https://arghyadutta.github.io/notebooks/python.html",
    "content": "McKinney, W. Python for Data Analysis A Visual Intro to NumPy and Data Representation by Jay Alammar McKinney, W. (2022). Python for Data Analysis: Data Wrangling with pandas, NumPy, and Jupyter (3rd edition). O’Reilly Media. Recommended 5 Good Python Habits; 2024. YouTube (accessed 2025-08-22). Why Python is Slow: Looking Under the Hood by Jake Vanderplus NumPy Illustrated: The Visual Guide to NumPy by Lev Maximov Ramalho, L. (2022). Fluent Python: Clear, Concise, and Effective Programming , Second Edition (Second edition). O’Reilly."
  },
  {
    "title": "Irrationality and Stupidity",
    "url": "https://arghyadutta.github.io/notebooks/stupidity.html",
    "content": "Recommended If I Get Time Frankfurt, H. G. (2005). On Bullshit (1st edition). Princeton University Press. Bonhoeffer's Theory of Stupidity"
  },
  {
    "title": "Books and Libraries",
    "url": "https://arghyadutta.github.io/notebooks/bookNLibrary.html",
    "content": "Piper, A. (2012). Book was there: Reading in electronic times . University of Chicago Press. If I get time Eco is interesting, as usual. Meanwhile, Carrière: And anyway, what about the cultures that haven’t developed what we call philosophy? That’s what I meant just now by saying that anthropology is just as important. The notion of a ‘philosophical concept’, for instance, is a purely Western one. Try explaining ‘concept’ to an Indian—even a highly sophisticated one—or ‘transcendence’ to a Chinese person! (p. 233 in the eBook) Really, my man? (To be fair, he wrote a long French play based on the Indian epic Mahabharata, as he was \"completely enchanted\" by it. So, maybe he meant something else here? I don't know.) Is it okay to appreciate creations by imperfect human beings? Morton offers a nuanced perspective. Piper, A. (2009). Dreaming in books: The making of the bibliographic imagination in the Romantic age . University of Chicago press. Carrière, J.-C., & Eco, U. (2011). This is Not the End of the Book . Harvill Secker. Piper, A. (2018). Enumerations: Data and literary study . The University of Chicago Press. Morton, B. (2019, January 8). Virginia Woolf? Snob! Richard Wright? Sexist! Dostoyevsky? Anti-Semite! The New York Times ."
  },
  {
    "title": "Ehrenfest Theorem",
    "url": "https://arghyadutta.github.io/notebooks/ehrenfest.html",
    "content": "A. Messiah. Quantum Mechanics: Two Volumes Bound as One; Dover Publications, Inc: Garden City, New York, 2020. Recommended Another surprisingly thorough note from Nicholas Wheeler from Reed College (Yes, Griffiths is from Reed, too). Wheeler, N. Remarks Concerning the Status & Some Ramifications of Ehrenfest’s Theorem, 1998. PDF (accessed 2024-10-03). Chapters 5, 6 discusses commutators and Ehrenfest's equation."
  },
  {
    "title": "Quantum Mechanics via Moment Dynamics",
    "url": "https://arghyadutta.github.io/notebooks/dynamics.html",
    "content": "Ballentine, L. E., & McRae, S. M. (1998). Moment equations for probability distributions in classical and quantum mechanics . Physical Review A, 58(3), 1799–1809 . Recommended Biswas, S., Chattopadhyay, R., & Bhattacharjee, J. K. (2018). Propagation of arbitrary initial wave packets in a quantum parametric oscillator: Instability zones for higher order moments . Physics Letters A, 382(18), 1202–1206 Chawla, R., & Bhattacharjee, J. K. (2019). Quantum dynamics from fixed points and their stability . European Physical Journal B, 92(9), 196 . Sarkar, P., & Bhattacharjee, J. K. (2020). Nonlinear parametric oscillator: A tool for probing quantum fluctuations . Physics Review E, 102(5), 052204 . Wheeler, N. (1998). Remarks Concerning the Status & Some Ramifications of Ehrenfest’s Theorem . PDF Can we study a quintessential quantum phenomenon—such as tunneling—by analyzing dynamical equations of the moments of the quantum distribution? Sarkar, P., Chattopadhyay, R., & Bhattacharjee, J. K. (2024). Quantum dynamics of wave packets in a Morse potential: A dynamical system approach . Physical Review E, 110(3), 034207 . Ray, S., Bhattacharyya, S., & Bhattacharjee, J. K. (2024). Dynamical System Description of Quantum Tunneling in a Double Well Potential . Physics Letters A, 130174 . Neat. Also see other notes by Wheeler. (This is not John, but Nicholas Wheeler. A colleague of David Griffiths at the Reed college)"
  },
  {
    "title": "Few well-written papers",
    "url": "https://arghyadutta.github.io/notebooks/wellWrittenPapers.html",
    "content": "Bauer, M.; Bialek, W. Information Bottleneck in Molecular Sensing. PRX Life 2023, 1 (2), 023005. link . Deserno, M. Fluid Lipid Membranes – a Primer link Fjelland, R. Why General Artificial Intelligence Will Not Be Realized. Humanit Soc Sci Commun 2020, 7 (1), 10. link . Cubuk, J.; Soranno, A. Macromolecular Crowding and Intrinsically Disordered Proteins: A Polymer Physics Perspective. ChemSystemsChem 2022. link . Powers, A. S.; Pham, V.; Burger, W. A. C.; Thompson, G.; Laloudakis, Y.; Sexton, P. M.; Paul, S. M.; Christopoulos, A.; Thal, D. M.; Felder, C. C.; Valant, C.; Dror, R. O. Structural Basis of Efficacy-Driven Ligand Selectivity at GPCRs. Nat Chem Biol 2023, 1–10. link . Some papers that I liked for their prose; of course, the content is great, too! Neidhardt, F. C. Bacterial Growth: Constant Obsession with dN/Dt. J Bacteriol 1999, 181 (24), 7405–7408. link . Greener, Joe G., Shaun M. Kandathil, Lewis Moffat, and David T. Jones. “A Guide to Machine Learning for Biologists.” Nature Reviews Molecular Cell Biology 23, no. 1 (January 2022): 40–55. link . Tanaka, H.; Tong, H.; Shi, R.; Russo, J. Revealing Key Structural Features Hidden in Liquids and Glasses. Nat Rev Phys 2019, 1 (5), 333–348. link . Binder, K. Theory of First-Order Phase Transitions. Rep. Prog. Phys. 1987, 50 (7), 783. link . Everaers, R.; Sukumaran, S. K.; Grest, G. S.; Svaneborg, C.; Sivasubramanian, A.; Kremer, K. Rheology and Microscopic Topology of Entangled Polymeric Liquids. Science 2004, 303 (5659), 823–826. link . Guven, J.; Manrique, G. Arresting the Collapse of a Catenary Arch. arXiv:1710.03433 [cond-mat, physics:physics] 2017. Braberg, H.; Echeverria, I.; Kaake, R. M.; Sali, A.; Krogan, N. J. From Systems to Structure — Using Genetic Data to Model Protein Structures. Nat Rev Genet 2022, 23 (6), 342–354. link . Well-written papers Efron, B.; Halloran, E.; Holmes, S. Bootstrap Confidence Levels for Phylogenetic Trees. Proceedings of the National Academy of Sciences 1996, 93 (23), 13429–13429. link . Mandelbrot, B. How Long Is the Coast of Britain? Statistical Self-Similarity and Fractional Dimension. Science 1967, 156 (3775), 636–638. link . Luttinger, J. M. An Exactly Soluble Model of a Many‐Fermion System. Journal of Mathematical Physics 1963, 4 (9), 1154–1162. link . Foley, S.; Deserno, M. Stabilizing Leaflet Asymmetry under Differential Stress in a Highly Coarse-Grained Lipid Membrane Model. J. Chem. Theory Comput. 2020, 16 (11), 7195–7206. link . Sing, C. E.; Perry, S. L. Recent Progress in the Science of Complex Coacervation. Soft Matter 2020, 16 (12), 2885–2914. link . Jo, M. H.; Meneses, P.; Yang, O.; Carcamo, C. C.; Pangeni, S.; Ha, T. Determination of Single-Molecule Loading Rate during Mechanotransduction in Cell Adhesion. Science 2024, 383 (6689), 1374–1379. link Deshpande, N. S.; Furbish, D. J.; Arratia, P. E.; Jerolmack, D. J. The Perpetual Fragility of Creeping Hillslopes. Nat Commun 2021, 12 (1), 3909. link . Cohen, A. E.; Shi, Z. Do Cell Membranes Flow Like Honey or Jiggle Like Jello? BioEssays 2020, 42 (1), 1900142. link . Park, J. J.; Lu, Y.-K.; Jamison, A. O.; Tscherbul, T. V.; Ketterle, W. A Feshbach Resonance in Collisions between Triplet Ground-State Molecules. Nature 2023, 614 (7946), 54–58. link . Another precise and well-written abstract! Jia, X.; Lynch, A.; Huang, Y.; Danielson, M.; Lang’at, I.; Milder, A.; Ruby, A. E.; Wang, H.; Friedler, S. A.; Norquist, A. J.; Schrier, J. Anthropogenic Biases in Chemical Reaction Data Hinder Exploratory Inorganic Synthesis. Nature 2019, 573 (7773), 251–255. link . Marder, E. Grandmother Elephants. eLife 2013, 2, e01140. link . van de Schoot, R.; Depaoli, S.; King, R.; Kramer, B.; Märtens, K.; Tadesse, M. G.; Vannucci, M.; Gelman, A.; Veen, D.; Willemsen, J.; Yau, C. Bayesian Statistics and Modelling. Nat Rev Methods Primers 2021, 1 (1), 1–26. link . França, T. F. A.; Monserrat, J. M. Writing Papers to Be Memorable, Even When They Are Not Really Read. BioEssays 2019, 41 (5), 1900035. link . Thurston, W. P. On Proof and Progress in Mathematics. arXiv March 31, 1994. link . Galstyan, V.; Phillips, R. Allostery and Kinetic Proofreading. J. Phys. Chem. B 2019, 123 (51), 10990–11002. link . Ghosh, A.; Radhakrishnan, J.; Chaikin, P. M.; Levine, D.; Ghosh, S. Coupled Dynamical Phase Transitions in Driven Disk Packings. Phys. Rev. Lett. 2022, 129 (18), 188002. link . I liked how concise and informative the abstract is! Overbeek, J. T. G.; Voorn, M. J. Phase Separation in Polyelectrolyte Solutions. Theory of Complex Coacervation. Journal of Cellular and Comparative Physiology 1957, 49 (S1), 7–26. link . Bialek, W. Physical Limits to Sensation and Perception. Annu. Rev. Biophys. Biophys. Chem. 1987, 16 (1), 455–478. link ."
  },
  {
    "title": "On Writing (and Some Examples)",
    "url": "https://arghyadutta.github.io/notebooks/writing.html",
    "content": "Common Errors in English Usage Recommended Kolln, M. J., & Gray, L. S. (2015). Rhetorical Grammar: Grammatical Choices, Rhetorical Effects . Pearson Education. Excellent! See, for example, the one on tense and aspect . Mukherjee, S. (2023). All the Carcinogens We Cannot See . The New Yorker . Neelakantan, A. (2021). Write because it makes you think or feel. the Record . Williams, J. M. (2000). Style: Ten lessons in clarity and grace (6. ed). Longman. Writing Style—An Online Guide Wolchover has an uncanny ability to phrase difficult ideas in popular language; this essay demonstrates that once again. Gaitskill M. (2022) The deracination of literature . Unheard . Some well-written essays Smith, Z. (2023). On Killing Charles Dickens . The New Yorker . Purdue Online Writing Lab Good introduction to English grammar and writing beyond simple definitions. Recommendations and resources on writing Yong, E. (2021). America Is Not Ready for Omicron . The Atlantic . Possibly the most famous essay on NYC—for good reasons. White, E. B. (1949). Here is New York . Reprinted in Essays of E. B. White (2016) . Writing is emotional: it's hard to remove unnecessary sentences that you've written. McPhees shares few personal stories. Wolchover, N. (2020). What Is a Particle? Quanta . Orwell, G. (1946). Politics and the English Language; George Orwell . Horizon . Ed Yong shows that one can write a good essay only when one deeply cares about the topic. Newport, C. (2024). One Reason Hybrid Work Makes Employees Miserable. And how to fix it. The Atlantic . McPhee J. (2015) The Art of Omission . The New Yorker . Zadie Smith tries—and fails—to ignore Dickens while writing a nineteenth-century historical fiction. Orwell gives five rules for good writing and adds a sixth: \"Break any of these rules sooner than say anything outright barbarous.\" Why read fiction? And does good writing matter? The Writer's workshop If I get time Grammar and Stylistics: Writing Clearly by Randall Eggert from University of Utah. Punctuation guide YouTube playlists Internet resources"
  },
  {
    "title": "Linear algebra",
    "url": "https://arghyadutta.github.io/notebooks/linearAlgebra.html",
    "content": "Also see: Basic Mathematics for Quantum Mechanics Singular Value Decomposition by Steven Brunton. The Matrix Cookbook by Kaare Brandt Petersen and Michael Syskind Pedersen Online resources"
  },
  {
    "title": "On music",
    "url": "https://arghyadutta.github.io/notebooks/music.html",
    "content": "Sheila Dhar’s witty reflections on the world of Indian classical music. Some descriptions are unforgettable, like when Pandit Pran Nath tuned a Tanpura for over an hour until it sounded just perfect. Chatterjee, S. (2023). Choral Voices: Ethnographic Imaginations of Sound and Sacrality . Bloomsbury Publishing USA. TMK has strong opinions, and I sometimes disagree with him, but the book is informative, empathetic, and well-written. Recommended. Incidentally, an organization—Kaahon—has a YouTube channel and they produced a couple of good documentaries on instrument makers from Bengal, like this one on Rudra Veena makers and this one on Sitar makers. Their tales are similar to the mridangam makers—especially the lack of recognition. Informative and sometimes fun, but the overall snobbish tone ruined it for me. The list is chronologically ordered. Recommended Krishna, T. M. (2022). Sebastian & Sons: A Brief History of Mrdangam Makers . Westland Publications Limited. Dhar, S. (2017). Raga'N Josh . The Orient Blackswan. Mukhaopadhyay, K. P. (2014). Kudrat Rangibirangi (1st edition). Ananda Publishers. (In Bengali) Ayyangar, R. R. (2019). History of South Indian (Carnatic) Music , Third edition. Vipanci Charitable Trust. Ethnographic studies of choirs in Shillong and Goa to explore choral singing and postcolonial musical practices, highlighting the interplay of individual and collective identities. The tone is academic, yet it's accessible and well-written—a fairly rare occurrence."
  },
  {
    "title": "C. S. Lewis",
    "url": "https://arghyadutta.github.io/notebooks/cSLewis.html",
    "content": "Recommended C. S Lewis, Meditation in a Toolshed , The Coventry Evening Telegraph (1945) . Lewis on looking at and looking along. Thoughtful piece."
  },
  {
    "title": "Bose–Einstein Condensates",
    "url": "https://arghyadutta.github.io/notebooks/bec.html",
    "content": "Recommended Proukakis, N. P. A Century of Bose–Einstein Condensation . Commun Phys 2025, 8 (1), 264 . Schwartz, M. Lecture 12: Bose-Einstein Condensation , 2019. PDF"
  },
  {
    "title": "Genetic Algorithm",
    "url": "https://arghyadutta.github.io/notebooks/geneticAlgorithm.html",
    "content": "Carr, J. (2014). An Introduction to Genetic Algorithms . PDF Procedure: It defines a population of chromosomes, which are candidate solutions to the optimization problem). It then does the following operations on the initial set of parents: selection, crossover, mutation, elicitation. (pp. 2–4, Carr 2014) Recommended Kim, C., Batra, R., Chen, L., Tran, H., & Ramprasad, R. (2021). Polymer design using genetic algorithm and machine learning . Computational Materials Science, 186, 110067 . GA is a type of evolutionary computing. It imitates biological evolution to find the `fittest' solution to an optimization problem. So, it is another optimization algorithm like gradient descent or exhaustive search. The fitness is determined by a fitness function . One practical example is shown in Kim et al. 2021 who designed new polymers starting from a set of parent polymers with the goal of achieving high glass-transition temperature $T_\\text{g}$ (to ensure mechanical stability at high temperatures) and high band-gap $E_\\text{g}$ (to ensure protection from dielectric breakdown). They proposed 192 new polymers that may have that function based on designing new polymers using GA and then evaluating the $T_\\text{g}$ and $E_\\text{g}$ of those from pre-trained ML models."
  },
  {
    "title": "Chemistry: Online Resources",
    "url": "https://arghyadutta.github.io/notebooks/chemistry.html",
    "content": "Beautiful Chemistry Illustrated Glossary of Organic Chemistry MolView"
  },
  {
    "title": "Machine Learning: Free Books",
    "url": "https://arghyadutta.github.io/notebooks/mlFreeBooks.html",
    "content": "Information Theory, Inference, and Learning Algorithms by David MacKay An Introduction to Statistical Learning by Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani Interpretable Machine Learning by Christoph Molnar Deep Learning for Molecules and Materials by Andrew D. White Model-Based Machine Learning by John Winn The Elements of Statistical Learning by Trevor Hastie, Robert Tibshirani, Jerome Friedman The Little Book of Deep Learning by François Fleuret. He formatted the book for reading on smartphones—such innovative people!"
  },
  {
    "title": "Physics: Visualizations and Experiments",
    "url": "https://arghyadutta.github.io/notebooks/physicsExperiments.html",
    "content": "Cameras and Lenses Brownian motion Critical Point of carbon dioxide A Nobel laureate and a flea circus join forces for an unforgettable demonstration of inertia Melting of a cube of gold metal using the embedded atom method (eam) force field Quantum mechanics: Wave Particle Duality Triple Point of Water Marangoni Bursting: Evaporation-Induced Emulsification of a Two-Component Droplet Quantum mechanics: Stern and Gerlach experiment Flight manifest: from take-off to landing, a bird’s eye introduction to flying Sublimation of iodine"
  },
  {
    "title": "Evaluating Clustering Performance",
    "url": "https://arghyadutta.github.io/notebooks/clustPerformance.html",
    "content": " Evaluating Clustering Performance NOTE: Most of this notebook's content is adapted—often copied!—from scikit-learn's excellent documentation on clustering . This is a compilation of definitions and code-snippets that I found useful while working on a project, along with some of my own thoughts. Evaluation of quality of clusters is often done in two ways: comparing how well the predicted clusters compare with the ground truth and checking if the generated clusters are consistent. The second approach is the only option if no ground truths are available. There are several coefficients that quantify the quality of clustering. Some that do not need a ground truth are Silhouette coefficient, Calinski–Harabasz index, and Davies–Bouldin index. Others such as homogeneity Score, completeness score, and V Measure need a ground truth. Scikit mentions a drawback: Completeness Score ¶ Silhouette coefficient ¶ For a single sample it is $$\ns = \\frac{b - a}{\\max(a, b)}.\n$$ $a$: mean distance between a sample all other points in the same cluster $b$: mean distance between a sample all other points in the next nearest cluster . $s$ takes values in $[-1,1]$ with $-1$ meaning incorrect clustering, 0 meaning overlapping clusters, and 1 meaning highly-dense, well-separated clusters. So, a higher silhouette score means better defined clusters. Important : Scikit returns the mean of all Silhouette coefficients of the samples. Scikit mentions a drawback: The Silhouette Coefficient is generally higher for convex clusters than other concepts of clusters, such as density based clusters like those obtained through DBSCAN. from clusim.clustering import Clustering import clusim.sim as sim true_labels = [ 1 , 1 , 1 , 2 , 2 , 2 , 3 , 3 , 3 ] predicted_labels = [ 1 , 2 , 2 , 3 , 3 , 1 , 1 , 1 , 1 ] single_cluster_labels = [ 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 ] completely_fragmented_labels = [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 ] # Their Data is Differently Formatted. true_clustering = Clustering () . from_membership_list ( true_labels ) predicted_clustering = Clustering () . from_membership_list ( predicted_labels ) predicted_single_cluster = Clustering () . from_membership_list ( single_cluster_labels ) predicted_completely_fragmented = Clustering () . from_membership_list ( completely_fragmented_labels ) for _ in [ predicted_clustering , predicted_single_cluster , predicted_completely_fragmented , ]: print ( f \"FMI = { sim . fowlkes_mallows_index ( true_clustering , _ ) } , NMI = { sim . nmi ( true_clustering , _ ) } , elem-cent = { sim . element_sim ( true_clustering , _ ) } \" ) # The Package Can Compute Many Scores such As... (code from Their Documentation https://hoosier-clusters.github.io/clusim/html/clusim.html) row_format2 = \" {:>25} \" * ( 2 ) for simfunc in sim . available_similarity_measures : print ( row_format2 . format ( simfunc , eval ( \"sim.\" + simfunc + \"(true_clustering, predicted_clustering)\" ) ) ) Element-centric similarity and issues with FMI and NMI ¶ Gates et al. raise objections against using Fowlkes–Mallows Index and NMI, specifically NMI; they proposed a new one. References: A. J. Gates et al. Element-Centric Clustering Comparison Unifies Overlaps and Hierarchy. Sci Rep 2019, 9 (1), 8574. DOI . A. J. Gates, Y.-Y. Ahn. CluSim: A Python Package for Calculating Clustering Similarity. Journal of Open Source Software 2019, 4 (35), 1264. DOI . https://github.com/Hoosier-Clusters/clusim of Gates et al. with their package Homogeneity Score ¶ Homogeneity score ($h$) is defined as\n$$\n\\begin{align*}\n&h = 1 - \\frac{H(C|K)}{H(C)}\\;\\text{where}\\\\\n&H(C) = - \\sum_{c=1}^{|C|} \\frac{n_c}{n} \\cdot \\log\\left(\\frac{n_c}{n}\\right)\\\\\n&H(C|K) = - \\sum_{c=1}^{|C|} \\sum_{k=1}^{|K|} \\frac{n_{c,k}}{n}\\cdot \\log\\left(\\frac{n_{c,k}}{n_k}\\right)\n\\end{align*}\n$$ $n$: total number of samples. $n_c$, $n_k$: number of samples in class $c$ and cluster $k$, respectively. $n_{c,k}$: number of samples from class $c$ assigned to cluster $k$. Higher $h$ means better clusters. $h$ takes values in $[0,1]$. AMI is adjusted against chance. NMI and MI are not. Higher $h$ means better clusters. $h$ takes values in $[0,1]$. In [4]: from clusim.clustering import Clustering import clusim.sim as sim true_labels = [ 1 , 1 , 1 , 2 , 2 , 2 , 3 , 3 , 3 ] predicted_labels = [ 1 , 2 , 2 , 3 , 3 , 1 , 1 , 1 , 1 ] single_cluster_labels = [ 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 ] completely_fragmented_labels = [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 ] # Their Data is Differently Formatted. true_clustering = Clustering () . from_membership_list ( true_labels ) predicted_clustering = Clustering () . from_membership_list ( predicted_labels ) predicted_single_cluster = Clustering () . from_membership_list ( single_cluster_labels ) predicted_completely_fragmented = Clustering () . from_membership_list ( completely_fragmented_labels ) for _ in [ predicted_clustering , predicted_single_cluster , predicted_completely_fragmented , ]: print ( f \"FMI = { sim . fowlkes_mallows_index ( true_clustering , _ ) } , NMI = { sim . nmi ( true_clustering , _ ) } , elem-cent = { sim . element_sim ( true_clustering , _ ) } \" ) # The Package Can Compute Many Scores such As... (code from Their Documentation https://hoosier-clusters.github.io/clusim/html/clusim.html) row_format2 = \" {:>25} \" * ( 2 ) for simfunc in sim . available_similarity_measures : print ( row_format2 . format ( simfunc , eval ( \"sim.\" + simfunc + \"(true_clustering, predicted_clustering)\" ) ) ) FMI = 0.4811252243246881, NMI = 0.5451600159416435, elem-cent = 0.5407407407407406\nFMI = 0.5, NMI = 0.0, elem-cent = 0.33333333333333326\nFMI = 0.0, NMI = 0.6666666666666665, elem-cent = 0.33333333333333326\n            jaccard_index                   0.3125\n               rand_index       0.6944444444444444\n            adjrand_index      0.26666666666666655\n    fowlkes_mallows_index       0.4811252243246881\n                 fmeasure      0.47619047619047616\n             purity_index       0.7777777777777777\n     classification_error      0.22222222222222232\n        czekanowski_index      0.47619047619047616\n               dice_index      0.47619047619047616\n           sorensen_index      0.47619047619047616\n    rogers_tanimoto_index       0.5319148936170213\n          southwood_index      0.45454545454545453\n      pearson_correlation      0.00102880658436214\n         corrected_chance      0.16994265720286564\n      sample_expected_sim      0.10526315789473684\n                      nmi       0.5451600159416435\n                       mi       0.8233232815796736\n                   adj_mi       0.3410389011275906\n                      rmi       0.1464053299155769\n                       vi       1.3738364418444755\n       geometric_accuracy       0.7777777777777778\n          overlap_quality                     -0.0\n                     onmi       0.6303315236619905\n              omega_index      0.26666666666666655 FMI = TP / np . sqrt (( TP + FP ) * ( TP + FN )) print ( FMI ) # You Can also Compute FMI Using Scikit-learn. The Results Match, of Course. print ( metrics . fowlkes_mallows_score ([ 0 , 0 , 1 , 1 ], [ 0 , 0 , 1 , 2 ])) # Also Two Same Partitions Will Have no Off-diagonal Elements in the Pair Confusion Matrix and a FMI Score of 1. C = pair_confusion_matrix ([ 0 , 0 , 1 , 1 ], [ 0 , 0 , 1 , 1 ]) print ( C ) TN = C [ 0 , 0 ] FP = C [ 0 , 1 ] FN = C [ 1 , 0 ] TP = C [ 1 , 1 ] FMI = TP / np . sqrt (( TP + FP ) * ( TP + FN )) print ( FMI ) print ( metrics . fowlkes_mallows_score ([ 0 , 0 , 1 , 1 ], [ 0 , 0 , 1 , 1 ])) Davies-Bouldin Index ¶ Surprisingly, DB values closer to zero indicate a better partition , unlike Calinski-Harabasz index and Silhouette coefficient. For CHI and DBI formulas, check the referenced scikit doc page. Surprisingly, DB values closer to zero indicate a better partition , unlike Calinski-Harabasz index and Silhouette coefficient. $s$ takes values in $[-1,1]$ with $n$: total number of samples. $<0$: bad (i.e. independent labeling) Calinski–Harabasz index (CHI) is the ratio of the sum of between-clusters dispersion for all clusters and sum of inter-cluster dispersion for all clusters. Better-defined clusters have higher CHI value. $c \\in[0,1]$. higher is better. FMI = 0.4811252243246881, NMI = 0.5451600159416435, elem-cent = 0.5407407407407406\nFMI = 0.5, NMI = 0.0, elem-cent = 0.33333333333333326\nFMI = 0.0, NMI = 0.6666666666666665, elem-cent = 0.33333333333333326\n            jaccard_index                   0.3125\n               rand_index       0.6944444444444444\n            adjrand_index      0.26666666666666655\n    fowlkes_mallows_index       0.4811252243246881\n                 fmeasure      0.47619047619047616\n             purity_index       0.7777777777777777\n     classification_error      0.22222222222222232\n        czekanowski_index      0.47619047619047616\n               dice_index      0.47619047619047616\n           sorensen_index      0.47619047619047616\n    rogers_tanimoto_index       0.5319148936170213\n          southwood_index      0.45454545454545453\n      pearson_correlation      0.00102880658436214\n         corrected_chance      0.16994265720286564\n      sample_expected_sim      0.10526315789473684\n                      nmi       0.5451600159416435\n                       mi       0.8233232815796736\n                   adj_mi       0.3410389011275906\n                      rmi       0.1464053299155769\n                       vi       1.3738364418444755\n       geometric_accuracy       0.7777777777777778\n          overlap_quality                     -0.0\n                     onmi       0.6303315236619905\n              omega_index      0.26666666666666655 $c = 1 - \\frac{H(K|C)}{H(K)}$ In [4]: from clusim.clustering import Clustering import clusim.sim as sim true_labels = [ 1 , 1 , 1 , 2 , 2 , 2 , 3 , 3 , 3 ] predicted_labels = [ 1 , 2 , 2 , 3 , 3 , 1 , 1 , 1 , 1 ] single_cluster_labels = [ 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 ] completely_fragmented_labels = [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 ] # Their Data is Differently Formatted. true_clustering = Clustering () . from_membership_list ( true_labels ) predicted_clustering = Clustering () . from_membership_list ( predicted_labels ) predicted_single_cluster = Clustering () . from_membership_list ( single_cluster_labels ) predicted_completely_fragmented = Clustering () . from_membership_list ( completely_fragmented_labels ) for _ in [ predicted_clustering , predicted_single_cluster , predicted_completely_fragmented , ]: print ( f \"FMI = { sim . fowlkes_mallows_index ( true_clustering , _ ) } , NMI = { sim . nmi ( true_clustering , _ ) } , elem-cent = { sim . element_sim ( true_clustering , _ ) } \" ) # The Package Can Compute Many Scores such As... (code from Their Documentation https://hoosier-clusters.github.io/clusim/html/clusim.html) row_format2 = \" {:>25} \" * ( 2 ) for simfunc in sim . available_similarity_measures : print ( row_format2 . format ( simfunc , eval ( \"sim.\" + simfunc + \"(true_clustering, predicted_clustering)\" ) ) ) There are several coefficients that quantify the quality of clustering. Some that do not need a ground truth are Silhouette coefficient, Calinski–Harabasz index, and Davies–Bouldin index. Others such as homogeneity Score, completeness score, and V Measure need a ground truth. In [2]: from sklearn.metrics.cluster import pair_confusion_matrix from sklearn import metrics import numpy as np C = pair_confusion_matrix ([ 0 , 0 , 1 , 1 ], [ 0 , 0 , 1 , 2 ]) print ( C ) TN = C [ 0 , 0 ] FP = C [ 0 , 1 ] FN = C [ 1 , 0 ] TP = C [ 1 , 1 ] [[8 0]\n [2 2]] Building on the pair-confusion matrix, FMI, a measure for cluster similarity, is computed using the elements of $C$ as\n$$FMI = \\frac{TP}{\\sqrt{(TP + FP) (TP + FN)}}.$$ Pair confusion matrix ¶ Two clusters can be compared using the pair-confusion Matrix. Mutual-information-based similarity score ¶ V-Measure ¶ 1 meaning highly-dense, well-separated clusters. In [1]: from sklearn import metrics labels_true = [ 0 , 0 , 0 , 1 , 1 , 1 ] labels_pred = [ 0 , 0 , 1 , 1 , 2 , 2 ] print ( metrics . homogeneity_score ( labels_true , labels_pred )) print ( metrics . completeness_score ( labels_true , labels_pred )) print ( metrics . v_measure_score ( labels_true , labels_pred , beta = 0.6 )) V-Measure ¶ The harmonic mean of the homogeneity score and completeness scores: $v = 2 \\cdot \\frac{h \\cdot c}{h + c}$. It's implemented in scikit as $v = \\frac{(1 + \\beta) \\times \\text{homogeneity} \\times \\text{completeness}}{(\\beta \\times \\text{homogeneity} + \\text{completeness})}$ In [4]: $-1$ meaning incorrect clustering, Mutual-information-based similarity score ¶ Let's say, we have 2 sets of labels, $U$ and $V$, for $N$ objects. If $P(i)=|U_i|/N$ and $P'(j)=|V_i|/N$ are the probabilities that a randomly-picked object from $U$ ($V$) falls into class $U_i$ ($V_j$), then the entropies and the mutual information between $U$ and $V$ are computed as\n$$\n\\begin{aligned}\nH(U) &= - \\sum_{i=1}^{|U|}P(i)\\log(P(i))\\\\\nH(V) &= - \\sum_{j=1}^{|V|}P'(j)\\log(P'(j))\\\\\n\\text{MI}(U, V) &= \\sum_{i=1}^{|U|}\\sum_{j=1}^{|V|}P(i, j)\\log\\left(\\frac{P(i,j)}{P(i)P'(j)}\\right)\\\\\n&= \\sum_{i=1}^{|U|} \\sum_{j=1}^{|V|} \\frac{|U_i \\cap V_j|}{N}\\log\\left(\\frac{N|U_i \\cap V_j|}{|U_i||V_j|}\\right)\\\\\n\\text{NMI}(U, V) &= \\frac{\\text{MI}(U, V)}{\\text{mean}(H(U), H(V))}\\\\\n\\text{AMI} &= \\frac{\\text{MI} - E[\\text{MI}]}{\\text{mean}(H(U), H(V)) - E[\\text{MI}]}\n\\end{aligned}\n$$ AMI is adjusted against chance. NMI and MI are not. AMI $<0$: bad (i.e. independent labeling) $=0$ (random labeling) $\\simeq 1$ (agreement between the ground truth and predicted labels) References: C. D. Manning, P. Raghavan, H. Schütze. Introduction to Information Retrieval; Cambridge University Press: New York, 2008. pp. 356–359 (Good discussion.) Calinski Harabasz Index ¶ Calinski–Harabasz index (CHI) is the ratio of the sum of between-clusters dispersion for all clusters and sum of inter-cluster dispersion for all clusters. Better-defined clusters have higher CHI value. $\\simeq 1$ (agreement between the ground truth and predicted labels) The second approach is the only option if no ground truths are available. Element-centric similarity and issues with FMI and NMI ¶ Gates et al. raise objections against using Fowlkes–Mallows Index and NMI, specifically NMI; they proposed a new one. Arghya Dutta Notebooks comparing how well the predicted clusters compare with the ground truth and Completeness Score ¶ All members of a class belongs to the same cluster. $c = 1 - \\frac{H(K|C)}{H(K)}$ $c \\in[0,1]$. higher is better. In [3]: [[8 0]\n [2 2]] AMI $<0$: bad (i.e. independent labeling) $=0$ (random labeling) $\\simeq 1$ (agreement between the ground truth and predicted labels) $n_c$, $n_k$: number of samples in class $c$ and cluster $k$, respectively. In [1]: from sklearn import metrics labels_true = [ 0 , 0 , 0 , 1 , 1 , 1 ] labels_pred = [ 0 , 0 , 1 , 1 , 2 , 2 ] print ( metrics . homogeneity_score ( labels_true , labels_pred )) print ( metrics . completeness_score ( labels_true , labels_pred )) print ( metrics . v_measure_score ( labels_true , labels_pred , beta = 0.6 )) 0.6666666666666669\n0.420619835714305\n0.5467344787062375 0.6666666666666669\n0.420619835714305\n0.5467344787062375 Two clusters can be compared using the pair-confusion Matrix. Homogeneity Score ¶ from sklearn.metrics.cluster import pair_confusion_matrix from sklearn import metrics import numpy as np C = pair_confusion_matrix ([ 0 , 0 , 1 , 1 ], [ 0 , 0 , 1 , 2 ]) print ( C ) TN = C [ 0 , 0 ] FP = C [ 0 , 1 ] FN = C [ 1 , 0 ] TP = C [ 1 , 1 ] NOTE: Most of this notebook's content is adapted—often copied!—from scikit-learn's excellent documentation on clustering . This is a compilation of definitions and code-snippets that I found useful while working on a project, along with some of my own thoughts. In [1]: $n_{c,k}$: number of samples from class $c$ assigned to cluster $k$. from sklearn import metrics labels_true = [ 0 , 0 , 0 , 1 , 1 , 1 ] labels_pred = [ 0 , 0 , 1 , 1 , 2 , 2 ] print ( metrics . homogeneity_score ( labels_true , labels_pred )) print ( metrics . completeness_score ( labels_true , labels_pred )) print ( metrics . v_measure_score ( labels_true , labels_pred , beta = 0.6 )) All members of a class belongs to the same cluster. $c = 1 - \\frac{H(K|C)}{H(K)}$ $c \\in[0,1]$. higher is better. Calinski Harabasz Index ¶ For CHI and DBI formulas, check the referenced scikit doc page. checking if the generated clusters are consistent. References: $$\ns = \\frac{b - a}{\\max(a, b)}.\n$$ In [3]: FMI = TP / np . sqrt (( TP + FP ) * ( TP + FN )) print ( FMI ) # You Can also Compute FMI Using Scikit-learn. The Results Match, of Course. print ( metrics . fowlkes_mallows_score ([ 0 , 0 , 1 , 1 ], [ 0 , 0 , 1 , 2 ])) # Also Two Same Partitions Will Have no Off-diagonal Elements in the Pair Confusion Matrix and a FMI Score of 1. C = pair_confusion_matrix ([ 0 , 0 , 1 , 1 ], [ 0 , 0 , 1 , 1 ]) print ( C ) TN = C [ 0 , 0 ] FP = C [ 0 , 1 ] FN = C [ 1 , 0 ] TP = C [ 1 , 1 ] FMI = TP / np . sqrt (( TP + FP ) * ( TP + FN )) print ( FMI ) print ( metrics . fowlkes_mallows_score ([ 0 , 0 , 1 , 1 ], [ 0 , 0 , 1 , 1 ])) 0.7071067811865475\n0.7071067811865476\n[[8 0]\n [0 4]]\n1.0\n1.0 So, a higher silhouette score means better defined clusters. Silhouette coefficient ¶ $b$: mean distance between a sample all other points in the next nearest cluster . 0.7071067811865475\n0.7071067811865476\n[[8 0]\n [0 4]]\n1.0\n1.0 Homogeneity score ($h$) is defined as\n$$\n\\begin{align*}\n&h = 1 - \\frac{H(C|K)}{H(C)}\\;\\text{where}\\\\\n&H(C) = - \\sum_{c=1}^{|C|} \\frac{n_c}{n} \\cdot \\log\\left(\\frac{n_c}{n}\\right)\\\\\n&H(C|K) = - \\sum_{c=1}^{|C|} \\sum_{k=1}^{|K|} \\frac{n_{c,k}}{n}\\cdot \\log\\left(\\frac{n_{c,k}}{n_k}\\right)\n\\end{align*}\n$$ A. J. Gates et al. Element-Centric Clustering Comparison Unifies Overlaps and Hierarchy. Sci Rep 2019, 9 (1), 8574. DOI . In [2]: from sklearn.metrics.cluster import pair_confusion_matrix from sklearn import metrics import numpy as np C = pair_confusion_matrix ([ 0 , 0 , 1 , 1 ], [ 0 , 0 , 1 , 2 ]) print ( C ) TN = C [ 0 , 0 ] FP = C [ 0 , 1 ] FN = C [ 1 , 0 ] TP = C [ 1 , 1 ] Notebooks The Silhouette Coefficient is generally higher for convex clusters than other concepts of clusters, such as density based clusters like those obtained through DBSCAN. Davies-Bouldin Index ¶ FMI is useful because it gives a number—and not a matrix like the pair confusion matrix—for quickly comparing two clusterings. Pair confusion matrix ¶ Important : Scikit returns the mean of all Silhouette coefficients of the samples. 0 meaning overlapping clusters, and It's implemented in scikit as $v = \\frac{(1 + \\beta) \\times \\text{homogeneity} \\times \\text{completeness}}{(\\beta \\times \\text{homogeneity} + \\text{completeness})}$ A. J. Gates, Y.-Y. Ahn. CluSim: A Python Package for Calculating Clustering Similarity. Journal of Open Source Software 2019, 4 (35), 1264. DOI . $a$: mean distance between a sample all other points in the same cluster The harmonic mean of the homogeneity score and completeness scores: $v = 2 \\cdot \\frac{h \\cdot c}{h + c}$. C. D. Manning, P. Raghavan, H. Schütze. Introduction to Information Retrieval; Cambridge University Press: New York, 2008. pp. 356–359 (Good discussion.) Let's say, we have 2 sets of labels, $U$ and $V$, for $N$ objects. If $P(i)=|U_i|/N$ and $P'(j)=|V_i|/N$ are the probabilities that a randomly-picked object from $U$ ($V$) falls into class $U_i$ ($V_j$), then the entropies and the mutual information between $U$ and $V$ are computed as\n$$\n\\begin{aligned}\nH(U) &= - \\sum_{i=1}^{|U|}P(i)\\log(P(i))\\\\\nH(V) &= - \\sum_{j=1}^{|V|}P'(j)\\log(P'(j))\\\\\n\\text{MI}(U, V) &= \\sum_{i=1}^{|U|}\\sum_{j=1}^{|V|}P(i, j)\\log\\left(\\frac{P(i,j)}{P(i)P'(j)}\\right)\\\\\n&= \\sum_{i=1}^{|U|} \\sum_{j=1}^{|V|} \\frac{|U_i \\cap V_j|}{N}\\log\\left(\\frac{N|U_i \\cap V_j|}{|U_i||V_j|}\\right)\\\\\n\\text{NMI}(U, V) &= \\frac{\\text{MI}(U, V)}{\\text{mean}(H(U), H(V))}\\\\\n\\text{AMI} &= \\frac{\\text{MI} - E[\\text{MI}]}{\\text{mean}(H(U), H(V)) - E[\\text{MI}]}\n\\end{aligned}\n$$ Evaluation of quality of clusters is often done in two ways: In [3]: FMI = TP / np . sqrt (( TP + FP ) * ( TP + FN )) print ( FMI ) # You Can also Compute FMI Using Scikit-learn. The Results Match, of Course. print ( metrics . fowlkes_mallows_score ([ 0 , 0 , 1 , 1 ], [ 0 , 0 , 1 , 2 ])) # Also Two Same Partitions Will Have no Off-diagonal Elements in the Pair Confusion Matrix and a FMI Score of 1. C = pair_confusion_matrix ([ 0 , 0 , 1 , 1 ], [ 0 , 0 , 1 , 1 ]) print ( C ) TN = C [ 0 , 0 ] FP = C [ 0 , 1 ] FN = C [ 1 , 0 ] TP = C [ 1 , 1 ] FMI = TP / np . sqrt (( TP + FP ) * ( TP + FN )) print ( FMI ) print ( metrics . fowlkes_mallows_score ([ 0 , 0 , 1 , 1 ], [ 0 , 0 , 1 , 1 ])) $=0$ (random labeling) Fowlkes–Mallows Index ¶ For a single sample it is In [2]: Fowlkes–Mallows Index ¶ Building on the pair-confusion matrix, FMI, a measure for cluster similarity, is computed using the elements of $C$ as\n$$FMI = \\frac{TP}{\\sqrt{(TP + FP) (TP + FN)}}.$$ FMI is useful because it gives a number—and not a matrix like the pair confusion matrix—for quickly comparing two clusterings. https://github.com/Hoosier-Clusters/clusim of Gates et al. with their package"
  },
  {
    "title": "Silence",
    "url": "https://arghyadutta.github.io/notebooks/silence.html",
    "content": "An astounding documentary! From the description: \"Nestled deep in the postcard-perfect French Alps, the Grande Chartreuse is considered one of the world's most ascetic monasteries. In 1984, German filmmaker Philip Gröning wrote to the Carthusian order for permission to make a documentary about them. They said they would get back to him. Sixteen years later, they were ready. Gröning, sans crew or artificial lighting, lived in the monks' quarters for six months—filming their daily prayers, tasks, rituals and rare outdoor excursions. This transcendent, closely observed film seeks to embody a monastery, rather than simply depict one—it has no score, no voiceover and no archival footage. What remains is stunningly elemental: time, space and light. One of the most mesmerizing and poetic chronicles of spirituality ever created, INTO GREAT SILENCE dissolves the border between screen and audience with a total immersion into the hush of monastic life. More meditation than documentary, it's a rare, transformative theatrical experience for all.\" Robert Gershon (Director). (2022, March 29). And They Kept Silence YouTube . Recommended As white light gathers all –\nThe rose and the amethyst, The ice-green and the copper-green, The peacock blue and the mist – If I Get Time IntoGreatSilence (Director). (2007, February 21). Official Into Great Silence US Trailer YouTube . On a Still Morning Nan Shepherd I hear the silence now, Alive within its heart Are the sounds that can not be heard That the ear may not dispart? Shepherd, N. (2008). The living mountain: A celebration of the Cairngorm Mountains of Scotland . Edinburgh: Canongate. ( Archive ) Bold Books and Bones (Director). (2022, April 23). What do you hear when you find silence? YouTube . Shepherd, N., & Macfarlane, R. (2019). In the Cairngorms (UK ed. edition). Galileo Publishers. So if I bend my ear To silence, I grown aware The stir of sounds I have almost heard That are not quite there. Pyrah, S. (2025, February 2). Quiet, please! The remarkable power of silence – for our bodies and our minds . The Guardian ."
  },
  {
    "title": "Socialism",
    "url": "https://arghyadutta.github.io/notebooks/socialism.html",
    "content": "Arrow, K. J. (1978). A Cautious Case for Socialism . Dissent Magazine Recommended Arrow offers a careful analysis of capitalism and socialism, without asking you to join a camp. The Communist party received enormous support from the citizens, mainly urban population. At the time of show trial of Nikolai Bukharin, after complications regarding the New Economic Policy, many prominent literary figures singed an open letter saying \"We demand the spies' execution! We shall not allow the enemies of the Soviet Union to live\". The writers include, among others, Mikhail Sholokov, Alexei Tolstoy, and surprisingly, Vasily Grossman, who was once saved from imprisonment by Bukharin's direct help. Grossman demanded \"No mercy to the Trotskyite degenerates, the murderous accomplishes of fascism.\" I mean, wow. A lucid and thoughtful book; the author has done an impressive job. Ferretter, L. (2006). Louis Althusser (R. Eaglestone, Ed.; 1st edition). Routledge. In the first part, he presents some arguments to convince the reader that Lenin, in the short time he was in power, had shown remarkable similarity with Stalin: punishing and 'liquidating' dissenters, being ruthless with the concerns of common Russians, specially peasants, overruling the possibility of a democracy—at times berating it—, and taking brutal steps to implement communist policies. Thorough summary; recommended. Gellately, R. (2008). Lenin, Stalin, and Hitler: The Age of Social Catastrophe (Reprint edition). Vintage. Russell, B. (1956). Why I am Not a Communist . In Portraits from Memory. https://www.rjgeib.com/thoughts/opiate/why.html A mostly cogent analysis of the similarities and dissimilarities between the totalitarian regimes of Lenin, Stalin, and Hitler—and he anticipated that the inclusion of Lenin with the other two will shock many. Gilabert, P., & O’Neill, M. (2024). Socialism . In E. N. Zalta & U. Nodelman (Eds.). The Stanford Encyclopedia of Philosophy One notable feature of this book is that it scarcely discussed about Marxism as implemented in Russia, North Vietnam, and China. Instead we get a thorough discussion of Cuban and Sweden’s socialism, which is interesting! Also the enrichment, and fragmentation, of Socialist ideal after the rise of Feminist and Left-Green movement were not left. The author is not oblivious to the issues of the system, but he doesn't shy away from pointing out the strengths. Recommended! In the subsequent sections, he shows parallels between Hitler and Stalin. His account of Stalin is particularly well written. While dekulakizing USSR, after the number of rich, and yet unpunished, peasants reached a really low number, it became difficult to determine \"who is a kulak?\". To settle that it was decided that any family with two samovars or a \"status symbol\" was to be considered as a kulak. One astonishing fact about that period is recent analyses show that the cheap slave labour by Gulag inmates was actually counter-productive for Russian economy. Like the common Russians, the party members also faced Stalin's wrath. According to Stalin, if any member committed suicide to evade a show trial, he \"is covering his track and deceiving the party for one last time. Suicide is simply a method to spit on the party for the last time\". Newman, M. Socialism: A Very Short Introduction . Oxford University Press, 2005. After describing the basic tenets of Socialism, the author discussed about the existent Socialist tradition before Marx came up with his scientific socialism. I liked the informative analysis on the early Utopians like the popular, but theoretically not so convincing, Etienne Cabet and more sound theorists like Henri de Saint-Simon, Charles Fourier, and Robert Owens. Then the Anarchist theories of Proudhon and Bakunin were also discussed in some detail. This studies were interspersed with the author’s thoughtful comment throughout, which I liked. And here is one slightly unnerving quote from Proudhon: To be governed is to be watched over, inspected, spied on, directed, legislated at, regulated, docketed, indoctrinated, preached at, controlled, assessed, weighed, censored, ordered about, by men who have neither the right nor the knowledge nor the virtue…. That’s government, that’s its justice, that’s its morality! Not entirely on socialism, but it touches on Marx's alienation. I think the author is hopelessly romantic—but then what’s life if one can't even dream? If I get time PhilosophyInsights (Director). (2018, May 23). Stephen Hicks: How Failed Marxist Predictions Led to the Postmodern Left YouTube Mommsen, P. (2025, March 3). The Quest to Emancipate Labor. Plough More A mostly cogent analysis of the similarities and dissimilarities between the totalitarian regimes of Lenin, Stalin, and Hitler—and he anticipated that the inclusion of Lenin with the other two will shock many. In the first part, he presents some arguments to convince the reader that Lenin, in the short time he was in power, had shown remarkable similarity with Stalin: punishing and 'liquidating' dissenters, being ruthless with the concerns of common Russians, specially peasants, overruling the possibility of a democracy—at times berating it—, and taking brutal steps to implement communist policies. In the subsequent sections, he shows parallels between Hitler and Stalin. His account of Stalin is particularly well written. While dekulakizing USSR, after the number of rich, and yet unpunished, peasants reached a really low number, it became difficult to determine \"who is a kulak?\". To settle that it was decided that any family with two samovars or a \"status symbol\" was to be considered as a kulak. One astonishing fact about that period is recent analyses show that the cheap slave labour by Gulag inmates was actually counter-productive for Russian economy. Like the common Russians, the party members also faced Stalin's wrath. According to Stalin, if any member committed suicide to evade a show trial, he \"is covering his track and deceiving the party for one last time. Suicide is simply a method to spit on the party for the last time\". The Communist party received enormous support from the citizens, mainly urban population. At the time of show trial of Nikolai Bukharin, after complications regarding the New Economic Policy, many prominent literary figures singed an open letter saying \"We demand the spies' execution! We shall not allow the enemies of the Soviet Union to live\". The writers include, among others, Mikhail Sholokov, Alexei Tolstoy, and surprisingly, Vasily Grossman, who was once saved from imprisonment by Bukharin's direct help. Grossman demanded \"No mercy to the Trotskyite degenerates, the murderous accomplishes of fascism.\" I mean, wow. Gellately sometimes has placed opinions, which are not well supported by data presented in the text, in between well-argued points. Though they do not contradict his claims, I found that problematic. Also while describing the show trials, he did not present the allegations, however superficial they may be, of the Communist party against its members. The clashes between the Nazis and the Communist party members of Germany are somewhat less described. Overall, I will recommend this book to anyone interested in the modus operandi of totalitarian regimes. Edwards, J., & Leiter, B. (2025). Marx . Routledge, Taylor & Francis group. My thoughts A lucid and thoughtful book; the author has done an impressive job. After describing the basic tenets of Socialism, the author discussed about the existent Socialist tradition before Marx came up with his scientific socialism. I liked the informative analysis on the early Utopians like the popular, but theoretically not so convincing, Etienne Cabet and more sound theorists like Henri de Saint-Simon, Charles Fourier, and Robert Owens. Then the Anarchist theories of Proudhon and Bakunin were also discussed in some detail. This studies were interspersed with the author’s thoughtful comment throughout, which I liked. And here is one slightly unnerving quote from Proudhon: To be governed is to be watched over, inspected, spied on, directed, legislated at, regulated, docketed, indoctrinated, preached at, controlled, assessed, weighed, censored, ordered about, by men who have neither the right nor the knowledge nor the virtue…. That’s government, that’s its justice, that’s its morality! One notable feature of this book is that it scarcely discussed about Marxism as implemented in Russia, North Vietnam, and China. Instead we get a thorough discussion of Cuban and Sweden’s socialism, which is interesting! Also the enrichment, and fragmentation, of Socialist ideal after the rise of Feminist and Left-Green movement were not left. The author is not oblivious to the issues of the system, but he doesn't shy away from pointing out the strengths. Recommended! Gellately sometimes has placed opinions, which are not well supported by data presented in the text, in between well-argued points. Though they do not contradict his claims, I found that problematic. Also while describing the show trials, he did not present the allegations, however superficial they may be, of the Communist party against its members. The clashes between the Nazis and the Communist party members of Germany are somewhat less described. Overall, I will recommend this book to anyone interested in the modus operandi of totalitarian regimes."
  },
  {
    "title": "Statistics with Python",
    "url": "https://arghyadutta.github.io/notebooks/statPython.html",
    "content": " probability or proportion: normalize such that bar heights sum to 1 In [2]: import pandas as pd import numpy as np from scipy import stats import matplotlib.pyplot as plt import seaborn as sns import statsmodels.api as sm # fix seed to get reproducible results # scipy.stats uses np.random to generate its random numbers, so setting seed using numpy works. (ref: https://stackoverflow.com/questions/16016959/scipy-stats-seed) np . random . seed ( seed = 23 ) nobs = 1000 # number of observations standard_normal = np . random . normal ( loc = 0.0 , scale = 1.0 , size = nobs ) shifted_normal = np . random . normal ( loc = 0.5 , scale = 1.0 , size = nobs ) plt . hist ( standard_normal , bins = 30 , histtype = \"step\" , label = \"standard normal\" ) plt . hist ( shifted_normal , bins = 30 , histtype = \"step\" , label = \"shifted normal\" ) plt . legend () # a quick way to fit a linear regression model using statmodels # ref: https://realpython.com/linear-regression-in-python/#advanced-linear-regression-with-statsmodels # simple linear regression # You need to add the column of ones to the inputs if you want statsmodels to calculate the intercept. It doesn’t take the intercept into account by default. x = standard_normal X = sm . add_constant ( standard_normal ) # print(x[:5], X[:5]) # create a y axis to test linear regression with an error term y = 2.3 + 1.55 * x + 2.3 * np . random . exponential ( size = nobs ) # be sure to provide X, not x lin_reg = sm . OLS ( y , X ) . fit () print ( lin_reg . summary ()) # you can get the predicted values using lin_reg.fittedvalues as used in the plot below. you can also use the model to predict y for new x values. x_new = np . linspace ( - 3 , 3 , 5 ) X_new = sm . add_constant ( x_new ) y_new = lin_reg . predict ( X_new ) # print(x_new,y_new) fig , ax = plt . subplots ( 1 , 2 ) ax [ 0 ] . scatter ( x , y , c = \"gray\" ) ax [ 0 ] . plot ( x , lin_reg . fittedvalues , c = \"orange\" ) ax [ 0 ] . scatter ( x_new , y_new , c = \"crimson\" ) # seaborn automatically plots with confidence interval sns . regplot ( x = x , y = y , ax = ax [ 1 ]) # multiple linear regression X = np . array ( list ( zip ( standard_normal , shifted_normal ))) X # You need to add the column of ones to the inputs if you want statsmodels to calculate the intercept. It doesn’t take intercept into account by default. X = sm . add_constant ( X ) X # create a y axis to test linear regression y = ( 2.3 + 1.55 * standard_normal + 2.4 * shifted_normal + 5 * np . sin ( np . random . random ( size = nobs )) ) lin_reg = sm . OLS ( y , X ) . fit () lin_reg . summary () # get statistical properties using scipy stats module stats . describe ( standard_normal ) stats . describe ( shifted_normal ) # checking if the distributions are normal (we know they are) stats . normaltest ( standard_normal ) stats . normaltest ( shifted_normal ) # KS-test stats . kstest ( standard_normal , stats . norm . cdf ) stats . kstest ( shifted_normal , stats . norm . cdf ) # two-tailed test # finds if the given random numbers are from same distribution stats . ttest_ind ( standard_normal , shifted_normal ) pass ; Make sure to mention what the y-axis denotes. count: number of observations in each bin density: normalize such that the total area of the histogram equals 1 import numpy as np import matplotlib.pyplot as plt import seaborn as sns fig , ax = plt . subplots ( 1 , 5 , figsize = ( 10 , 2 ), constrained_layout = True ) x = np . random . randint ( 0 , 9 , 1000 ) properties = { \"kde\" : True , \"bins\" : 9 } sns . histplot ( x , ax = ax [ 0 ], stat = \"count\" , ** properties ) sns . histplot ( x , ax = ax [ 1 ], stat = \"frequency\" , ** properties ) sns . histplot ( x , ax = ax [ 2 ], stat = \"probability\" , ** properties ) sns . histplot ( x , ax = ax [ 3 ], stat = \"percent\" , ** properties ) sns . histplot ( x , ax = ax [ 4 ], stat = \"density\" , ** properties ) pass ; import pandas as pd import numpy as np from scipy import stats import matplotlib.pyplot as plt import seaborn as sns import statsmodels.api as sm # fix seed to get reproducible results # scipy.stats uses np.random to generate its random numbers, so setting seed using numpy works. (ref: https://stackoverflow.com/questions/16016959/scipy-stats-seed) np . random . seed ( seed = 23 ) nobs = 1000 # number of observations standard_normal = np . random . normal ( loc = 0.0 , scale = 1.0 , size = nobs ) shifted_normal = np . random . normal ( loc = 0.5 , scale = 1.0 , size = nobs ) plt . hist ( standard_normal , bins = 30 , histtype = \"step\" , label = \"standard normal\" ) plt . hist ( shifted_normal , bins = 30 , histtype = \"step\" , label = \"shifted normal\" ) plt . legend () # a quick way to fit a linear regression model using statmodels # ref: https://realpython.com/linear-regression-in-python/#advanced-linear-regression-with-statsmodels # simple linear regression # You need to add the column of ones to the inputs if you want statsmodels to calculate the intercept. It doesn’t take the intercept into account by default. x = standard_normal X = sm . add_constant ( standard_normal ) # print(x[:5], X[:5]) # create a y axis to test linear regression with an error term y = 2.3 + 1.55 * x + 2.3 * np . random . exponential ( size = nobs ) # be sure to provide X, not x lin_reg = sm . OLS ( y , X ) . fit () print ( lin_reg . summary ()) # you can get the predicted values using lin_reg.fittedvalues as used in the plot below. you can also use the model to predict y for new x values. x_new = np . linspace ( - 3 , 3 , 5 ) X_new = sm . add_constant ( x_new ) y_new = lin_reg . predict ( X_new ) # print(x_new,y_new) fig , ax = plt . subplots ( 1 , 2 ) ax [ 0 ] . scatter ( x , y , c = \"gray\" ) ax [ 0 ] . plot ( x , lin_reg . fittedvalues , c = \"orange\" ) ax [ 0 ] . scatter ( x_new , y_new , c = \"crimson\" ) # seaborn automatically plots with confidence interval sns . regplot ( x = x , y = y , ax = ax [ 1 ]) # multiple linear regression X = np . array ( list ( zip ( standard_normal , shifted_normal ))) X # You need to add the column of ones to the inputs if you want statsmodels to calculate the intercept. It doesn’t take intercept into account by default. X = sm . add_constant ( X ) X # create a y axis to test linear regression y = ( 2.3 + 1.55 * standard_normal + 2.4 * shifted_normal + 5 * np . sin ( np . random . random ( size = nobs )) ) lin_reg = sm . OLS ( y , X ) . fit () lin_reg . summary () # get statistical properties using scipy stats module stats . describe ( standard_normal ) stats . describe ( shifted_normal ) # checking if the distributions are normal (we know they are) stats . normaltest ( standard_normal ) stats . normaltest ( shifted_normal ) # KS-test stats . kstest ( standard_normal , stats . norm . cdf ) stats . kstest ( shifted_normal , stats . norm . cdf ) # two-tailed test # finds if the given random numbers are from same distribution stats . ttest_ind ( standard_normal , shifted_normal ) pass ; In Seaborn, you can choose it via the stat parameter. From Seaborn documentation, stat has the following options [ws:seaborn] In [1]: import numpy as np import matplotlib.pyplot as plt import seaborn as sns fig , ax = plt . subplots ( 1 , 5 , figsize = ( 10 , 2 ), constrained_layout = True ) x = np . random . randint ( 0 , 9 , 1000 ) properties = { \"kde\" : True , \"bins\" : 9 } sns . histplot ( x , ax = ax [ 0 ], stat = \"count\" , ** properties ) sns . histplot ( x , ax = ax [ 1 ], stat = \"frequency\" , ** properties ) sns . histplot ( x , ax = ax [ 2 ], stat = \"probability\" , ** properties ) sns . histplot ( x , ax = ax [ 3 ], stat = \"percent\" , ** properties ) sns . histplot ( x , ax = ax [ 4 ], stat = \"density\" , ** properties ) pass ; frequency: number of observations divided by the bin width percent: normalize such that bar heights sum to 100 In [1]: See: Seaborn histplot documentation Histogram and KDE ¶ In [2]: import pandas as pd import numpy as np from scipy import stats import matplotlib.pyplot as plt import seaborn as sns import statsmodels.api as sm # fix seed to get reproducible results # scipy.stats uses np.random to generate its random numbers, so setting seed using numpy works. (ref: https://stackoverflow.com/questions/16016959/scipy-stats-seed) np . random . seed ( seed = 23 ) nobs = 1000 # number of observations standard_normal = np . random . normal ( loc = 0.0 , scale = 1.0 , size = nobs ) shifted_normal = np . random . normal ( loc = 0.5 , scale = 1.0 , size = nobs ) plt . hist ( standard_normal , bins = 30 , histtype = \"step\" , label = \"standard normal\" ) plt . hist ( shifted_normal , bins = 30 , histtype = \"step\" , label = \"shifted normal\" ) plt . legend () # a quick way to fit a linear regression model using statmodels # ref: https://realpython.com/linear-regression-in-python/#advanced-linear-regression-with-statsmodels # simple linear regression # You need to add the column of ones to the inputs if you want statsmodels to calculate the intercept. It doesn’t take the intercept into account by default. x = standard_normal X = sm . add_constant ( standard_normal ) # print(x[:5], X[:5]) # create a y axis to test linear regression with an error term y = 2.3 + 1.55 * x + 2.3 * np . random . exponential ( size = nobs ) # be sure to provide X, not x lin_reg = sm . OLS ( y , X ) . fit () print ( lin_reg . summary ()) # you can get the predicted values using lin_reg.fittedvalues as used in the plot below. you can also use the model to predict y for new x values. x_new = np . linspace ( - 3 , 3 , 5 ) X_new = sm . add_constant ( x_new ) y_new = lin_reg . predict ( X_new ) # print(x_new,y_new) fig , ax = plt . subplots ( 1 , 2 ) ax [ 0 ] . scatter ( x , y , c = \"gray\" ) ax [ 0 ] . plot ( x , lin_reg . fittedvalues , c = \"orange\" ) ax [ 0 ] . scatter ( x_new , y_new , c = \"crimson\" ) # seaborn automatically plots with confidence interval sns . regplot ( x = x , y = y , ax = ax [ 1 ]) # multiple linear regression X = np . array ( list ( zip ( standard_normal , shifted_normal ))) X # You need to add the column of ones to the inputs if you want statsmodels to calculate the intercept. It doesn’t take intercept into account by default. X = sm . add_constant ( X ) X # create a y axis to test linear regression y = ( 2.3 + 1.55 * standard_normal + 2.4 * shifted_normal + 5 * np . sin ( np . random . random ( size = nobs )) ) lin_reg = sm . OLS ( y , X ) . fit () lin_reg . summary () # get statistical properties using scipy stats module stats . describe ( standard_normal ) stats . describe ( shifted_normal ) # checking if the distributions are normal (we know they are) stats . normaltest ( standard_normal ) stats . normaltest ( shifted_normal ) # KS-test stats . kstest ( standard_normal , stats . norm . cdf ) stats . kstest ( shifted_normal , stats . norm . cdf ) # two-tailed test # finds if the given random numbers are from same distribution stats . ttest_ind ( standard_normal , shifted_normal ) pass ; OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.262\nModel:                            OLS   Adj. R-squared:                  0.261\nMethod:                 Least Squares   F-statistic:                     354.1\nDate:                Thu, 04 Sep 2025   Prob (F-statistic):           7.79e-68\nTime:                        14:19:30   Log-Likelihood:                -2342.7\nNo. Observations:                1000   AIC:                             4689.\nDf Residuals:                     998   BIC:                             4699.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          4.5249      0.080     56.637      0.000       4.368       4.682\nx1             1.5599      0.083     18.816      0.000       1.397       1.723\n==============================================================================\nOmnibus:                      822.687   Durbin-Watson:                   1.998\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            27895.609\nSkew:                           3.536   Prob(JB):                         0.00\nKurtosis:                      27.890   Cond. No.                         1.08\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified. Statistics with Python Histogram and KDE ¶ Histogram and KDE for 1000 random integers from [0,8]. Make sure to mention what the y-axis denotes. In Seaborn, you can choose it via the stat parameter. From Seaborn documentation, stat has the following options [ws:seaborn] count: number of observations in each bin frequency: number of observations divided by the bin width probability or proportion: normalize such that bar heights sum to 1 percent: normalize such that bar heights sum to 100 density: normalize such that the total area of the histogram equals 1 See: Seaborn histplot documentation Histogram and KDE for 1000 random integers from [0,8]. OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.262\nModel:                            OLS   Adj. R-squared:                  0.261\nMethod:                 Least Squares   F-statistic:                     354.1\nDate:                Thu, 04 Sep 2025   Prob (F-statistic):           7.79e-68\nTime:                        14:19:30   Log-Likelihood:                -2342.7\nNo. Observations:                1000   AIC:                             4689.\nDf Residuals:                     998   BIC:                             4699.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          4.5249      0.080     56.637      0.000       4.368       4.682\nx1             1.5599      0.083     18.816      0.000       1.397       1.723\n==============================================================================\nOmnibus:                      822.687   Durbin-Watson:                   1.998\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            27895.609\nSkew:                           3.536   Prob(JB):                         0.00\nKurtosis:                      27.890   Cond. No.                         1.08\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified. In [2]: Linear Regression in Python ¶"
  },
  {
    "title": "Symbolic Regression",
    "url": "https://arghyadutta.github.io/notebooks/symbolicRegression.html",
    "content": "Crutchfield, J. P. (1987). Equations of Motion from a Data Series . Complex Systems, 1, 417–452. Dutta, A., Vreeken, J., Ghiringhelli, L. M., & Bereau, T. (2021). Data-driven equation for drug–membrane permeability across drugs and membranes. The Journal of Chemical Physics, 154(24), 244114. https://doi.org/10.1063/5.0053931 Timeline of papers AI Feynmann from Tegmark. Recommended Schmidt, M., & Lipson, H. (2009). Distilling Free-Form Natural Laws from Experimental Data . Science, 324(5923), 81–85 . Yes, that's me. :P A method of systematically generating algebraic equations from data, with potential applications in discovering equations and even laws. The result, so far, has been quite promising, but no \"laws\" have been discovered, as yet. SISSO (Luca's group) and subsequent applications in material science. Crutchfield, J. P., & Young, K. (1989). Inferring statistical complexity . Physical Review Letters, 63(2), 105–108 . Ouyang, R., Curtarolo, S., Ahmetcik, E., Scheffler, M., & Ghiringhelli, L. M. (2018). SISSO: A compressed-sensing method for identifying the best low-dimensional descriptor in an immensity of offered candidates . Physical Review Materials, 2(8), 083802 . Ouyang, R., Ahmetcik, E., Carbogno, C., Scheffler, M., & Ghiringhelli, L. M. (2019). Simultaneous learning of several materials properties from incomplete databases with multi-task SISSO . Journal of Physics: Materials, 2(2), 024002 . Udrescu, S.-M., & Tegmark, M. (2020). AI Feynman: A Physics-Inspired Method for Symbolic Regression arXiv:1905.11481 . Purcell, T. A. R., Scheffler, M., Carbogno, C., & Ghiringhelli, L. M. (2022). SISSO++: A C++ Implementation of the Sure-Independence Screening and Sparsifying Operator Approach . Journal of Open Source Software, 7(71), 3960 . Schmidt and Lipson (2009). Note this paper has been criticized for not citing the literature (Crutchfield et al. 1987 and 1998). Koza, J. R. (1994). Genetic programming as a means for programming computers by natural selection . Statistics and Computing, 4(2), 87–112 . Genetic algorithm paper by Koza (1994)"
  },
  {
    "title": "Funny",
    "url": "https://arghyadutta.github.io/notebooks/funny.html",
    "content": "Joel Grus—Fizz Buzz in Tensorflow Cartoons by Sidney Harris Steve Plimpton’s collection of quotes A map of every city in Europe"
  },
  {
    "title": "Mathematics for Quantum Mechanics",
    "url": "https://arghyadutta.github.io/notebooks/qMechMath.html",
    "content": "Orthogonal Vectors Subspace Inner product in a tensor-product space A unitary operator is a normal operator with $A^\\dagger A = A A^\\dagger=I$. Since $U$ is normal, it's diagonalizable, too. $U$ preserves inner product $(U\\ket{v}, U\\ket{w}) = \\ket{v}\\ket{w}$. $U = \\sum_i \\ket{w_i}\\bra{v_i}$ Savov, I. (2020). No Bullshit Guide to Linear Algebra (2nd V2.2 ed. edition). Minireference Co. Incomplete, brief, and evolving \"lookup table\" on quantum mechanics for personal use, closely following Nielsen and Chuang (2010) and others. There may be typos (even conceptual errors!). If you find one, please tell me about it (argphy@gmail.com). Also, feel free to use the material for your own use. Constructing a basis can be very non-trivial for infinite-dimensional vector spaces. (More on this on pp. 29–30, Johnston 2021) Nielsen, M. A., & Chuang, I. L. (2010). Quantum computation and quantum information (10th anniversary ed). Cambridge University Press. $P=\\sum_{i=1}^k \\ket{i}\\bra{i}$ First, a definition. The adjoint/Hermitian conjugate of an operator $A$ is the unique operator $A^\\dagger$ that satisfies the relation $(\\ket{v},A\\ket{w})=(A^\\dagger \\ket{v},\\ket{w})$ for any $\\ket{v}$ and $\\ket{w}$. Mermin, N. D. (2007). Quantum Computer Science: An Introduction . Cambridge University Press. Interestingly, if the eigenvectors of $A: \\mathcal{V} \\rightarrow \\mathcal{V}$ have non-degenerate eigenvalues, then they form a basis (eigenbasis) $A = \\sum_i \\lambda_i \\ket{i}\\bra{i}$. Example: our favorite Hamiltonian operator and it's energy eigenvalues: $H=\\sum_E E\\ket{E}\\bra{E}$ Linear Operators It obeys all properties of linear operators. Spanning set need not be unique . $\\begin{smallmatrix}1 \\\\1 \\end{smallmatrix}$ and $\\begin{smallmatrix}1 \\\\-1 \\end{smallmatrix}$ span $\\mathbb{C}^2$, too. Also, $\\hat{i}$ and $\\hat{j}$ and $45$° anti-clockwise-rotated vectors $(\\hat{i}+\\hat{j})$ and $(-\\hat{i}+\\hat{j})$ both spans $\\mathbb{R}^2$. $\\{I,X,Y,Z\\}$ spans $\\mathcal{M}_2(\\mathbb{C})$. Functions of Operators Gram–Schmidt method is used to make a linearly independent set of vectors an orthonormal basis set. This is done by normalizing the first vector from the linearly independent set to make it the first basis element, and then progressively making the other vectors orthonormal by removing the projections. Wikipedia has a neat illustration, check it. Also Savov (2020) explained it well. Todo: Add a better synopsis later. $P^\\dagger=P$ The number of vectors in a basis sets the dimension of the vector space. If $\\mathcal{V}$ has a basis with $n$ vectors, then any set of $m>n$ vectors will be linearly dependent and any set of $m < n$ vectors cannot span $\\mathcal{V}$. (p. 27, Johnston 2021) $P_i P_j = \\delta_{ij}P_i$ Bra Vectors If $\\mathcal{V}$ is a vector space and $\\mathcal{S} \\subseteq \\mathcal{V}$, then $\\mathcal{S}$ is a subspace iff it is closed under addition and scalar multiplication. (You need not check all ten properties. See p. 9, Johnston (2021) for a proof). Vector Space Outer-product and Diagonal Representation $\\ket{v}\\otimes(\\ket{w_1}+\\ket{w_2}) = \\ket{v}\\otimes\\ket{w_1} + \\ket{v}\\otimes\\ket{w_2}$ $\\rm{Tr}(zA)=z\\rm{Tr}(A)$ Example: the set of non-invertible $2 \\times 2$ matrices is *not* a subspace of $\\mathcal{M}_2$. $\\begin{smallmatrix}1 0 \\\\ 0 0\\end{smallmatrix}$ and $\\begin{smallmatrix} 0 0 \\\\ 0 1\\end{smallmatrix}$ are non-invertible but their sum $\\begin{smallmatrix}1 0 \\\\ 0 1\\end{smallmatrix}$ is invertible. Recommended Singular Value Decomposition Some Useful Relations (incomplete, like other good things in life) Sadun, L. A. (2008). Applied linear algebra: The decoupling principle (2nd ed). American Mathematical Society. Tensor products of (Hermitian, unitary, positive, projector) operators retain those properties. $\\rm{Tr}(A+B)=\\rm{Tr}(A)+\\rm{Tr}(B)$ $\\frac{1}{\\sqrt{2}}(\\ket{0}-i\\ket{1} = \\frac{1}{\\sqrt{2}}\\begin{bmatrix}1 \\\\-i\\end{bmatrix} \\equiv \\ket{y_-}$ Associativity: $(\\ket{v}+\\ket{w})+\\ket{x}=\\ket{v}+(\\ket{w}+\\ket{x})$ Computational Basis Orthonormal Basis Set Kronecker product as matrix representation for $A\\otimes B$ Let $\\mathcal{V}$ be a set of elements (vectors) with two operations, addition and multiplication with a scalar from a field $\\mathbb{F}$ (typically $\\mathbb{C}$ in QM). $\\mathcal{V}$ is a called vector space if for vectors $\\ket{v},\\ket{w},\\ket{x} \\in \\mathcal{V}$ and scalars $c,d \\in \\mathbb{F}$, the following ten conditions are satisfied: Bases For a positive operator : $(\\ket{v},A\\ket{v})\\geq 0$. If it is positive, then $A$ is called positive definite. $A^\\dagger A$ is positive for any operator $A$. $\\rm{Tr}(A)=\\sum_i A_{ii}$ $\\frac{1}{\\sqrt{2}}(\\ket{0}+\\ket{1}) = \\frac{1}{\\sqrt{2}}\\begin{bmatrix} 1 \\\\1 \\end{bmatrix} \\equiv\\ket{+}$ Distributivity: $(c+d)\\ket{v}=c\\ket{v}+d\\ket{v}$ Existence of a 'zero vector' $\\ket{0}$ such that $\\ket{v}+\\ket{0}=\\ket{v}$ Linear Combination Spectral Decomposition $\\frac{1}{\\sqrt{2}}(\\ket{0}-\\ket{1}) = \\frac{1}{\\sqrt{2}}\\begin{bmatrix}1 \\\\-1\\end{bmatrix} \\equiv \\ket{-}$ Johnston, N. (2021). Advanced Linear and Matrix Algebra . Springer International Publishing. A basis of a vector space $\\mathcal{V}$ is a set of vectors in $\\mathcal{V}$ that spans it and is linearly independent. Example, $\\begin{smallmatrix} 1 \\\\ 0 \\end{smallmatrix}$ and $\\begin{smallmatrix} 0 \\\\1 \\end{smallmatrix}$ spans $\\mathbb{C}^2$. Linear operators on $\\mathcal{V}\\otimes \\mathcal{W}$ : If $A: \\mathcal{V} \\rightarrow \\mathcal{V}'$ and $B: \\mathcal{W} \\rightarrow \\mathcal{W}'$ are two linear operators, then $A\\otimes B: \\mathcal{V}\\otimes \\mathcal{W} \\rightarrow \\mathcal{V}' \\otimes \\mathcal{W}'$ is defined as A function $(,):\\mathcal{V} \\times \\mathcal{V} \\rightarrow \\mathbb{C}$. Some properties: Cohen Tannoudji (2020) (pp. 103–108) and Mermin (2007) pp. 159–164 for careful definition of bra vectors A self-adjoint/Hermitian operator satisfies $A^\\dagger=A$. A bra vector is a linear functional $\\mathcal{V} \\rightarrow \\mathbb{C}$; i.e. it takes a ket vector to a complex number: $(\\bra{v})\\ket{w} = \\braket{v}{w}$. Cohen Tannoudji (2020) showed that the space of all such linear functionals forms another vector space $V^*$—known as the dual space of $V$ (pp. 103–108). A bra vector is an element of $V^*$ . A bra vector can also be represented as an adjoint vector: $\\ket{v}^\\dagger \\equiv \\bra{v}$. Distributivity: $c(\\ket{v}+\\ket{w}) = c\\ket{v} +c\\ket{w}$ Levi-Civita All *finite* linear combinations of vectors of $\\mathcal{B} \\subseteq \\mathcal{V}$. $\\rm{span}(\\mathcal{B})$ is a subspace of $\\mathcal{V}$. If $\\rm{span}(\\mathcal{B})=\\mathcal{V}$ then $\\mathcal{V}$ is said to be spanned by $\\mathcal{B}$. For example, a vector spans a line and two non-parallel vectors spans $\\mathbb{R}^2$. Some properties of the adjoint operator: Types of Linear Operators Notation: $\\ket{\\psi}^{\\otimes 2} \\equiv \\ket{\\psi}\\otimes \\ket{\\psi}$ Span Polar Decomposition Eigenvectors Cohen-Tannoudji, C., Diu, B., & Laloë, F. (2020). Quantum mechanics. Volume 1: Basic concepts, tools, and applications (S. Reid Hemley, N. Ostrowsky, & D. Ostrowsky, Trans.; Second edition). Wiley-VCH Verlag GmbH & Co. KGaA. $\\rm{Tr}(A\\ket{\\psi}\\bra{\\psi})=\\bra{\\psi}{A}\\ket{\\psi}$ Useful When we represent a vector in some basis, we order the basis set so that the vector can be written in terms of its 'coordinates' $[\\ket{v}]_\\mathcal{B} \\equiv (c_1,\\cdots,c_n)$. The coordinates depend on the bases . If $\\mathcal{V}$ and $\\mathcal{W}$ are $m$ and $n$ dimensional vector spaces, then $\\mathcal{V} \\otimes \\mathcal{W}$ (read as 'V tensor W') is an $mn$ dimensional vector space. A set of vectors $\\mathcal{B} \\subseteq \\mathcal{V}$ is linearly dependent if there exists a set of scalars $c_1,c_2,\\cdots,c_k$, not all zeros, such that $c_i\\ket{v}*i=\\ket{0}$. $\\mathcal{B}$ is linearly independent if it is not linearly dependent. (It's funny 😄, but see the example for what is means.) Interestingly, for any ket vector in $V$, there exists a bra vector in $V^*$; but for any bra vector, there may not exist a ket vector if the ket belongs to an infinite-dimensional Hilbert space (Cohen Tannoudji 2020 showed a counter-example). For finite-dimensional vector spaces, they are of the same size, and so you'll always get kets corresponding to bras, though. $\\rm{Tr}(AB)=\\rm{Tr}(BA)$ Sadun (2008) has a neat discussion in section 6.3 titled 'Bras, Kets, and Duality' pp. 152–156 Any vector of the form $\\sum^k_{i=1} c_i \\ket{v_i}$ is called a linear combination. Interesting: The Identity matrix *cannot* be written as a linear combination of Pauli matrices. Linear Dependence and Independence Why you may not want to read this page Trace Projector Operator Commutativity: $\\ket{v} + \\ket{w} = \\ket{w} + \\ket{v}$ $c(d\\ket{v})=(cd)\\ket{v}$ Special case: Measurement operator $M=\\sum_i\\lambda_i P_i$ A basis set whose elements are orthonormal $\\bra{i},\\ket{j}=\\delta_{ij}$ (like unit vectors). They are complete: $\\sum_i \\ket{i}\\bra{i} = I$. A map between two vector spaces $\\mathcal{V}$ and $\\mathcal{W}$, $A: \\mathcal{V} \\rightarrow \\mathcal{W}$, that preserves linearity $A (c_1 \\ket{v_1} + c_2 \\ket{v_2}) = c_1 A \\ket{v_1} + c_2 A \\ket{v_2}$. Generally, it has a matrix representation given by $A\\ket{v_j}=\\sum_i A_{ij}\\ket{w_i}$. The representation depends on the basis. Inner Product The vectors in $\\mathcal{B}$ uniquely combine to generate vectors in $\\mathcal{V}$. (Of course! $2\\hat{i}+3\\hat{j}$ is a unique vector in the $\\hat{i}, \\hat{j}$ basis. For a proof, see p. 21 of Johnston 2021). Examples of vector spaces include $\\mathbb{R}^n$, $\\mathbb{C}^n$, $\\mathcal{M}_{m,n}(\\mathbb{F})$ (space of $m\\times n$ matrices with elements from the field $\\mathbb{F}$). One important point is when it is said that $\\mathcal{V}$ is a vector space over a field $\\mathbb{F}$, it means that the scalars used in the scalar multiplication comes from $\\mathbb{F}$ (also known as a ground field ), not that the elements of the vectors are from $\\mathbb{F}$. For example Hermitian matrices can be defined over $\\mathbb{R}$ or $\\mathbb{C}$, though its elements are from $\\mathbb{C}$. This is important since, for example, the space of $n\\times n$ Hermitian matrices $\\mathcal{M}^{\\rm H}_n$ is not a vector space over $\\mathbb{C}$, but it is a vector space over $\\mathbb{R}$. Example: $A=\\begin{pmatrix} 0 1 \\\\ 1 0 \\end{pmatrix} \\in \\mathcal{M}^{\\rm H}_2$ but since $(iA)^\\dagger\\neq iA$, it's not Hermitian, so $\\mathcal{M}^{\\rm H}_2$ isn't closed under scalar multiplication over $\\mathbb{C}$. $\\frac{1}{\\sqrt{2}}(\\ket{0}+i\\ket{1}) = \\frac{1}{\\sqrt{2}}\\begin{bmatrix}1 \\\\i \\end{bmatrix} \\equiv \\ket{y_+}$ Existence of an 'inverse': a vector $-\\ket{v}$ such that $\\ket{v}+(-\\ket{v})=\\ket{0}$ Identity and Pauli Matrices Similarity Transformation Outer-product representation reduces to diagonal representation (eigendecomposition/orthonormal decomposition) when $\\ket{v_i}, \\ket{w_j}$ are orthonormal eigenvalues of $A$. Simultaneous Diagonalization Theorem Tensor Products $(\\ket{v_1}+\\ket{v_2})\\otimes\\ket{w}=\\ket{v_1}\\otimes\\ket{w}+\\ket{v_2}\\otimes\\ket{w}$ If $\\ket{i}$ and $\\ket{j}$ are orthonormal bases in $\\mathcal{V}$ and $\\mathcal{W}$ then $\\ket{i}\\otimes \\ket{j}$ is a basis for $\\mathcal{V}\\otimes \\mathcal{W}$. Example: Two non-parallel vectors on a plane are linearly independent, three of more of them on a plane are linearly dependent. Notice that an infinite set of vectors can be linearly independent—you only need to show that there are not any finite linear combination of them such that all the scalars used in the combination are zeros. Think about the set $\\{1,x,x^2,\\cdots\\}$. Though the set contains infinitely many elements, it is linearly independent. Because if $\\sum_{i=0}^p c_i x^i=0$ then all $c_i$s are equal to 0. (set $x=0 \\implies c_0=0$; take derivatives and show $c_1=0$ and so on. See p. 17 of Johnston (2021)). So we cannot find any linear combination which is equal to 0 with at least one non-zero coefficient, implying that the set is not linearly dependent. So it's linearly independent! $1\\ket{v}=\\ket{v}$ Closure under scalar multiplication: $c\\ket{v} \\in \\mathcal{V}$ $P^2 =P$ Normal operator : $A$ is normal if $A^\\dagger A = A A^\\dagger$. A normal matrix is Hermitian if and only if it has real eigenvalues. Eigenvectors of an Hermitian operators with distinct eigenvalues are orthogonal. $z(\\ket{v}\\otimes\\ket{w})=(z\\ket{v})\\otimes\\ket{w}=\\ket{v}\\otimes(z\\ket{w})$ Closure under addition: $\\ket{v} + \\ket{w} \\in \\mathcal{V}$"
  },
  {
    "title": "Non-linear Dynamics",
    "url": "https://arghyadutta.github.io/notebooks/nld.html",
    "content": "The chapter on nonlinear differential equations is especially neat. I liked the idea of introducing critical points and paths first for linear systems, and then for nonlinear systems. Ross, S. L. (2004). Differential equations (Third edition, Wiley student edition). Wiley India. Percival, I.; Richards, D. Introduction to Dynamics; Cambridge University Press: Cambridge ; New York, 1982. Recommended Strogatz, S. H. (2015). Nonlinear dynamics and chaos: With applications to physics, biology, chemistry, and engineering (Second edition). Westview Press. Excellent! Krishnaswami, G. S. (2020). Notes on Nonlinear Dynamics . PDF Concise."
  },
  {
    "title": "Ursula K. Le Guin",
    "url": "https://arghyadutta.github.io/notebooks/leguin.html",
    "content": "Recommended Guin, U. K. L. (2016). A Wizard of Earthsea . Puffin. Guin, U. K. L. (2018). The Books of Earthsea . Gollancz. Le Guin's prose flows, interlaced with intricate, compassionate thoughts."
  },
  {
    "title": "Buddha and Buddhism",
    "url": "https://arghyadutta.github.io/notebooks/buddhism.html",
    "content": "The Dhammapada: Verses and Stories Recommended If I Get Time Kornfield, J. (2005). The Dhammapada: A New Translation of the Buddhist Classic with Annotations (G. Fronsdal, Trans.). Shambhala. Bhattacharya, K. (1975). On the Bramhan in Buddhist Literature. Oriental Journal Tirupati, XVIII. Rahula, W. (1974). What the Buddha Taught: Revised and Expanded Edition with Texts from Suttas and Dhammapada (Revised edition). Grove Press."
  },
  {
    "title": "Foundations of Quantum mechanics",
    "url": "https://arghyadutta.github.io/notebooks/qMechFoundations.html",
    "content": "Foundations of quantum mechanics is a great subject, until you want to publish, of course. Norsen's accessible and mildly opinionated book provides a good introduction. Foundations of Quantum Mechanics by Florian Marquardt . Recommended Norsen, T. (2017). Foundations of Quantum Mechanics: An Exploration of the Physical Meaning of Quantum Theory (1st ed.). Springer International Publishing Courses on YouTube Peres, A. (2010). Quantum theory: Concepts and methods . Kluwer Acad. Publ. Another series, by Rob Spekkens, on Foundations of Quantum Mechanics . Asher Peres's book goes deep early on and is easily the most difficult of all the books mentioned in this list. It's very rewarding, though. Fun slides."
  },
  {
    "title": "Complex Systems",
    "url": "https://arghyadutta.github.io/notebooks/complexity.html",
    "content": "For Ising Model: If I get time Bianconi, G. et al. (2023). Complex systems in the spotlight: Next steps after the 2021 Nobel Prize in Physics . Journal of Physics: Complexity, 4(1), 010201 . Recommended Taroni, A. (2015). 90 years of the Ising model . Nature Physics, 11(12), 997–997. (for the story) Mitchell, M. (2009). Complexity: A guided tour . Oxford University Press. Böttcher, L., & Herrmann, H. J. (2021). Computational Statistical Physics (1st ed.). Cambridge University Press. Chandler, D. (1987). Introduction to Modern Statistical Mechanics . Oxford University Press. (Excellent book.) Anderson, P. W. (1972). More Is Different: Broken symmetry and the nature of the hierarchical structure of science . Science, 177(4047), 393–396 For Computational Physics Parisi, G. (2002). Complex Systems: A Physicist’s Viewpoint arXiv:cond-mat/0205297 Crutchfield, J. P., & Young, K. (1989). Inferring statistical complexity . Physical Review Letters, 63(2), 105–108 ."
  },
  {
    "title": "Statistical Mechanics: Resources",
    "url": "https://arghyadutta.github.io/notebooks/statMechReadingMaterial.html",
    "content": "This is a list of advanced and specialized textbooks. For introductory books, see this list . Also see A thorough book and—unusually—starts with detailed discussions on entropy. The book uses slightly unwieldy, verbose notations, but I think explicit notations is a feature here, not a problem. Swendsen, R. H. (2019). An Introduction to Statistical Mechanics and Thermodynamics (2nd ed.). Oxford University Press. Bose–Einstein Condensates Courses on YouTube Non-equilibrium Statistical Mechanics by V. S. Balakrishnan . Introductory lectures on statistical mechanics by John Preskill Chandler, D. (1987). Introduction to Modern Statistical Mechanics. OUP USA. Dalvit, D. A. R., Frastai, J., & Lawrie, I. D. (1999). Problems on statistical mechanics . Institute of Physics. Huang, K. (1987). Statistical mechanics (2nd ed). Wiley. Balakrishnan, V. (2021). Elements of Nonequilibrium Statistical Mechanics. Springer International Publishing. Ambegaokar, V. (1996). Reasoning about luck: Probability and its uses in physics . Cambridge University Press. (Archive) Clean, accessible discussions on numerical approaches in statistical mechanics. For instance, check the discussion on how ensembles are used in molecular dynamics simulations. Kardar, M. (2007). Statistical Physics of Fields (1st ed.). Cambridge University Press. Recommended Books Nishimori, H., & Ortiz, G. (2011). Elements of phase transitions and critical phenomena . Oxford University Press. Toda, M., Kubo, R., & Saitô, N. (1992). Statistical Physics I: Equilibrium Statistical Mechanics (Vol. 30). Springer Berlin Heidelberg. Kubo, R., Toda, M., & Hashitsume, N. (1991). Statistical Physics II (Vol. 31). Springer Berlin Heidelberg. Good book with a spectacular beginning where he puts the partition function in the place it deserves. Allen, M. P., & Tildesley, D. J. (2017). Computer simulation of liquids (Second edition). Oxford University Press. To read Thorough, excellent treatment of non-equilibrium stat mech. Hydrophobicity Broad yet concise. The chapter on the general properties of the partition function has a nice discussion on the zeroes of the partition function and connections to phase transition. Complex Systems (related ideas to stat mech.) Concise, but so well-written! Check, for example, the first chapter on thermodynamics (and especially Legendre transforms). Non-equilibrium Statistical Mechanics by V. S. Balakrishnan . I've only had a look at this book, but I'm thoroughly impressed by its depth. Feynman, R. (1998). Statistical Mechanics: A Set Of Lectures . Taylor & Francis. Self-Avoiding Random Walks It has a neat discussion on mean field theories (particularly $\\phi^6$ theory related to tricritical points). Introductory lectures on statistical mechanics by John Preskill If you're trying to calculate higher-order Feynman diagrams for scalar field theories, Kleinert and Schulte-Frohlinde's book is the best reference . Baxter, R. J. (2008). Exactly Solved Models in Statistical Mechanics . Dover Publications. Kleinert, H., & Schulte-frohlinde, V. (2001). Critical Properties Of $\\phi^4$-Theories . World Scientific. Krapivsky, P. L., Redner, S., & Ben-Naim, E. (2010). A Kinetic View of Statistical Physics (1st ed.). Cambridge University Press. The best resource for exactly solvable models. Bayesian Statistics The chapters on aggregation, fragmentation, and adsorption are particularly good. Tolman, R. C. (1979). The principles of statistical mechanics . Dover Publications. Ryogo Kubo's books are some of the most didactic books I've ever read. Excellent book on problems, more so because it comes with solutions. Check it if you want to know, for example, under which conditions bosons with a $p^s$ dispersion relation in $d$-dimensions will form a condensate. My favorite book regarding field-theoretic formulation of critical phenomena. The chapters on the perturbative renormalization group of $\\phi^4$ theories are superb."
  }
]